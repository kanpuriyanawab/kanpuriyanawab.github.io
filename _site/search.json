[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Building Production Ready Text-to-SQL Agent from Scratch\n\n\nTired of Writing SQL? Let’s Build an AI That Does It For You.\n\n\n\n\n\nJun 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Memory Layer Will Change Everything\n\n\nHow Stateful Memory is Revolutionizing AI Agents and Applications\n\n\n\n\n\nJun 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with Aakash Kumar Nain, MLE at Merlyn Minds\n\n\njoint work with Abheesht Sharma\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIllustrated LLM OS\n\n\nAn Implementational Perspective of LLMs as Operating Systems\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with Matthew Watson, Keras Team, Google\n\n\njoint work with Abheesht Sharma\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating High-quality Images with SD.Next, HuggingFace Diffusers and W&B\n\n\nA walkthrough of using SD.Next (Advanced webUI for Stable Diffusion) for generating high-quality images using HuggingFace Diffusers and managing experiments with W&B.\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with Rajaswa Patil, AI Researcher at Postman Labs\n\n\njoint work with Abheesht Sharma\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Semantic Similarity using KerasNLP\n\n\nThis report demonstrates how to use the Weights&Biases to speedup experiments with Keras\n\n\n\n\n\nSep 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning in Julia\n\n\nImplementing ML models in Flux, the Julia Deep Learning Package!\n\n\n\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive data exploration with Julia\n\n\nExploratory Data Analysis of Housing dataset in Julia\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD - Ravi Ramakrishnan\n\n\njoint work with Abheesht Sharma\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/life.html",
    "href": "pages/life.html",
    "title": "Life",
    "section": "",
    "text": "&lt;img src=“../assets/images/life/mysore-road\n\n\n\n\nMy Strava: https://www.strava.com/athletes/141188010"
  },
  {
    "objectID": "pages/life.html#rasta-cafe-126-km",
    "href": "pages/life.html#rasta-cafe-126-km",
    "title": "Life",
    "section": "",
    "text": "&lt;img src=“../assets/images/life/mysore-road\n\n\n\n\nMy Strava: https://www.strava.com/athletes/141188010"
  },
  {
    "objectID": "posts/interview_matt.html",
    "href": "posts/interview_matt.html",
    "title": "Interview with Matthew Watson, Keras Team, Google",
    "section": "",
    "text": "by Anshuman Mishra & Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard-of) stories out into the world — their struggles, their triumphs, and their journey. We will be conversing with people we have looked up to and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nIn this installment of our series, we sit down with Matthew Watson for a chat. Matt, a Stanford CS alumnus, is a software engineer with the Keras team at Google, where he works on KerasNLP, Keras’ native NLP offering. He has also been instrumental in developing Keras 3 (its announcement created significant buzz on Twitter!), a multi-backend Keras that can run on TensorFlow, JAX, and PyTorch. In the past decade or so, Matt has already had a wonderfully diverse career — he started off with literature in his undergrad, before developing a love for CS (more on this in the interview!).\nAll words seem inadequate when one sets out to describe Matt. Matt is one of those “10x” engineers/researchers and always has a solution to every problem. We have always found his intuition to be bang on target. Matt has played a pivotal role in both of our careers; he has mentored us during (and after) Google Summer of Code. But one needs to look beyond just his technical expertise. Matt has infinite patience; we’ve spent late-night debugging sessions with him, and he has always been super-helpful and giving. Matt is an “open source stan”, and you can see his love for open source in every response of his on GitHub, and in every meticulously drafted PR (Pull Request). Safe to say, Matt has influenced us like no one else.\nLet’s begin!\nAnshuman & Abheesht: Welcome, Matt! Thanks for doing this.\nMatt: Hi Anshuman and Abheesht! And thanks for putting this whole series together. I should probably quickly note that opinions here are my own; I’m not speaking on behalf of Google or any organization.\nAnshuman: Can you tell us about your journey in the field of natural language processing (NLP) and how you became involved with the Keras team at Google?\nMatt: Honestly I have taken a meandering path, but two consistent threads I can pick out of the last decade of my career are a love of language and of open-source software.\nI started my undergraduate studies with a focus on literature before getting lured to Computer Science by some really fantastic teachers. Through that switch, I have remained fascinated with the intersection of language and math. My first hands-on experience with NLP was helping with a research project at my university extracting information from Wikipedia via support vector machines written in Java (times have changed!). I started using Keras in 2017 when everyone was discovering text generation with LSTMs.\nI’ve had the chance to spend most of my career so far working in open source (first in the Linux community and now ML) and have always been so grateful for it. So joining Keras honestly felt like a no-brainer; it was a chance to do open work on my interests with a library I already loved.\nAbheesht: I’d like to talk a bit more about the career pivots you’ve done. In school/college, you say that you were interested in literature, post which you pivoted to Computer Science. Even in the field of Computer Science, you started off with Computer Graphics, started working on building an operating system at Endless Computers, and worked in the Firebase team, before joining Keras. How much did learning/working in different “disciplines/fields” help you? Were there any opportunities for “cross-discipline” learning, say, something you learned from discipline A which helped you in discipline B?\nMatt: I’m really glad I got a start in the humanities because writing is so important for almost every field. Being able to articulate a thought clearly and concisely is an important skill we underplay too often in the hard sciences.\nComputer graphics and the ML work I’m doing today feel closely intertwined. My advisor for my Master’s degree, Pat Hanrahan, was a driving force behind programmable shading language, and that blows my mind to this day. The early days of GPUs were all about these fixed-function pipelines for creating and rasterizing triangles. Then came the idea to write these domain-specific shading languages that added a ton of expressivity to the process. And from that came the idea of general-purpose GPU computations, and things like CUDA, and here we are today! ML as a field has probably been moved forward literal decades thanks to ideas from the graphics world.\nI’m grateful for my time in the Linux open-source world for quite different reasons. I can’t say there’s much overlap in subject matter, but it was a wonderful start in the world of open source. It definitely made me a bit of a tinkerer, with everything from my OS to shell to IDE. Linux is built around supporting that. But it’s something I think equally applies to the world of ML. The deeper you get into these big deep-learning frameworks the more clear it becomes they all have bugs. You gain so much as an ML developer when you can get into the weeds of a library and start changing things.\nAnyway, that’s a long-winded way of saying I’m quite glad my career hasn’t been a straight line.\nAnshuman: Given your interest in English literature, which novels are you currently reading? What’s on your to-read list? Do you have any writing aspirations?\nMatt: Hah, for any attempt to read literature with a capital “L” in school, my reading diet since I was 10 has been mostly sci-fi and fantasy. Ursula K. Le Guin is a big hero of mine. I recently read Klara and the Sun, a dystopian novel narrated by an “autonomous friend” that certainly feels relevant to all the work we do.\nI would love to try my hand at fiction someday, but most of my writing is technical these days.\nAbheesht: What are the focal attributes of KerasNLP that make it different from other NLP libraries out there?\nMatt: One thing that makes Keras unique is a detailed and strict style guide for any new API. Our top-level APIs should be accessible to a very broad audience. We achieve flexibility underneath this top level with a focus on modular components and layered abstractions.\nThis gives KerasNLP a somewhat unique positioning among “pre-trained modeling” libraries. When we port a model from a research codebase, we tend to heavily rewrite the forward pass so it matches our conventions and uses common building blocks.\nThis has some neat side effects. For one, it’s a great library to learn from–you can define a transformer from scratch with our base layers in about 20 lines of code. It also gives a very uniform experience of slicing and dicing our models. That’s a big one given how many real-world workflows, from PEFT strategies to interpretability techniques, require reaching into model internals.\nAbheesht: How do you see the current landscape of NLP and deep learning evolving, and how does KerasNLP fit into this evolving landscape?\nMatt: It’s dizzying how fast the field is changing these days. There is a ton of energy in open-source NLP development. There are so many valid concerns with how all this new technology is being used. At a broad level, I keep trying to remind myself what a massive period of flux this is for the field. We should all hold our opinions lightly and stay curious.\nI think multi-backend Keras 3 is really a game changer for how KerasNLP fits it. On the KerasNLP side, we can take a pre-trained model, write our numerics once, and distribute our weights once. You can fine-tune a model in any of TensorFlow, Jax, or Torch, and use Keras’ save format as a way to seamlessly move between frameworks. I think this is a new tool we are putting in ML developer’s toolboxes, and I am excited to see what people do with it.\nAnshuman: Could you share some real-world applications or projects where KerasNLP has been particularly instrumental or innovative?\nMatt: One of the first projects to use KerasNLP is the AutoKeras library, and I really love how it fits into the Keras broader Keras ecosystem. It’s a super accessible, code-first library. You can just define your data and problem and it will handle choosing an architecture, optimizer, and hyperparameters. Or you can define your own much more complex and tailored workflow as you get deeper in.\nI think there is a huge opportunity to take a similar approach to generative problems going forward. Meet people with accessible, understandable, code-first flows that still give you full control over an underlying language model when you need it. I haven’t quite seen an “auto ML” offering on the generative modeling front that has really blown me away, but I would bet it’s just a matter of time.\nAbheesht: Natural language understanding and generation have seen remarkable advancements. What do you think are the key challenges and opportunities in NLP today?\nMatt: There’s a huge amount of opportunity here. We are far from understanding all the ways we can put the emergent properties of LLMs to use. Given the amount of investment and attention, I am not particularly worried about the field suddenly losing steam.\nAn obvious challenge is the reliability of language models. If you had a service where a fellow human would often give you helpful advice, and occasionally confidently lie to your face, people would be out there flipping tables.\nI think there are two things we could do here. First, keep working on the engineering problem of reliable models that can, at least, indicate uncertainty when present. Second, don’t oversell these things! There are a lot of unsolved problems with LLMs, and there is a persistent, magnetic attraction towards anthropomorphizing something that appears to understand language. Everyone working in this space needs to be careful with how we describe these systems, and continually flag the obvious limitations of these tools.\nAbheesht: One thing I’ve seen with the Keras team is the collaborative culture that has been fostered. All members of the Keras team are very helpful to the outside community. Evidence of this is seen in the number of students who contribute to the Keras ecosystem. Can you provide insights into the collaborative development process within the Keras team at Google and outside contributors and how it contributes to the library’s success?\nMatt: A huge reason I was attracted to the Keras team was that everyone on it really cares about open-source tools, not as an afterthought or a way to boost your profile, but as an end unto itself.\nThe vast majority of my work is out in the open on GitHub, and I think that is true of the whole Keras ecosystem. Most real discussion about new APIs is happening on issues and pull requests.\nI think everyone on the team really believes in the virtuous cycle here. There is far more knowledge and skill in the large pool of Keras users than just the Keras team at Google.\nAnshuman: In the ever-evolving field of AI, staying updated is crucial. How do you personally keep up with the latest developments and trends in NLP and deep learning?\nMatt: It’s a mix right now of Twitter, word of mouth, and anything that naturally comes up in my work. There is just no way for anyone to ingest all the research in this space. I constantly try to curate a short list of concepts to understand deeply, rather than attempting to ingest the whole fire hose.\nIf I encounter a new claim that breaks an assumption I was holding, or a bug in my code I can’t effectively explain, it’s almost always worth digging in. ML as a field I think really rewards going deep on unknowns and constantly leveling up your understanding.\nAnshuman: You’ve done your Bachelor’s and Master’s from Stanford, widely regarded as one of the best universities in the world. How was your time at Stanford? How do you think your education has shaped your career as an engineer over the past decade? What makes Stanford Stanford; what does Stanford do differently from other universities that makes it the best? Is it the faculty? Is it the peer group?\nMatt: The main thing I think Stanford provided was this massive playground to go out and build your own understanding. They give a lot of resources and freedom to students, and there’s a lot of bright people trying weird stuff you can talk to. There’s definite energy in that environment.\nStanford also killed it with the introductory Computer Science courses. The lecturers and teaching assistants were really supportive and focused on sparking curiosity about what computers can do. That is for sure what lured me out of the social sciences.\nI wouldn’t put too much stock into any one place though. The best coworkers in my working life have truly come from all sorts of educational backgrounds; I cannot pick out a pattern. People who teach themselves and even get a bit irritated by things they don’t yet understand tend to make great engineers. And open source in particular is one of the most accessible ways to wade into real problems from anywhere in the world.\nAbheesht: What, do you think, is the future for KerasNLP? Where do you see it going? What are some exciting developments that are planned in the near future?\nMatt: The near future for KerasNLP is all about sticking the landing with Keras 3 (multi-backend Keras). We want to make sure we have the models people are most excited about (e.g. LLama 2 and Falcon), but equally important is a good ecosystem of tools to scale these models up and down.\nOn the scaling down front, we want a good multi-backend workflow for LoRA, and for scaling up, we are working on some dead simple model parallelism (think sharding individual matrices across GPUs). It’s definitely a lot to execute on, but that’s kinda fun. We are all heads down on our projects till the release of Keras 3 later this year.\nAbheesht: Having talked to you before, I know how bullish you are on open-source software. Is open source the way forward with LLMs? The upsides are obvious; the downsides include potential misuse, etc. What are your views on this?\nMatt: I think the biggest thing we can all do is stay open to changing our viewpoints. We are way out in uncharted territory in terms of model capability, so the most accurate (and maybe unhelpful) thing we can say is that we don’t know what the biggest risks are or how to mitigate them.\nI should probably just recuse myself from any conversations of existential risk from language models; I don’t know enough. I am glad some people out there are thinking about it, but personally, I am much more worried about the more immediate societal issues–bias, misuse, misinformation, disruption from automation, etc. These I feel strongly need to be addressed in the public square, and open models will be an essential tool.\nFirst, with an open model, you can crowdsource finding all the issues and fail states. More people probing a model will do a much more thorough job of making it “misbehave.” Second, if we really open up the whole training recipe (what data is going in?!), we can have a much more open conversation about equitable outcomes with this new tech.\nI think we have a ways to go as a field, particularly with opening up details on training data. We also need to avoid a “race to the bottom,” just throwing more in more compute and data to beat benchmarks without understanding what we are scaling to. But if we want this new tech to do right by society at large, I think open models have a large role to play.\nAbheesht: Keras 3, i..e, multi-backend Keras which allows users to switch backends (TensorFlow, JAX, and PyTorch) was released to much hubbub and fanfare a couple of months ago. What do you think are some of the most exciting use cases of a multi-backend Keras? Why should people be excited about it?\nMatt: Maybe a bit abstract, but I think Keras has the opportunity to become a “lingua franca” for modeling problems for a lot more people with Keras 3. Keras is a familiar API to basically everyone in the ML field–even if you don’t realize it, its fingerprints are all over abstractions in so many deep learning toolkits.\nWith Keras 3, you can start with a backend agnostic modeling flow, and “lower” it into TensorFlow, JAX, or PyTorch anytime you need. There’s no transpilation, all layers/metrics/optimizers are using backend native calls for the selected framework. So basically Keras just becomes an incredibly efficient thing to learn. You can use it as a diving off point for basically any ML problem, and have the expressivity of a low-level framework as soon as you need it.\nAbheesht: What is one research paper you’ve read recently that gave you that “aha” feeling? Basically, a paper that blew your mind!\nMatt: A while ago I was in a paper reading group digging into graph neural networks. I found the notions of message passing and permutation invariance among graph vertices to be this big lightbulb moment where a lot of disparate notions in my head suddenly coalesced.\nBasically, you can think of CNNs and Transformers, and RNNs as graph networks. A convolution is just a message passing among neighboring pixels. Transformers are quite literally densely connected graph attention networks, with positional information added in.\nFraming everything as a graph gives you this uniform lens to look across the whole field of deep learning, which is so cool. This article is a good introduction to the idea.\nAnshuman & Abheesht: Thank you, Matt! It’s been great talking to you!\nAnd that concludes the third interview of our “AI Chronicles” series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP.\nBest regards and happy learning!"
  },
  {
    "objectID": "posts/semantic_sim_kerasnlp.html",
    "href": "posts/semantic_sim_kerasnlp.html",
    "title": "Predicting Semantic Similarity using KerasNLP",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXItdGl0bGU=\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6V29yaw==\"&gt;Work&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6L3BhZ2VzL3dvcmsuaHRtbA==\"&gt;/pages/work.html&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6QmxvZw==\"&gt;Blog&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6L2Jsb2cuaHRtbA==\"&gt;/blog.html&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly90d2l0dGVyLmNvbS9rYW5wdXJpeWFuYXdhYg==\"&gt;https://twitter.com/kanpuriyanawab&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL2Fuc2h1aXptZS8=\"&gt;https://www.linkedin.com/in/anshuizme/&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9naXRodWIuY29tL2thbnB1cml5YW5hd2Fi\"&gt;https://github.com/kanpuriyanawab&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly95b3V0dWJlLmNvbS9AMXNtb2xsY29kZXI=\"&gt;https://youtube.com/@1smollcoder&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9mdWxsc3RhY2thZ2VudHMuc3Vic3RhY2suY29t\"&gt;https://fullstackagents.substack.com&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6e3s8IGFpIGdvb2dsZS1zY2hvbGFyID59fQ==\"&gt;&lt;i class=\"ai  ai-google-scholar\" aria-label=\"google-scholar\"&gt;&lt;/i&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5mci9jaXRhdGlvbnM/dXNlcj03MHdoOWpjQUFBQUo=\"&gt;https://scholar.google.fr/citations?user=70wh9jcAAAAJ&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW1ldGF0aXRsZQ==\"&gt;Predicting Semantic Similarity using KerasNLP – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=\"&gt;Predicting Semantic Similarity using KerasNLP – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW9nY2FyZHRpdGxl\"&gt;Predicting Semantic Similarity using KerasNLP – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW1ldGFzaXRlbmFtZQ==\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==\"&gt;This report demonstrates how to use the Weights&amp;Biases to speedup experiments with Keras&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW9nY2FyZGRkZXNj\"&gt;This report demonstrates how to use the Weights&amp;Biases to speedup experiments with Keras&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\n  window.document.addEventListener(\"DOMContentLoaded\", function (event) {\n    const icon = \"\";\n    const anchorJS = new window.AnchorJS();\n    anchorJS.options = {\n      placement: 'right',\n      icon: icon\n    };\n    anchorJS.add('.anchored');\n    const isCodeAnnotation = (el) =&gt; {\n      for (const clz of el.classList) {\n        if (clz.startsWith('code-annotation-')) {                     \n          return true;\n        }\n      }\n      return false;\n    }\n    const onCopySuccess = function(e) {\n      // button target\n      const button = e.trigger;\n      // don't keep focus\n      button.blur();\n      // flash \"checked\"\n      button.classList.add('code-copy-button-checked');\n      var currentTitle = button.getAttribute(\"title\");\n      button.setAttribute(\"title\", \"Copied!\");\n      let tooltip;\n      if (window.bootstrap) {\n        button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n        button.setAttribute(\"data-bs-placement\", \"left\");\n        button.setAttribute(\"data-bs-title\", \"Copied!\");\n        tooltip = new bootstrap.Tooltip(button, \n          { trigger: \"manual\", \n            customClass: \"code-copy-button-tooltip\",\n            offset: [0, -8]});\n        tooltip.show();    \n      }\n      setTimeout(function() {\n        if (tooltip) {\n          tooltip.hide();\n          button.removeAttribute(\"data-bs-title\");\n          button.removeAttribute(\"data-bs-toggle\");\n          button.removeAttribute(\"data-bs-placement\");\n        }\n        button.setAttribute(\"title\", currentTitle);\n        button.classList.remove('code-copy-button-checked');\n      }, 1000);\n      // clear code selection\n      e.clearSelection();\n    }\n    const getTextToCopy = function(trigger) {\n        const codeEl = trigger.previousElementSibling.cloneNode(true);\n        for (const childEl of codeEl.children) {\n          if (isCodeAnnotation(childEl)) {\n            childEl.remove();\n          }\n        }\n        return codeEl.innerText;\n    }\n    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {\n      text: getTextToCopy\n    });\n    clipboard.on('success', onCopySuccess);\n    if (window.document.getElementById('quarto-embedded-source-code-modal')) {\n      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {\n        text: getTextToCopy,\n        container: window.document.getElementById('quarto-embedded-source-code-modal')\n      });\n      clipboardModal.on('success', onCopySuccess);\n    }\n      var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n      var mailtoRegex = new RegExp(/^mailto:/);\n        var filterRegex = new RegExp(\"https:\\/\\/www\\.heyyanshuman\\.com\");\n      var isInternal = (href) =&gt; {\n          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n      }\n      // Inspect non-navigation links and adorn them if external\n     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');\n      for (var i=0; i&lt;links.length; i++) {\n        const link = links[i];\n        if (!isInternal(link.href)) {\n          // undo the damage that might have been done by quarto-nav.js in the case of\n          // links that we want to consider external\n          if (link.dataset.originalHref !== undefined) {\n            link.href = link.dataset.originalHref;\n          }\n        }\n      }\n    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n      const config = {\n        allowHTML: true,\n        maxWidth: 500,\n        delay: 100,\n        arrow: false,\n        appendTo: function(el) {\n            return el.parentElement;\n        },\n        interactive: true,\n        interactiveBorder: 10,\n        theme: 'quarto',\n        placement: 'bottom-start',\n      };\n      if (contentFn) {\n        config.content = contentFn;\n      }\n      if (onTriggerFn) {\n        config.onTrigger = onTriggerFn;\n      }\n      if (onUntriggerFn) {\n        config.onUntrigger = onUntriggerFn;\n      }\n      window.tippy(el, config); \n    }\n    const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n    for (var i=0; i&lt;noterefs.length; i++) {\n      const ref = noterefs[i];\n      tippyHover(ref, function() {\n        // use id or data attribute instead here\n        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n        try { href = new URL(href).hash; } catch {}\n        const id = href.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note) {\n          return note.innerHTML;\n        } else {\n          return \"\";\n        }\n      });\n    }\n    const xrefs = window.document.querySelectorAll('a.quarto-xref');\n    const processXRef = (id, note) =&gt; {\n      // Strip column container classes\n      const stripColumnClz = (el) =&gt; {\n        el.classList.remove(\"page-full\", \"page-columns\");\n        if (el.children) {\n          for (const child of el.children) {\n            stripColumnClz(child);\n          }\n        }\n      }\n      stripColumnClz(note)\n      if (id === null || id.startsWith('sec-')) {\n        // Special case sections, only their first couple elements\n        const container = document.createElement(\"div\");\n        if (note.children && note.children.length &gt; 2) {\n          container.appendChild(note.children[0].cloneNode(true));\n          for (let i = 1; i &lt; note.children.length; i++) {\n            const child = note.children[i];\n            if (child.tagName === \"P\" && child.innerText === \"\") {\n              continue;\n            } else {\n              container.appendChild(child.cloneNode(true));\n              break;\n            }\n          }\n          if (window.Quarto?.typesetMath) {\n            window.Quarto.typesetMath(container);\n          }\n          return container.innerHTML\n        } else {\n          if (window.Quarto?.typesetMath) {\n            window.Quarto.typesetMath(note);\n          }\n          return note.innerHTML;\n        }\n      } else {\n        // Remove any anchor links if they are present\n        const anchorLink = note.querySelector('a.anchorjs-link');\n        if (anchorLink) {\n          anchorLink.remove();\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        if (note.classList.contains(\"callout\")) {\n          return note.outerHTML;\n        } else {\n          return note.innerHTML;\n        }\n      }\n    }\n    for (var i=0; i&lt;xrefs.length; i++) {\n      const xref = xrefs[i];\n      tippyHover(xref, undefined, function(instance) {\n        instance.disable();\n        let url = xref.getAttribute('href');\n        let hash = undefined; \n        if (url.startsWith('#')) {\n          hash = url;\n        } else {\n          try { hash = new URL(url).hash; } catch {}\n        }\n        if (hash) {\n          const id = hash.replace(/^#\\/?/, \"\");\n          const note = window.document.getElementById(id);\n          if (note !== null) {\n            try {\n              const html = processXRef(id, note.cloneNode(true));\n              instance.setContent(html);\n            } finally {\n              instance.enable();\n              instance.show();\n            }\n          } else {\n            // See if we can fetch this\n            fetch(url.split('#')[0])\n            .then(res =&gt; res.text())\n            .then(html =&gt; {\n              const parser = new DOMParser();\n              const htmlDoc = parser.parseFromString(html, \"text/html\");\n              const note = htmlDoc.getElementById(id);\n              if (note !== null) {\n                const html = processXRef(id, note);\n                instance.setContent(html);\n              } \n            }).finally(() =&gt; {\n              instance.enable();\n              instance.show();\n            });\n          }\n        } else {\n          // See if we can fetch a full url (with no hash to target)\n          // This is a special case and we should probably do some content thinning / targeting\n          fetch(url)\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.querySelector('main.content');\n            if (note !== null) {\n              // This should only happen for chapter cross references\n              // (since there is no id in the URL)\n              // remove the first header\n              if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n                note.children[0].remove();\n              }\n              const html = processXRef(null, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      }, function(instance) {\n      });\n    }\n        let selectedAnnoteEl;\n        const selectorForAnnotation = ( cell, annotation) =&gt; {\n          let cellAttr = 'data-code-cell=\"' + cell + '\"';\n          let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n          return selector;\n        }\n        const selectCodeLines = (annoteEl) =&gt; {\n          const doc = window.document;\n          const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n          const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n          const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n          const lineIds = lines.map((line) =&gt; {\n            return targetCell + \"-\" + line;\n          })\n          let top = null;\n          let height = null;\n          let parent = null;\n          if (lineIds.length &gt; 0) {\n              //compute the position of the single el (top and bottom and make a div)\n              const el = window.document.getElementById(lineIds[0]);\n              top = el.offsetTop;\n              height = el.offsetHeight;\n              parent = el.parentElement.parentElement;\n            if (lineIds.length &gt; 1) {\n              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n              const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n              height = bottom - top;\n            }\n            if (top !== null && height !== null && parent !== null) {\n              // cook up a div (if necessary) and position it \n              let div = window.document.getElementById(\"code-annotation-line-highlight\");\n              if (div === null) {\n                div = window.document.createElement(\"div\");\n                div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n                div.style.position = 'absolute';\n                parent.appendChild(div);\n              }\n              div.style.top = top - 2 + \"px\";\n              div.style.height = height + 4 + \"px\";\n              div.style.left = 0;\n              let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n              if (gutterDiv === null) {\n                gutterDiv = window.document.createElement(\"div\");\n                gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n                gutterDiv.style.position = 'absolute';\n                const codeCell = window.document.getElementById(targetCell);\n                const gutter = codeCell.querySelector('.code-annotation-gutter');\n                gutter.appendChild(gutterDiv);\n              }\n              gutterDiv.style.top = top - 2 + \"px\";\n              gutterDiv.style.height = height + 4 + \"px\";\n            }\n            selectedAnnoteEl = annoteEl;\n          }\n        };\n        const unselectCodeLines = () =&gt; {\n          const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n          elementsIds.forEach((elId) =&gt; {\n            const div = window.document.getElementById(elId);\n            if (div) {\n              div.remove();\n            }\n          });\n          selectedAnnoteEl = undefined;\n        };\n          // Handle positioning of the toggle\n      window.addEventListener(\n        \"resize\",\n        throttle(() =&gt; {\n          elRect = undefined;\n          if (selectedAnnoteEl) {\n            selectCodeLines(selectedAnnoteEl);\n          }\n        }, 10)\n      );\n      function throttle(fn, ms) {\n      let throttle = false;\n      let timer;\n        return (...args) =&gt; {\n          if(!throttle) { // first call gets through\n              fn.apply(this, args);\n              throttle = true;\n          } else { // all the others get throttled\n              if(timer) clearTimeout(timer); // cancel #2\n              timer = setTimeout(() =&gt; {\n                fn.apply(this, args);\n                timer = throttle = false;\n              }, ms);\n          }\n        };\n      }\n        // Attach click handler to the DT\n        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n        for (const annoteDlNode of annoteDls) {\n          annoteDlNode.addEventListener('click', (event) =&gt; {\n            const clickedEl = event.target;\n            if (clickedEl !== selectedAnnoteEl) {\n              unselectCodeLines();\n              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n              if (activeEl) {\n                activeEl.classList.remove('code-annotation-active');\n              }\n              selectCodeLines(clickedEl);\n              clickedEl.classList.add('code-annotation-active');\n            } else {\n              // Unselect the line\n              unselectCodeLines();\n              clickedEl.classList.remove('code-annotation-active');\n            }\n          });\n        }\n    const findCites = (el) =&gt; {\n      const parentEl = el.parentElement;\n      if (parentEl) {\n        const cites = parentEl.dataset.cites;\n        if (cites) {\n          return {\n            el,\n            cites: cites.split(' ')\n          };\n        } else {\n          return findCites(el.parentElement)\n        }\n      } else {\n        return undefined;\n      }\n    };\n    var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n    for (var i=0; i&lt;bibliorefs.length; i++) {\n      const ref = bibliorefs[i];\n      const citeInfo = findCites(ref);\n      if (citeInfo) {\n        tippyHover(citeInfo.el, function() {\n          var popup = window.document.createElement('div');\n          citeInfo.cites.forEach(function(cite) {\n            var citeDiv = window.document.createElement('div');\n            citeDiv.classList.add('hanging-indent');\n            citeDiv.classList.add('csl-entry');\n            var biblioDiv = window.document.getElementById('ref-' + cite);\n            if (biblioDiv) {\n              citeDiv.innerHTML = biblioDiv.innerHTML;\n            }\n            popup.appendChild(citeDiv);\n          });\n          return popup.innerHTML;\n        });\n      }\n    }\n  });\n  &lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/interview_rajaswa.html",
    "href": "posts/interview_rajaswa.html",
    "title": "Interview with Rajaswa Patil, AI Researcher at Postman Labs",
    "section": "",
    "text": "by Anshuman Mishra and Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard of) stories out into the world — their struggles, their triumphs and their journey. We will be conversing with people we have looked up to, and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nRajaswa’s Stellar Profile Today, we are honored to have Rajaswa Patil with us. Rajaswa, who graduated in 2021 from BITS Goa, has already had a chequered career so far. He started with a Pre-doctoral position at TCS Research, before working at Microsoft as a Research Fellow. He now works at Postman Labs as an AI Research Associate. Rajaswa is leveraging Generative AI and Large Language Models (LLMs) to build “Postbot” at Postman.\nRajaswa is one of those people with whom you can never have a meaningless conversation. He is a cornucopia of knowledge, and always has tidbits of useful information to give out, not just restricted to AI or tech. Before he joined the industry, Rajaswa introduced a research culture in NLP at BITS Goa. He is one of the reasons why I, Abheesht, was able to get out of my initial failures in research and actually get some papers published during my undergrad.\nLet’s begin!\nAnshuman & Abheesht: Hello, Rajaswa! Thank you for doing this!\nRajaswa: Hi, thanks for having me!\nAbheesht: Before we even start this interview, I want to get one heavy question out of the way 😂. What is your opinion on whether LLMs are sentient? Do you think LLMs are intelligent? Which side of the debate are you on?\nRajaswa: I don’t have a strong opinion on this matter, as I find it challenging to provide precise definitions for terms like “sentient” and “intelligent.” The complexity deepens when we consider the broader concept of Artificial General Intelligence (AGI).\nTo gain a more informed perspective on sentience and AGI, I’ve been delving into a book called “Superintelligence” by Nick Bostrom. I highly recommend this book to anyone curious about these questions. It offers valuable insights into the subject.\nOne point we can generally agree upon is the potential for computational modeling of human-like intelligence in the future. Human intelligence is the result of millions of years of evolutionary computation, providing us with a relatively intelligent blank slate at birth, which then undergoes further training to reach adult human-level intelligence. In many ways, we’re already making significant strides in engineering computational systems that can provide a head start equivalent to millions of years of evolution in just a month of model pre-training.\nHowever, there are still significant challenges to overcome. Limitations in computational power, data availability, data modalities (which may require more sophisticated model architectures), and other factors are currently hindering our progress. Nevertheless, we’re making rapid advancements toward achieving human-level intelligence in machines.\nAnshuman: You’ve done your Bachelor’s in Electrical Engineering. What brought you to data science and ML? Please tell the reader about your Machine Learning journey!\nRajaswa: My journey in engineering began in 2017, and my passion for Computer Science was evident from the start. However, despite my enthusiasm, I couldn’t secure a spot in a prestigious Computer Science program at any institute. I faced a difficult choice between the branch of study and the quality of peers and the prevailing “coding culture.” In the end, I made the decision to pursue Electrical Engineering at BITS Goa over Computer Science/Information Technology programs at institutions like COEP Pune and IIIT Delhi. What influenced my choice was BITS Goa’s remarkable achievement of having the highest number of Google Summer of Code (GSoC) selections across the country in 2017, with many of those selections coming from students with non-CS backgrounds. This spoke volumes about the thriving tech culture at BITS Goa.\nThe year 2017 marked the rapid rise of Machine Learning and Deep Learning among the tech community. Platforms like Kaggle and Google Colab were emerging, creating new opportunities. Two key factors drew me towards Data Science and Machine Learning during this period. First, there were on-campus courses and bootcamps taught by senior students through programs like CTE or QSTP, which provided valuable insights into these fields. Second, my brother served as a project mentor for Udacity’s Self-Driving Car Nanodegree program, introducing me to the fascinating world of AI and its potential for lucrative careers. This initial exposure piqued my interest, and as I explored further, my fascination with these fields deepened.\nI made a bold decision during this time by forgoing a summer internship offered by BITS in favor of a Summer Internship at IIT Bombay, where I worked on Information Retrieval and Natural Language Processing (NLP). This experience opened my eyes to the significance of research papers in the field of AI, and I learned about prestigious conferences like ACL and EMNLP. I returned from IIT Bombay with a strong determination to publish my own research and started participating in shared-tasks like SemEval. In 2020, our efforts resulted in the publication of three papers. This success led to the establishment of the Language Research Group (LRG) at BITS Goa.\nNLP was not a popular choice at the time, as it was more challenging to program compared to some other fields, and it lacked the visual appeal of areas like Computer Vision with GANs and Image Segmentation. However, our unique focus on NLP gave us a distinct advantage. The Language Research Group thrived, and we all built impressive profiles for ourselves in the field, thanks to our work in NLP.\nAbheesht: In my conversations with you back in 2021, I remember how eager you were to take up a PhD in Computational Linguistics. What made you change your mind? Why did you decide to ply your trade in the tech industry over choosing a career in academia? Have you completely closed the door on doing a PhD in the future?\nRajaswa: Indeed, I had been contemplating pursuing a PhD since my sophomore year. Teaching has always been a passion of mine, and I could envision myself in academia. However, I made the choice to remain in the industry for several reasons, a mix of personal and professional considerations.\nOn a personal level, I had originally envisioned a future where I would build my academic career in the West while also starting a family. Unfortunately, a series of unforeseen events led me to reconsider and ultimately give up on that dream. It became clear to me that academia, regardless of the location, would impose significant limitations on both my social and financial freedom. This realization played a significant role in shaping my decision to remain in the industry.\nWhen I turned 23, my friends gifted me “The Almanack Of Naval Ravikant”, and within its pages, I stumbled upon a quote that left a profound impact on me, particularly concerning Applied Scientists:\nSociety, business & money are downstream of technology, which is itself downstream of science. Science applied is the engine of humanity.\nCorollary: Applied Scientists are the most powerful people in the world. This will be more obvious in coming years.\nIt doesn’t take a genius to recognize that AI is poised to become one of the most significant advancements of the 21st century. This quote forced me to reevaluate how I could position myself in the midst of this historic progression. At the time, I had been immersed in the AI4Code domain since my graduation, and it was evident that this field was already undergoing substantial transformations with the emergence of groundbreaking tools like Copilot during the Large Language Model era. The evidence was right before my eyes, affirming the relevance and power of applied scientists in shaping the future.\nDuring that period, I was actively preparing for my PhD applications, juggling tasks like publishing papers at TCS Research and taking exams like GRE and TOEFL. It was during this time that Sumit Gulwani, a luminary in the field of AI4Code, approached me with an intriguing opportunity: a Research Fellowship at his team, Microsoft PROSE. This offer presented an industry alternative to pursuing a PhD. It entailed the possibility of joining the team as a Research Fellow, followed by a role as a Research Associate for 3–4 years, with a smooth transition into a full-time Scientist position, all without the need for a Master’s or PhD. This proposal put me in a considerable dilemma because, by then, I had more or less ruled out the traditional academic path and was leaning towards an industry career. Pursuing a PhD for an industry position seemed less attractive in comparison to this enticing offer.\nDespite my confusion, I sought counsel from a wide range of individuals with diverse backgrounds and varying levels of experience. However, even after these discussions, I found it challenging to make a decision. Eventually, I chose to move forward with an industry career primarily because it offered greater flexibility compared to the commitment required for a PhD.\nMy perspective, then and now, is that the industry holds immense potential for value creation. I believe this trend will continue for the next few years. With substantial investments from Big Tech companies, investors, and governments pouring into AI, there is no doubt that the industry is where the action is. Many top AI/non-AI academics worldwide are either launching startups, contributing to open-source projects, or collaborating with big tech firms and governments to apply AI in practical contexts. It’s evident that most major advancements, whether theoretical or practical, are emerging from the industry. This is because the industry boasts the necessary resources and motivation.\nAs of today, this is where I want to be. However, I haven’t entirely abandoned my PhD aspirations. I do envision pursuing a PhD at some point, but it will likely be a part-time endeavor and may not necessarily be in Computer Science, although it will remain closely related to AI. I anticipate that this plan will materialize several years down the line, possibly 5 to 6 years from now, at the earliest.\nAbheesht: Your journey so far has been inspirational. You come from a third tier town, and did not have the opportunities one has in bustling, metropolitan cities. You fought all the odds, made it to a reputable college, performed research and now work at an enviable position at Postman. Tell us a bit more about your struggles, say, the culture shock you faced when you joined college, or how you gained the confidence to rub shoulders with the best in the business.\nRajaswa: Honestly, I’ve been incredibly fortunate in my life journey. Despite hailing from a Tier-3 city, I’ve had the privilege of a strong educational foundation — a bit like being a big fish in a small pond. My father holds a PhD in Biotechnology, and my mother earned a Master’s degree in Organic Chemistry. Moreover, my older brother, who is five years my senior, has always been a trailblazer for me. Whether it was excelling in the Joint Entrance Examination (JEE) or venturing into Software Engineering and Machine Learning, he led the way, providing me with invaluable guidance based on his experiences.\nHowever, when I entered college, especially during my first year, I experienced a significant culture shock. I quickly realized that people judged you based on your manner of speaking, your choice of clothing, and various other factors. BITS Goa was not home to many Tier-3 city students. In fact, I’m quite certain that over 70% of the student body came from India’s major metropolitan areas, with a notably Westernized mindset. This was a stark departure from the slightly more conservative culture I was accustomed to in Tier-3 cities in India. I was also highly introverted during this phase. In fact, during my first semester, I hardly left the campus at all. For the initial couple of months, I didn’t even venture across to the A-wing of the campus. I didn’t have much disposable income, I didn’t indulge in drinking or openly participate in parties and celebrations — factors that significantly impacted my social life. To compound matters, I missed my mid-semester examinations due to a family medical emergency and found myself falling behind in all my courses.\nIt was an undeniably challenging period. However, things began to change when I made the decision to forego a summer internship opportunity and instead pursued an off-campus internship that allowed me to return, publish research papers, and start earning money through internship or project stipends. This shift helped me gain recognition on campus. Part of it stemmed from the fact that I became known as the person who could assist in securing research papers for MS applications or help others connect with top professors for projects and reading courses. With the establishment of the Language Research Group (LRG) and my role in teaching ML courses at CTE and QSTP, my social life improved, as did my self-confidence. Over time, I ended up assisting numerous individuals, making my on-campus experience quite rewarding. It became common for people to greet me with a smile of acknowledgment as we passed one another on walkways and in corridors.\nEven today, when I’m out in Bengaluru, it’s a delightful surprise to be recognized by people, primarily BITSian juniors, who remember me from somewhere, despite the fact that I believe I haven’t achieved anything extraordinary to warrant such recognition.\nAbheesht: I want to talk a bit more about how giving you’ve been to the community. You’ve never “gatekept” the knowledge you’ve had. A good example of this is when you formed the Language Research Group (LRG) at BITS Goa. What motivates you to keep giving back to the community?\nRajaswa: My passion for teaching is quite straightforward. Additionally, I am a staunch advocate of the “Feynman Technique.” I truly solidified my understanding of Machine Learning theory when I had the opportunity to teach it to my junior peers. Notably, my first significant paper was published with me as the last author, serving as a supervisor without direct contributions. This teaching and mentoring role has proven to be an invaluable professional growth experience for me, providing a natural incentive to continue.\nIn a broader context, I view education as an arena where one can achieve “low-effort high-impact” outcomes. Consequently, I consistently invest my time and resources in educational pursuits, whether it’s for the betterment of society or to serve my own personal interests. I actively engage in both aspects, finding them equally rewarding and fulfilling.\nAnshuman: Could you tell us more about your role at Postman Labs and how a day looks like?\nRajaswa: My current role is nothing short of incredible. I’m a part of the Labs team, where we operate in a zero-to-one environment, emphasizing rapid experimentation and swift product deployment. At present, I’m the sole core-AI specialist within the team, which is composed of remarkable talents who inspire me daily. Among them are Abhijit Kane, the co-founder of Postman, and Shamasis Bhattacharya, who serves as my manager and is the Head of Labs. Alongside these brilliant minds, we have a dedicated team of engineers, designers, and data analysts. In many ways, I have all the essential elements within my team to transform the product of my imagination into a reality.\nMy role is exceptionally diverse, allowing me to engage in various facets of the product development process, including research, design, analytics, and even occasional contributions to our backend services. Given my unique position as the sole core-AI member, I also play a crucial role in cultivating an AI culture within Postman. This involves sharing daily updates on AI developments, organizing invited talks, and facilitating communication with external teams.\nThe discussions we have within the team are remarkably ambitious, challenging me to adopt new perspectives and ways of thinking to achieve goals on a larger scale. My work primarily involves researching new directions for our product, particularly Postbot, and I take great pride in contributing to Postman’s broader objectives.\nAbheesht: LLMs are prone to hallucination, prompt injection, etc. How do you deal with these issues? Are LLMs reliable enough in production?\nRajaswa: To mitigate potential attacks or abuses of the product, it’s essential to engineer specific guardrails that closely monitor either the input or output to the system, as well as user behavior patterns. It’s worth noting that large language models (LLMs) are susceptible to generating incorrect or “hallucinated” content, which can occur periodically in most products. While there isn’t a perfect solution available at this time, one temporary measure is to make the system as human-in-the-loop (HITL) as possible. This entails providing the system with capabilities for recording consent, gathering user feedback, and enabling correction mechanisms. The primary objective is not to create a flashy system but to develop a practical and usable one, doing so as quickly as possible.\nAnshuman : AI is moving at breakneck pace. How do you stay up to date with cutting edge?\nRajaswa: To be honest, I don’t exert an immense effort to stay updated, but I have some effective strategies in place. I maintain a dedicated Twitter list focused on AI development, which I monitor daily. My LinkedIn feed has also become a valuable source of relevant updates. I subscribe to several newsletters and mailing lists, primarily related to the research community. Additionally, I’m an active participant in various communities on platforms like WhatsApp, Slack, and Discord. These communities provide valuable insights, and I engage actively in discussions within some of them.\nAbheesht: You’ve worked in several research labs. How has working in research labs been like for you? How do labs in India compare with labs in the US?\nRajaswa: My experience with Indian research labs, in general, has been less than stellar, and I’ve noticed that many of my peers share similar sentiments. However, it’s worth noting that one potential upside to these experiences is that the learning curve can often be steeper, which can ultimately benefit you in the long term. I haven’t had the opportunity to work with any research labs in the United States, so I can’t provide a direct comparison in that regard.\nAbheesht: What suggestions/advice would you give someone who has just started learning/working on ML?\nRajaswa: Begin your AI journey by diving into building. Today, with the abundance of APIs available, you can construct and showcase an end-to-end working system swiftly. Simultaneously, explore the intricacies of machine learning on the side.\nI firmly believe that with your programming skills, you can assemble roughly 80% of any practical AI system. The remaining 20% can be tackled with in-depth ML knowledge, which will empower you to debug and optimize your creations. However, given the rapid pace of AI development, I don’t think your primary objective should be focusing on that final 20%. Instead, prioritize getting started with building as soon as possible.\nAnshuman & Abheesht: Thank you, Rajaswa! It’s been great talking to you!\nAnd that concludes the second interview of our “AI Chronicles” series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "posts/julia_house_price_prediction.html",
    "href": "posts/julia_house_price_prediction.html",
    "title": "Comprehensive data exploration with Julia",
    "section": "",
    "text": "Source : Analytics India MagazineIn this tutorial, we’ll learn using Julia for exploratory data analysis. This is my first blog post ever and first article of my series Julia For The Win. Please feel free to give feedback ! Let’s get started. In this tutorial we’ll be reproducing Comprehensive data exploration with Python notebook, but in Julia.\nDataset used in this tutorial can be found here.\nWe’ll be using following packages:\nAbove packages can be installed by using the following block of code (we take PlotlyJS for example)\nThis quote belongs to Thales of Miletus. Thales was a Greek/Phonecian philosopher, mathematician and astronomer, which is recognised as the first individual in Western civilisation known to have entertained and engaged in scientific thought (source: https://en.wikipedia.org/wiki/Thales)\nI wouldn’t say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, it’s easy to overlook this initial step and jump too soon into the water.\nSo I tried to learn how to swim before jumping into the water. Based on Hair et al. (2013), chapter ‘Examining your data’, I did my best to follow a comprehensive, but not exhaustive, analysis of the data. I’m far from reporting a rigorous study in this kernel, but I hope that it can be useful for the community, so I’m sharing how I applied some of those data analysis principles to this problem.\nDespite the strange names I gave to the chapters, what we are doing in this kernel is something like:\nNow, it’s time to have fun!\nDataFrames.jl is the stable alternative of Pandas in Julia. Its design and functionality are similar to those of pandas (in Python) and data.frame, data.table and dplyr (in R), making it a great general purpose data science tool.\nTo peek into the dataframe, Julia alternative of head() method is first"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#so-what-can-we-expect",
    "href": "posts/julia_house_price_prediction.html#so-what-can-we-expect",
    "title": "Comprehensive data exploration with Julia",
    "section": "So… What can we expect?",
    "text": "So… What can we expect?\nIn order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\nIn order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\nVariable - Variable name.\nType - Identification of the variables’ type. There are two possible values for this field: ‘numerical’ or ‘categorical’. By ‘numerical’ we mean variables for which the values are numbers, and by ‘categorical’ we mean variables for which the values are categories.\nSegment - Identification of the variables’ segment. We can define three possible segments building, space or location. When we say ‘building’, we mean a variable that relates to the physical characteristics of the building (e.g. ‘OverallQual’). When we say ‘space’, we mean a variable that reports space properties of the house (e.g. ‘TotalBsmtSF’). Finally, when we say a ‘location’, we mean a variable that gives information about the place where the house is located (e.g. ‘Neighborhood’).\nExpectation - Our expectation about the variable influence in ‘SalePrice’. We can use a categorical scale with ‘High’, ‘Medium’ and ‘Low’ as possible values.\nConclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in ‘Expectation’.\nComments - Any general comments that occured to us.\n\nWhile ‘Type’ and ‘Segment’ is just for possible future reference, the column ‘Expectation’ is important because it will help us develop a ‘sixth sense’. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\n\nDo we think about this variable when we are buying a house? (e.g. When we think about the house of our dreams, do we care about its ‘Masonry veneer type’?).\nIf so, how important would this variable be? (e.g. What is the impact of having ‘Excellent’ material on the exterior instead of ‘Poor’? And of having ‘Excellent’ instead of ‘Good’?).\nIs this information already described in any other variable? (e.g. If ‘LandContour’ gives the flatness of the property, do we really need to know the ‘LandSlope’?).\n\nAfter this daunting exercise, we can filter the spreadsheet and look carefully to the variables with ‘High’ ‘Expectation’. Then, we can rush into some scatter plots between those variables and ‘SalePrice’, filling in the ‘Conclusion’ column which is just the correction of our expectations.\nI went through this process and concluded that the following variables can play an important role in this problem:\n\nOverallQual (which is a variable that I don’t like because I don’t know how it was computed; a funny exercise would be to predict ‘OverallQual’ using all the other variables available).\nYearBuilt.\nTotalBsmtSF.\nGrLivArea.\n\nI ended up with two ‘building’ variables (‘OverallQual’ and ‘YearBuilt’) and two ‘space’ variables (‘TotalBsmtSF’ and ‘GrLivArea’). This might be a little bit unexpected as it goes against the real estate mantra that all that matters is ‘location, location and location’. It is possible that this quick data examination process was a bit harsh for categorical variables. For example, I expected the ‘Neigborhood’ variable to be more relevant, but after the data examination I ended up excluding it. Maybe this is related to the use of scatter plots instead of boxplots, which are more suitable for categorical variables visualization. The way we visualize data often influences our conclusions.\nHowever, the main point of this exercise was to think a little about our data and expectactions, so I think we achieved our goal. Now it’s time for ‘a little less conversation, a little more action please’. Let’s shake it!\n\n Subscribe"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#analysing-saleprice",
    "href": "posts/julia_house_price_prediction.html#analysing-saleprice",
    "title": "Comprehensive data exploration with Julia",
    "section": "Analysing SalePrice",
    "text": "Analysing SalePrice\n‘SalePrice’ is the reason of our quest. It’s like when we’re going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)\nUsing the women analogy, let’s build a little story, the story of ‘How we met ’SalePrice’.\nEverything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. That’s a sign that she’s there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:\n‘Hi, I’m Kaggly! And you? ’SalePrice’? What a beautiful name! You know ‘SalePrice’, could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. I’d like to apply it to us!’\ndescribe(df_train, cols=:SalePrice)\n\nVery well… It seems that your minimum price is larger than zero. Excellent! You don’t have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I don’t know… like, you in the beach… or maybe a selfie in the gym?\nplot(df_train, x=:SalePrice, kind=\"histogram\")\n\nAh! I see you that you use seaborn makeup when you’re going out… That’s so elegant! I also see that you:\n\nDeviate from the normal distribution\nHave appreciable positive skewness\nShow peakedness\n\nThis is getting interesting! ‘SalePrice’, could you give me your body measures?\nusing StatsBase\nskw, kurt = skewness(collect(df_train.SalePrice)), kurtosis(collect(df_train.SalePrice))\nprintln(\"Skewness: $skw \\nKurtosis: $kurt\")\nThis prints:\nSkewness: 1.8809407460340335 \nKurtosis: 6.509812011089398\nAmazing! If my love calculator is correct, our success probability is 97.834657%. I think we should meet again! Please, keep my number and give me a call if you’re free next Friday. See you in a while, crocodile!\n‘SalePrice’, her buddies and her interests\nIt is military wisdom to choose the terrain where you will fight. As soon as ‘SalePrice’ walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. It’s just an intense research of an individual, if you know what I mean.\nAccording to her profile, we have some common friends. Besides Chuck Norris, we both know ‘GrLivArea’ and ‘TotalBsmtSF’. Moreover, we also have common interests such as ‘OverallQual’ and ‘YearBuilt’. This looks promising!\nTo take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests. Relationship with numerical variables\nplot(scatter(df_train, x=:GrLivArea, y=:SalePrice, mode=\"markers\"))\n\nHmmm… It seems that ‘SalePrice’ and ‘GrLivArea’ are really old friends, with a linear relationship.\nAnd what about ‘TotalBsmtSF’?\nplot(scatter(df_train, x=:TotalBsmtSF, y=:SalePrice, mode=\"markers\"))\n\n‘TotalBsmtSF’ is also a great friend of ‘SalePrice’ but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. Moreover, it’s clear that sometimes ‘TotalBsmtSF’ closes in itself and gives zero credit to ‘SalePrice’."
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#relationship-with-categorical-features",
    "href": "posts/julia_house_price_prediction.html#relationship-with-categorical-features",
    "title": "Comprehensive data exploration with Julia",
    "section": "Relationship with categorical features",
    "text": "Relationship with categorical features\nplot(df_train, x=:OverallQual, y=:SalePrice, color=:OverallQual, kind=\"box\")\n\nLike all the pretty girls, ‘SalePrice’ enjoys ‘OverallQual’. Note to self: consider whether McDonald’s is suitable for the first date.\nplot(df_train, x=:YearBuilt, y=:SalePrice, color=:OverallQual, kind=\"box\")\n\nAlthough it’s not a strong tendency, I’d say that ‘SalePrice’ is more prone to spend more money in new stuff than in old relics.\nNote: we don’t know if ‘SalePrice’ is in constant prices. Constant prices try to remove the effect of inflation. If ‘SalePrice’ is not in constant prices, it should be, so than prices are comparable over the years.\n\n Subscribe"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#in-summary",
    "href": "posts/julia_house_price_prediction.html#in-summary",
    "title": "Comprehensive data exploration with Julia",
    "section": "In summary",
    "text": "In summary\nStories aside, we can conclude that: - ‘GrLivArea’ and ‘TotalBsmtSF’ seem to be linearly related with ‘SalePrice’. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of ‘TotalBsmtSF’, we can see that the slope of the linear relationship is particularly high. - ‘OverallQual’ and ‘YearBuilt’ also seem to be related with ‘SalePrice’. The relationship seems to be stronger in the case of ‘OverallQual’, where the box plot shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\nThat said, let’s separate the wheat from the chaff.\nAs an engineer, I don’t feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. There’s a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.\nSo, let’s overcome inertia and do a more objective analysis.\nThe ‘plasma soup’\n‘In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.’ (source: http://umich.edu/~gs265/bigbang.htm)\nTo explore the universe, we will start with some practical recipes to make sense of our ‘plasma soup’:\n\nCorrelation matrix (heatmap style).\n‘SalePrice’ correlation matrix (zoomed heatmap style).\nScatter plots between the most correlated variables (move like Jagger style).\n\nFor heatmaps, I found JuliaPlots to be more userfriendly than PlotlyJS.jl\nimport Plots.heatmap as ht \n# alias because Plotly namescope conflicts with Plots.jl\n\nusing Statistics: cor \n# correlation matrix\n\navoid = names(df_train, String)\n\n# avoiding columns with string values : raises error\ndf_non_str = select(df_train, Not(avoid));\nco = cor(Matrix(df_non_str));\n\nnot_avoid = names(df_train, Not(avoid));\n\nht(co, xticks=(1:35, not_avoid), yticks=(1:35, not_avoid), \n    aspect_ratio=:equal, fill_z=co, xrotation=90, xtickfontsize=5, \n    ytickfontsize=5)\n\nAccording to our crystal ball, these are the variables most correlated with ‘SalePrice’. My thoughts on this:\n\n‘OverallQual’, ‘GrLivArea’ and ‘TotalBsmtSF’ are strongly correlated with ‘SalePrice’. Check!\n‘GarageCars’ and ‘GarageArea’ are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. ‘GarageCars’ and ‘GarageArea’ are like twin brothers. You’ll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep ‘GarageCars’ since its correlation with ‘SalePrice’ is higher).\n‘TotalBsmtSF’ and ‘1stFloor’ also seem to be twin brothers. We can keep ‘TotalBsmtSF’ just to say that our first guess was right (re-read ‘So… What can we expect?’).\n‘FullBath’?? Really?\n‘TotRmsAbvGrd’ and ‘GrLivArea’, twin brothers again. Is this dataset from Chernobyl?\nAh… ‘YearBuilt’… It seems that ‘YearBuilt’ is slightly correlated with ‘SalePrice’. Honestly, it scares me to think about ‘YearBuilt’ because I start feeling that we should do a little bit of time-series analysis to get this right. I’ll leave this as a homework for you.\n\nLet’s proceed to the pair plots.\nPair plots between ‘SalePrice’ and correlated variables (move like Jagger style)\ncorner(df_train[:, cols])\n\nAlthough we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\nOne of the figures we may find interesting is the one between ‘TotalBsmtSF’ and ‘GrLiveArea’. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you’re trying to buy a bunker).\nThe plot concerning ‘SalePrice’ and ‘YearBuilt’ can also make us think. In the bottom of the ‘dots cloud’, we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the ‘dots cloud’ (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\nPheww ! That’s it for today. Today we took a look at how cool Julia is and why can it rescue data scientists just in case Python disappears from this earth !\n\n\n Subscribe"
  },
  {
    "objectID": "posts/interview_rrk.html",
    "href": "posts/interview_rrk.html",
    "title": "Interview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD - Ravi Ramakrishnan",
    "section": "",
    "text": "by Anshuman Mishra & Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard of) stories out into the world — their struggles, their triumphs and their journey. We will be conversing with people we have looked up to, and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nKaggle Profile of 2xGrandmaster Ravi Ramakrishnan Today, we are honored to have Ravi Ramakrishnan with us. Ravi is a Data Science Manager-Credit Risk at Emirates NBD. Prior to Emirates NBD, he used to work at the Commercial Bank of Dubai as a Data Scientist, and has 8+ years of experience in the industry. He holds a Bachelor’s degree in Electronics, and completed his MBA in Finance from NMIMS. Ravi contributes extensively on Kaggle, and is a Notebooks Grandmaster (ranked #23) and a Discussions Grandmaster (ranked #2) on Kaggle.\nAnshuman & Abheesht : Namaste, Grandmaster! Thank you for taking the time to do this.\nRavi: Hello, Anshuman and Abheesht. Thanks for inviting me to do this. Saadar pranaam!\nAnshuman: You’ve done your Bachelor’s in Electrical Engineering, post which you did an MBA. What brought you to data science and ML? Please tell the reader about your Machine Learning journey!\nRavi: Absolutely, I’d be happy to share my journey. While my academic background is in electrical engineering (BE) and I later pursued an MBA, my entry into the world of data science was somewhat serendipitous. Back in 2015, during a visit to Dubai for my CFA Level 3 exam, I had the opportunity to attend interviews with my previous employer, who was in the process of establishing a central analytics structure known as BICC. To my surprise, I was selected for the role of a data scientist, even though my previous focus had been on finance and investment banking. I decided to take up the challenge, giving myself a year to assess my career path. My initial days in this role were demanding as I had to bridge the gap in my knowledge of data science tools like SQL, SAS, MATLAB, SPSS, EDW, and CRM development. However, with dedication, guidance from colleagues, and consistent learning, I quickly overcome these challenges.\nOver the years, I became deeply involved in model development across various domains, including customer segmentation, CRM lead management, IFRS9 PD-LGD model development, ECL calculation engine development, and campaign management. This journey of hands-on experience and continuous learning led me to discover my passion for data science.\nWhen I look back, I believe that my career in data science found me rather than the other way around. It’s been a rewarding journey of almost a decade, and I’m excited about what the future holds. I hope my experience can inspire others to explore and embrace this dynamic field of data science and machine learning.\nAnshuman: Given your serendipitous journey into data science and your passion for continuous learning, I’m curious to know: what specifically inspired you to pick up Kaggle and dive into the world of machine learning competitions?\nRavi: My yester manager introduced me to Kaggle in 2020 during the lockdown. I was busy with some certifications in fitness then and promised him to have a look at it after passing those exams. I eventually joined Kaggle in September 2021 after completing my Orthopedics specialization. I thank him for introducing me to Kaggle as I feel I would have missed out on a very important component of my learning journey had I not joined Kaggle. I perused the tiers on this platform slowly and steadily and established a learning and development plan for myself akin to a fitness plan. I usually advise my patrons to follow a macro-meso-micro fitness periodization cycle with planned gains and achievements all along the journey. I followed my fitness regimen in Kaggle, focussing on small but consistent efforts on a daily basis. I set realistic goals for myself and consistently achieved them, else reset the goals apropos to contemporary situations, keeping a balance between Kaggle, fitness, running and office commitments.\nAbheesht: What was your motivation behind starting with the discussions track as opposed to say, the competitions track on Kaggle?\nRavi: My motivation to excel in the discussions track on Kaggle, ultimately achieving the status of Grandmaster, was driven by several factors. Firstly, discussions on Kaggle offer a unique platform for knowledge exchange and collaboration with fellow machine learning enthusiasts. I was drawn to the open and creative environment that discussions provided, offering an opportunity to engage meaningfully with the community. Additionally, active participation in discussions allowed me to continuously learn and stay updated with the latest trends and techniques in the field. It was a way for me to both contribute to the community and absorb insights from others, contributing to my own growth as a data scientist.\nFinally, achieving Grandmaster status in discussions represented a significant milestone in my Kaggle journey, reflecting my commitment to mastering various aspects of Kaggle. It served as a testament to my dedication to sharing knowledge, fostering collaboration, and contributing to the thriving Kaggle community.\nAbheesht: Given your demanding role as a Data Science manager, how do you effectively allocate time to maintain an active presence on Kaggle? This is something I struggle a bit with — managing time between work, and open source.\nRavi: Managing my role as a Data Science manager at Emirates Bank while maintaining an active presence on Kaggle requires effective time management and discipline. I’ve structured my daily routine to strike a balance between work and Kaggle activities.\nMy day is divided into distinct parts: 1. Morning (6 am — 8 am): Dedicated to exercise, ensuring physical well-being. 2. 8:30 am — 6 pm: Focused on my responsibilities at the bank. 3. 6 pm — 9:30 pm: Allocated for Kaggle activities, where I participate in discussions, competitions, and kernels. 4. 9:30 pm — 10:00 pm: Reserved for meditation and mindfulness activities to maintain mental clarity.\nThis disciplined approach allows me to optimize my time efficiently. I also prioritize health through mindfulness practices, exercise, and a balanced diet, which helps maintain physical and mental well-being. By adhering to this routine, I can effectively manage my demanding work and Kaggle commitments.\nAnshuman: People participating in the discussion track on Kaggle are very opinionated, and most competition grandmasters, from time to time, appeal to remove it from a ranking tier. What is your opinion on this?\nRavi: I respect their opinions but wish to mention that the discussion track is akin to a glue that binds the community together. Everyone can’t be a competition participant but quite a few wish to learn as well. The discussion track posits some valuable information regarding general ML practices, MLOps, coding elements, competition solutions, resources, career advice, progression appreciation and key industry updates and provides valuable information for one and all, especially beginners to onboard, learn and develop appropriately. I wish to opine that we have 300+ competition GMs as on date and 66–67 discussion GMs too. It is far more difficult to progress in this tier than people’s general fads. One needs to be good at communication and present one’s content well to secure any form of progress in this tier.\nAnshuman: Could you please share a few occasions when you made valuable connections or benefitted in great deal from it?\nRavi: I have learnt a lot from elite Kaggle users throughout my tryst with Kaggle. Users like AmbrosM, Marialla Pratta, Dr. Chris Deotte, Laurent Purchout, Dr. Carl McBridde Ellis, Sanyam Bhutani, Parul Pandey, SRK, Rohan Rao, etc. will always be my inspiration on Kaggle and even otherwise. I have learnt a lot from their Kaggle and other work and will consider them my mentors always. I wish to thank you as well for connecting with me and providing me a chance to elicit my thoughts on this topic.\nAbheesht: You are very fond of Kaggle’s playground series. Why do you prefer it over other competitions on Kaggle?\nRavi: I like to participate in the playground series due to the below reasons-\nThis series offers room to experiment with feature engineering and models to a great extent. Datasets are simpler than featured competitions and hardware requirements are tepid. One could ace these competitions using the freely available resources on Kaggle and Colab without any undue advantage based on hardware. I opine that the select few participants in other featured competitions category using professional login Ids and having access to premium resources do carry a sizable advantage over the rest, rendering these competitions a no-so-level-playing-field. This issue is circumvented in this series almost entirely. I like ML model development on tabular data and these competitions offer me opportunities exactly matching my interests. These competitions are well spaced through the year with a 2–3 week cycle. This offers time to develop models within a limited time frame and move onto subsequent challenges swiftly. I somehow fail to align with a longer duration of 3 months elsewhere without any major reward for 90% participants elsewhere on the platform. I admire the extent of insightful discussions and kernels shared in the forums as we don’t have any predisposed inhibitions arising from medal attributions herewith. I have learnt a lot from these forums and contribute to my best extent too, keeping a few tricks private. Most of the playground competitions have 1000+ participants making them liquid from a LB perspective. I have performed well in quite a few episodes and am receiving consistent results nowadays, making my participation more lucrative. Anshuman: Do you feel the Kaggle competitions are related to your work?\nRavi: I do not think that this series is particularly linked to my work tasks but opine that it mirrors my interest areas greatly. I can manage my daily routine perfectly with this series as one may spend a couple of hours daily to secure a good score in these competitions. This enables me to invest time in other activities keeping my Kaggle participation to a best feasible optimum.\nAnshuman: What kind of challenges do you look for today? How do you decide if the competition is worth your time?\nRavi: I wish to expand my horizons in featured competitions in the medium run of the next 1–2 years. In choosing Kaggle challenges today, my focus has evolved over time. I’m increasingly interested in featured competitions as I aspire to expand my horizons in this area over the next 1–2 years. Additionally, I aim to maintain a balance by contributing to the Kaggle playground series, which aligns well with my interests.\nWhen deciding if a competition is worth my time, I consider several factors:\nData Size: I assess the data size; smaller datasets can be appealing as they allow for quicker code completion and often align with simpler models. However, I consider the complexity of handling small datasets with numerous columns. CV Score Stability: I examine the standard deviation of the CV (cross-validation) score to gauge model stability across iterations and folds. Consistent CV scores are typically more appealing. CV-PLB Relations: I look for competitions where there’s a meaningful relationship between CV scores and the public leaderboard (PLB). This helps in making informed final submissions. Time Availability: Personal and professional commitments play a significant role. I avoid challenges where I may not be able to devote sufficient time toward the end, instead opting for competitions that align better with my schedule. Balancing these considerations helps me make informed choices about which competitions to participate in, ensuring that my time is well-invested and aligned with my goals.\nAbheesht: For noobs like me who haven’t dabbled much in Kaggle competitions and discussions, what would be your best advice?\nRavi: I may suggest a few points based on my experience on the platform-\nKeep learning as cynosure of all your activities on Kaggle and otherwise. This is a much more satisfying experience than aiming for medals. Till date, I never aimed to become a GM but aimed to become a better ML enthusiast. Stay consistent in any life and personal endeavor. This stems from 2 elements- interest in the activity and realistic goal setting. I believe in SMART goal planning and periodization and have implemented it across all walks of life. This is an open secret to my success on Kaggle and in other walks of life too. Leann to be modestly assertive. Saying a no without being rude is a very important skill that requires some training and experience. This will help one and all at work I have deep respect for time. I value my time a lot and respect it a lot. I try and be punctual and leave on time. I don’t believe in late work practices and to this day, have mostly avoided this to good effect and lots of professional success too. Develop a life-work balance — this becomes important as one ages, as one delves into multiple commitments too. One needs to balance multiple life events in parallel and one’s planning and execution skills are put to a rigorous test. Balancing various life and routine events is key to success in multiple facets of one’s personality Take structured breaks from one activity at a time rather than a complete break from all ongoing activities. I usually take structured breaks from the office (only) followed by a break from my fitness practice and then a break from Kaggle amortized over a span of 6–8 weeks. This keeps me motivated for a longer period of time, enabling me to render a fine balance across multiple events of almost equal priority Try and automate as much code as possible- this is specifically useful for competitions and repetitive tasks like curating baseline models, feature processing, preprocessing and general training. One may then edit the general pipeline to add assignment specifics. This is likely to save time and enable greater productivity Team up well and plan your strategy. Teaming up with friends helps a lot across all 4 tiers. Collective endeavor elicits significant synergy based power. Avoid dubious practices that could harm your reputation. This may hinder your progress a lot. Use Kaggle free resources to good effect. 50 GPU + TPU hours is significant and is available at your disposal per week. Post content consistently on Kaggle and share ideas, Inhibition is the enemy of collaboration and collaboration is a good route to success. Try and learn new skills/ improve existing skills periodically. Also try and match your current learning patterns with your long run learning goals periodically. If you digress from your long run goals, you may be better with either rebalancing the long run goal/ current activity. Develop an all-round profile outside of Kaggle too. ML is an ocean of opportunities and Kaggle is one of the ways to attain success. Stay active elsewhere as well, including but not limited to Analytics vidya, YouTube, GitHub, medium.com and any other community you find suitable. Hugging Face competitions are also good to learn and grow in this regard. I encourage one and all to participate in hackathons and any local competitions and conclaves that offer networking opportunities too. Remember that networking is as important as learning as the industry relies a lot on this aspect for referrals and job opportunities. Enjoy the journey and derive value from every step of your journey. I suggest one could break down a long run problem into a series of structured and achievable micro-goals that could eventually lead him/ her to success. I usually do this to good effect (with some meticulous planning and experience) and encourage others to follow suit. Keep others updated with your successes. LinkedIn is a good place to post about your professional successes including Kaggle progression, ranks and competition approaches too. Abheesht: When you are given a problem statement, how do you devise AI solutions for it? Do you mostly use classical ML models, since most of the data you use is tabular? What do you look for in a proposed solution?\nI predominantly work in credit risk areas that do not involve AI models to any extent. I think this is a huge drawback of this career path as Financial Regulators refrain from accepting results from AI models and latest advancements in this field of knowledge. Most of our models are classical ML models with emphasis on tabular data and simple algorithms.\nI usually devise my work assignment into the below steps for convenience and project planning-\nI understand the end-user’s requirement and the assignment’s long run usage before starting work on the project. Usually I am involved in assignments that necessitate continual usage and a user-friendly and clean data and model production pipeline. I break down the assignment into several micro-goals spanning over a 1–2 week period. I plan these goals with emphasis on data wrangling as a primary task spanning over 80% of the overall time invested. I usually automate the data pipeline efficiently, working towards a production and deployment all through the development process. This eases the production and deployment process substantially and improves stakeholder satisfaction. I usually conduct a catch-up meeting with the team and the end user at the end of every micro-goal. Most of our projects and assignments are internally and externally validated. I ensure that all our data processes are completely reproducible and are validated before we commence with the model development. Hence, I engage the internal validation team early in the project, facilitating timely comments and concerns addressing them immediately. Once we zero in on the development data, we conduct a bigger meeting with senior management, explaining the key data challenges and assumptions in the data pipeline. We also demonstrate the feature shortlisting processes (we have built several internal automated tools for the same) and revert to feedback from key stakeholders. This enables us to build models freely thereafter without any adverse comments later in the project lifecycle. We document the minutes of this meeting and circulate to Steering committees and auditors to ensure a 4-eye check on the progress and results. We then build a simple baseline model and showcase the selected variables to the key end-users, accepting their feedback and working on the same. Finally, we tune our models, ensemble results (if needed) and prepare the final model and submit it to the business team for review. As a standard practice, I build 10–15 candidate options and deploy them simultaneously to elicit an end-state result. This may perhaps posit a provision forecast/ NPA value/ default rate prediction/ PPNR per model selected. Business teams are highly comfortable discussing the model along with the result in this manner rather than a percentage result usually generated from the model. We then engage the internal and external validators and ensure our project is well documented. This is highly important in my area of work as Regulators usually peruse our model documents in detail. As a final step, we engage the IT and deployment teams to deploy the model in production integrating the model results into a report and entering controls as deemed necessary. This is a lengthy process and has to be facilitated with several UAT rounds and stake-holder approvals. This usually spans across a couple of quarters after the model is internally validated. As a final step, we are also supposed to document the IT implementation reports in a prescribed format and send them to the Regulator upon instructions. The model governance process along with these documents and code are thoroughly scrutinized regularly by the Regulator as part of their regular audits and reviews. Considering the sensitivity of the results involved, we usually select a simple model with explainable features as our chosen model for an assignment. We are particularly careful about production specific costs and time involvement to deploy the model and usually do not choose variables that otherwise perform well in the training period but are difficult to curate in production. We usually consult domain experts (economic research teams, credit underwriters and credit policy experts) to ratify and opine on the model development process and consider their subjective inputs as part of model governance.\nAbheesht: For the initial years of your career, you must have been an Individual Contributor (IC), before you transitioned to a managerial role. What are the major changes one has to make, to succeed in a managerial role? Which role do you like more?\nMy role in my team is a combination of an individual contributor and a manager. As a manager, I have the freedom to design my project plan to good effect based on the overall resources available in the team and the budgets involved. I usually resort to the below norm while working on several assignments-\nI prefer to perform data wrangling individually if the project spans over a longer duration. I opine that this provides me more control on an important aspect of the project helping me to automate and design the pipeline to my strengths and weaknesses. I also engage the stakeholders to my schedule and am more comfortable engaging the validator in this process. Once the data is finalized, I hand it over to junior colleagues to build a model. This fosters a win-win for all colleagues involved and often results in a timely completion. In some cases, I individually complete the entire assignment on my own, including data and models too and hand over the results to others to deploy. Other projects often require collaboration. As an example, IFRS9 models for PD-LGD are long run assignments that span over a year. We usually split the project into 3–4 managers, with each manager responsible for a set of products/ business entities. Our bank is a large international conglomerate, hence we need to engage foreign teams too (this is an interesting challenge in itself). I usually collaborate with my team and foreign teams in such assignments and demarcate roles for each participant with strict timelines. We usually split the overall assignment into micro-cycles and plan them properly to ensure smooth and effective progress. I usually assign tasks to junior colleagues based on their strengths and weaknesses and often ensure a well documented peer review. This ensures correctness and smooth progress through the project. I usually liaise with external stakeholders and internal validators and often defend our assumptions in meetings. This requires some negotiation skills in my opinion. I leant this from my seniors in my 2 employers and feel that I am adept at this aspect of project management currently. My role in my previous employer was an individual contributor while my current role involves both aspects of project management. I opine that one needs to be highly diligent at time and resource management and set and plan goals efficiently as a manager/ lead. One’s technical skills are seldom tested while managing assignments, but one’s negotiation skills, ability to work under tight timelines and strict budgets, handling escalations are tested to a greater extent as a lead. I think soft skills are more important herewith and this is gained with experience and inputs from senior colleagues. I have a good network in the industry and I use this to good effect to learn and improve myself over time. I usually do not have a preference for a role type, but given the overall career progression paths in the industry, I may perhaps choose to progress as a lead / senior lead with collaborative skills and roles going ahead.\nBest regards and happy learning!\nThat’s it. This was the very first interview of our new blog series AI Chronicles ! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "posts/flux_is_so_flexible.html",
    "href": "posts/flux_is_so_flexible.html",
    "title": "Deep Learning in Julia",
    "section": "",
    "text": "Flux is amazing and it’s far more than just an ML framework. Differentiable programming & Zygote, first class GPU support are features that set it apart among ML systems. The best thing I love about flux is mixing neural nets with differential equations, to get the best of black box and mechanistic modelling, this is what SciML doing.\nFlux is fairly new, and needs attention of community ! In this article we’ll learn about how to implement a model in Flux.jl . We’ll walkthrough UNet implementation, which I’ve been working on lately to contribute to flux’s model zoo Metalhead.jl.\nAfter reading this article you’ll learn about"
  },
  {
    "objectID": "posts/flux_is_so_flexible.html#unet",
    "href": "posts/flux_is_so_flexible.html#unet",
    "title": "Deep Learning in Julia",
    "section": "UNet 🥅",
    "text": "UNet 🥅\nUNet is a deep learning model which was released in the paper U-Net: Convolutional Networks for Biomedical Image Segmentation. The architecture of the network looks like : Fig. 1 ( Source: UNet Paper)We’ll not go into detail about UNet theory, as the paper explains it in best way, and explaination here would be redundant anyway.\n\n\n\nSource: UNet Paper\n\n\nThis tutorial will focus on implementation.\nLet’s dive into code rightaway ! The article is written presuming that you have knowledge of implementing a neural networks in PyTorch ! We simply create a class that inherits nn.Module . We now create layers and assign it to class with self . When this model is instantiated, these layers will become attribute of the model object.\n\n&gt;&gt;&gt; exec(open(\"basic_model.py\").read())\ntorch.Size([1, 5, 8, 8])\nLet’s see how it looks like in Julia.\n\njulia&gt; include(\"basic_model.jl\");\n(8, 8, 5, 1)\nSome Key things to note here are :\n\nFlux follows (H, W, C, N) standard for images while PyTorch uses (N, C, H, W)\nChain function is similar to nn.Sequential method from PyTorch\nJulia implements Multiple Dispatch unlike PyThon which is designed on Single Dispatch paradigm. In short multiple dispatch allows us to implement a single method for different combination of different type arguments, unlike python which restricts the methods to be bound to a single object and reimplemented for different classes. Read more about multiple dispatch here .\n\nLet’s see how official UNet implementation from Torchhub looks like\nThe author of original implementation (hereon referred just as “author”) created a helper method to create a convolutional block . This is a good practise called as DRY ( Do not Repeat Yourself).\n\n\n\nSource: UNet Paper\n\n\nThis allows us to create these conv blocks (fig. 2 zooms into fig. 1 to show these conv blocks) just by passing input and output channels. To implement it with Flux, we reuse our knowledge from gist 1 :\n\nThis allows us to create these conv blocks (fig. 2 zooms into fig. 1 to show these conv blocks) just by passing input and output channels.\nPretty simple isn’t it ?\nOne thing to note here is, Flux doesn’t allow us to name the layers explicitly, the reason can be found in this github issue. Now let’s go ahead and see how layers were created by author.\nNow we see that we have an encoder, bottleneck, decoder and an upconv layer. 1. Encoder : The four downstairs in Fig. 1 form the encoder block, and it encodes image by successive convolutions 2. Bottleneck : The layer between encoder and decoder is called bottleneck. The output of decoder is passed on to decoder & upconv block. 3. Decoder & Upconv block : The upconv block upsamples the input matrix, i.e. deconvolves the input to output with bigger size ( H x W ) than input. Decoder follows upconv layer and increases channels by performing convolution.\nLet’s create the model class in Flux first. Julia doesn’t have classes, it has structs . So model struct would look like this.\n\nTo reduce the redundancy in code I’ll implement the layers as array of layers, it’ll allow us to write a clean forward pass later. Don’t forget to notice the relation between number of channels of different blocks of UNet model.\nThe unet_block is the convolutional block that we defined earlier is a simple chain of Convolutional and BatchNorm layers. We further chain these blocks keeping in mind the input & output features using the Chain function. See ! How easy it is to create a model in Flux. Now we have one last thing remaining, the forward pass . We do it like\n\nin PyTorch. The cat operation orchestrates the connection between encoder and decoder demonstrated by copy and crop represented by gray arrow in Fig. 1.\nBut don’t you find the code above messy ? That’s where our definition of layers of arrays comes in. Let’s see how the forward pass is written for UNet.\n\nPretty simple & clean, we managed to keep the entire logic same.\nSome key points to be noted here:\n\nσ is nothing else but sigmoid function ! Julia allows us to use all the mathematical symbols as variables . Thus, flux defined sigmoid as σ rather than sigmoid()\nJulia uses matlab like syntax for ranges (see 1:4 for iterating over 1, 2, 3, 4 and 4:-1:1 for 4, 3, 2, 1).\nJulia uses 1 based indexing.\nAny Julia function with a trailing ! tells that operation will be inplace ( remember pass by reference from C++ using & operation).\nThe return keyword in last line is redundant, simply writing σ(u.final_conv(out)) would work as Julia always returns the output of last line of code, from any code block.\n\nThat brings us to the end of this tutorial. Thanks for reading !\nThis is the second article of my first blog series Julia For the Win. You can find the previous article Kaggle x Julia : Advanced House Price Prediction : EDA."
  },
  {
    "objectID": "posts/illustrated_llmos.html",
    "href": "posts/illustrated_llmos.html",
    "title": "Illustrated LLM OS",
    "section": "",
    "text": "This blog post explores the implementation of large language models (LLMs) as operating systems, inspired by Andrej Karpathy’s vision of AI resembling an OS, akin to Jarvis from Iron Man. The focus is on practical considerations, proposing an application-level integration for LLMs within a terminal session. A novel approach involves injecting state machines into the decoding process, enabling real-time code execution and interaction. Additionally, this post proposes Reinforcement Learning by System Feedback (RLSF),” a reinforcement learning technique applied to code generation tasks. This method leverages a reward model to evaluate code correctness through Python subprocess execution, enhancing LLM performance. The findings contribute insights into the dynamic control of LLMs and their potential applications beyond coding tasks.\nImage source: [1hr Talk] Intro to Large Language Models by Andrej Karpathy"
  },
  {
    "objectID": "posts/illustrated_llmos.html#preliminaries",
    "href": "posts/illustrated_llmos.html#preliminaries",
    "title": "Illustrated LLM OS",
    "section": "Preliminaries:",
    "text": "Preliminaries:\n\nState Machines: A state machine is a mathematical abstraction used to design algorithms. A state machine reads a set of inputs and changes to a different state based on those inputs. A state is a description of the status of a system waiting to execute a transition. A transition is a set of actions to execute when a condition is fulfilled or an event is received. In a state diagram, circles represent each possible state and arrows represent transitions between states.\nConstrained Decoding: Constrained decoding is a technique used in natural language processing and sequence generation tasks, including those involving large language models (LLMs). In constrained decoding, the generation of sequences is guided or restricted by certain constraints or conditions. This approach is particularly useful when you want to control or influence the output of a language model to meet specific requirements or criteria.\n\nA major challenge in the implementation of LLM OS is establishing a Link between the Operating System and LLM, that ensures training following the principle of Responsible AI."
  },
  {
    "objectID": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "href": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "title": "Illustrated LLM OS",
    "section": "Where should LLM sit? Concerns, Possibilities, and Limitations",
    "text": "Where should LLM sit? Concerns, Possibilities, and Limitations\nImagine a futuristic Jarvis-like AI. It’ll be able to search through the internet, access local files, videos, and images on the disk, and execute programs. Where should it sit? At the kernel level? At Python Level?\nAt Kernel Level: Consider integrating our advanced language model with the Linux kernel. This would provide the AI with comprehensive access to the operating system’s core functionalities. However, it’s important to recognize that large language models (LLMs) are designed for human-like interaction, not intricate coding tasks. While embedding the model at the kernel level offers the advantage of understanding and controlling detailed system operations, it raises valid security concerns. Responsible development is crucial to ensure that the AI’s evolving decision-making capabilities don’t inadvertently compromise system integrity.\nAt Application Level: Alternatively, we can position the LLM at the application level, operating seamlessly within a terminal session. Leveraging the abstractions of a programming language, such as Python, provides optimal control over the AI. This approach facilitates easy updates to the LLM OS and allows for extensive customization."
  },
  {
    "objectID": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "href": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "title": "Illustrated LLM OS",
    "section": "Towards Implementation: Injecting a State Machine in Decoder",
    "text": "Towards Implementation: Injecting a State Machine in Decoder\nFollowing is the illustration of how the generation process works in decoder-like models source.\n\nImage source: The Illustrated GPT-2 by Jay Alammar\nThe seamless interaction of a Large Language Model (LLM) with an operating system (OS) can be achieved through the strategic injection of a State Machine. By utilizing this approach, the model’s behavior can be dynamically controlled, enhancing its capability to engage with the OS. In particular, the injection of special tokens acts as triggers, facilitating interaction with the Python interpreter through subprocesses.\nTo illustrate this concept, consider the scenario where the LLM is tasked with generating the sum of the first N natural numbers. Here, I request the model to encapsulate the generated code within designated special tokens, [CODE] and [/CODE], during its response. The State Machine is activated upon the generation of the [CODE] token, initiating the collection of the generated code in a buffer.\nUpon the subsequent generation of the [/CODE] token, the State Machine orchestrates a temporary pause in the generation process. This interlude allows for the invocation of a Python subprocess to execute the collected code. The output of this execution is then appended to the current cache. Resuming the generation process, this technique enables the model to dynamically learn program execution behavior in real time.\nIn the following sections of this blog, we delve deeper into how this innovative technique empowers the model to adapt and refine its program execution understanding on the fly.\n\n\n\nimage/png\n\n\nFollowing is an illustration of a state machine to do various operations like db queries, accessing file systems, and even internet searches through Python subprocesses.\n\n\n\nimage/png\n\n\nThe Python programming language boasts a robust ecosystem that opens up a multitude of possibilities, especially when harnessed through subprocesses. By utilizing subprocesses, we can seamlessly execute Shell or Programming Language codes, expanding the functionality of our applications. Python’s versatility extends to internet access, where we can leverage powerful libraries like requests and urllib. This capability allows the Large Language Model (LLM) to interact dynamically with online resources. Additionally, the integration of command line packages, such as the one offered by Google, further enhances the LLM’s capacity to fetch information from the web. Python’s sophisticated file-handling utilities provide the LLM with the capability to navigate and manipulate files and multimedia at the user’s request. This functionality extends beyond mere text-based interactions, offering a rich, multimodal experience. The inclusion of multimodal models further augments the LLM’s ability to interpret and respond to a diverse range of user inputs."
  },
  {
    "objectID": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "href": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "title": "Illustrated LLM OS",
    "section": "Tuning LLMs for OS use case: A RL Problem:",
    "text": "Tuning LLMs for OS use case: A RL Problem:\nImagine confronting a challenging programming problem and enlisting the assistance of the Large Language Model (LLM) to generate a solution. Traditionally, the process involves articulating the problem to the LLM, receiving the generated code, manual inspection for correctness, and iterative feedback for bug resolution.\nNow, let’s recast this procedure as a Reinforcement Learning (RL) challenge, casting our LLM as an agent navigating a realm of computer processes. In this RL framework, the agent’s objective is to generate code, subject to correctness scrutiny.\nTo enhance this code generation process, we leverage a fundamental concept in RL — the reward model. This model quantifies the quality of generated code based on execution results. Through training the reward model, the LLM is systematically penalized for generating incorrect code, eliminating the need for manual intervention.\nI call this technique Reinforcement Learning by System Feedback (RLSF). This technique can not only be applied to improving LLMs on code generation tasks but to a variety of tasks as shown in the state machine diagram."
  },
  {
    "objectID": "posts/illustrated_llmos.html#concluding-notes",
    "href": "posts/illustrated_llmos.html#concluding-notes",
    "title": "Illustrated LLM OS",
    "section": "Concluding Notes",
    "text": "Concluding Notes\nThe reason I published this blog post is because I think the discussion of integrating LLMs with Operating Systems should be Open. It should be developed responsibly in an Open Source Environment. If there is an initiative like this in the future, I would love to collaborate on it! At this stage, I couldn’t get a working prototype of the approaches I discussed due to a lack of computing resources.\nJust Imagine an operating system built on HuggingFace transformer’s LLM pipelines. This setup not only accommodates the effortless integration of new models but also ensures the system remains adaptable to the latest advancements in the field. LLMs can add a new layer of accessibility to our Operating System by acting as smart interfaces."
  },
  {
    "objectID": "posts/illustrated_llmos.html#acknowledgements",
    "href": "posts/illustrated_llmos.html#acknowledgements",
    "title": "Illustrated LLM OS",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nRavindra Majumdar and Sunil Mallya for introducing me to the concept of constrained decoding."
  },
  {
    "objectID": "posts/sd_next.html",
    "href": "posts/sd_next.html",
    "title": "Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&B",
    "section": "",
    "text": "&lt;div id=\"quarto-navigation-envelope\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1zaWRlYmFyLXRpdGxl\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXItdGl0bGU=\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6V29yaw==\"&gt;Work&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6L3BhZ2VzL3dvcmsuaHRtbA==\"&gt;/pages/work.html&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6QmxvZw==\"&gt;Blog&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6L2Jsb2cuaHRtbA==\"&gt;/blog.html&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly90d2l0dGVyLmNvbS9rYW5wdXJpeWFuYXdhYg==\"&gt;https://twitter.com/kanpuriyanawab&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly93d3cubGlua2VkaW4uY29tL2luL2Fuc2h1aXptZS8=\"&gt;https://www.linkedin.com/in/anshuizme/&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9naXRodWIuY29tL2thbnB1cml5YW5hd2Fi\"&gt;https://github.com/kanpuriyanawab&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly95b3V0dWJlLmNvbS9AMXNtb2xsY29kZXI=\"&gt;https://youtube.com/@1smollcoder&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9mdWxsc3RhY2thZ2VudHMuc3Vic3RhY2suY29t\"&gt;https://fullstackagents.substack.com&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6e3s8IGFpIGdvb2dsZS1zY2hvbGFyID59fQ==\"&gt;&lt;i class=\"ai  ai-google-scholar\" aria-label=\"google-scholar\"&gt;&lt;/i&gt;&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLWludC1uYXZiYXI6aHR0cHM6Ly9zY2hvbGFyLmdvb2dsZS5mci9jaXRhdGlvbnM/dXNlcj03MHdoOWpjQUFBQUo=\"&gt;https://scholar.google.fr/citations?user=70wh9jcAAAAJ&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW1ldGF0aXRsZQ==\"&gt;Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&amp;B – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkdGl0bGU=\"&gt;Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&amp;B – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW9nY2FyZHRpdGxl\"&gt;Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&amp;B – Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW1ldGFzaXRlbmFtZQ==\"&gt;Anshuman Mishra&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLXR3aXR0ZXJjYXJkZGVzYw==\"&gt;A walkthrough of using SD.Next (Advanced webUI for Stable Diffusion) for generating high-quality images using HuggingFace Diffusers and managing experiments with W&amp;B.&lt;/span&gt; &lt;span class=\"hidden quarto-markdown-envelope-contents\" data-render-id=\"cXVhcnRvLW9nY2FyZGRkZXNj\"&gt;A walkthrough of using SD.Next (Advanced webUI for Stable Diffusion) for generating high-quality images using HuggingFace Diffusers and managing experiments with W&amp;B.&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\n  window.document.addEventListener(\"DOMContentLoaded\", function (event) {\n    const icon = \"\";\n    const anchorJS = new window.AnchorJS();\n    anchorJS.options = {\n      placement: 'right',\n      icon: icon\n    };\n    anchorJS.add('.anchored');\n    const isCodeAnnotation = (el) =&gt; {\n      for (const clz of el.classList) {\n        if (clz.startsWith('code-annotation-')) {                     \n          return true;\n        }\n      }\n      return false;\n    }\n    const onCopySuccess = function(e) {\n      // button target\n      const button = e.trigger;\n      // don't keep focus\n      button.blur();\n      // flash \"checked\"\n      button.classList.add('code-copy-button-checked');\n      var currentTitle = button.getAttribute(\"title\");\n      button.setAttribute(\"title\", \"Copied!\");\n      let tooltip;\n      if (window.bootstrap) {\n        button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n        button.setAttribute(\"data-bs-placement\", \"left\");\n        button.setAttribute(\"data-bs-title\", \"Copied!\");\n        tooltip = new bootstrap.Tooltip(button, \n          { trigger: \"manual\", \n            customClass: \"code-copy-button-tooltip\",\n            offset: [0, -8]});\n        tooltip.show();    \n      }\n      setTimeout(function() {\n        if (tooltip) {\n          tooltip.hide();\n          button.removeAttribute(\"data-bs-title\");\n          button.removeAttribute(\"data-bs-toggle\");\n          button.removeAttribute(\"data-bs-placement\");\n        }\n        button.setAttribute(\"title\", currentTitle);\n        button.classList.remove('code-copy-button-checked');\n      }, 1000);\n      // clear code selection\n      e.clearSelection();\n    }\n    const getTextToCopy = function(trigger) {\n        const codeEl = trigger.previousElementSibling.cloneNode(true);\n        for (const childEl of codeEl.children) {\n          if (isCodeAnnotation(childEl)) {\n            childEl.remove();\n          }\n        }\n        return codeEl.innerText;\n    }\n    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {\n      text: getTextToCopy\n    });\n    clipboard.on('success', onCopySuccess);\n    if (window.document.getElementById('quarto-embedded-source-code-modal')) {\n      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {\n        text: getTextToCopy,\n        container: window.document.getElementById('quarto-embedded-source-code-modal')\n      });\n      clipboardModal.on('success', onCopySuccess);\n    }\n      var localhostRegex = new RegExp(/^(?:http|https):\\/\\/localhost\\:?[0-9]*\\//);\n      var mailtoRegex = new RegExp(/^mailto:/);\n        var filterRegex = new RegExp(\"https:\\/\\/www\\.heyyanshuman\\.com\");\n      var isInternal = (href) =&gt; {\n          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);\n      }\n      // Inspect non-navigation links and adorn them if external\n     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');\n      for (var i=0; i&lt;links.length; i++) {\n        const link = links[i];\n        if (!isInternal(link.href)) {\n          // undo the damage that might have been done by quarto-nav.js in the case of\n          // links that we want to consider external\n          if (link.dataset.originalHref !== undefined) {\n            link.href = link.dataset.originalHref;\n          }\n        }\n      }\n    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {\n      const config = {\n        allowHTML: true,\n        maxWidth: 500,\n        delay: 100,\n        arrow: false,\n        appendTo: function(el) {\n            return el.parentElement;\n        },\n        interactive: true,\n        interactiveBorder: 10,\n        theme: 'quarto',\n        placement: 'bottom-start',\n      };\n      if (contentFn) {\n        config.content = contentFn;\n      }\n      if (onTriggerFn) {\n        config.onTrigger = onTriggerFn;\n      }\n      if (onUntriggerFn) {\n        config.onUntrigger = onUntriggerFn;\n      }\n      window.tippy(el, config); \n    }\n    const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n    for (var i=0; i&lt;noterefs.length; i++) {\n      const ref = noterefs[i];\n      tippyHover(ref, function() {\n        // use id or data attribute instead here\n        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n        try { href = new URL(href).hash; } catch {}\n        const id = href.replace(/^#\\/?/, \"\");\n        const note = window.document.getElementById(id);\n        if (note) {\n          return note.innerHTML;\n        } else {\n          return \"\";\n        }\n      });\n    }\n    const xrefs = window.document.querySelectorAll('a.quarto-xref');\n    const processXRef = (id, note) =&gt; {\n      // Strip column container classes\n      const stripColumnClz = (el) =&gt; {\n        el.classList.remove(\"page-full\", \"page-columns\");\n        if (el.children) {\n          for (const child of el.children) {\n            stripColumnClz(child);\n          }\n        }\n      }\n      stripColumnClz(note)\n      if (id === null || id.startsWith('sec-')) {\n        // Special case sections, only their first couple elements\n        const container = document.createElement(\"div\");\n        if (note.children && note.children.length &gt; 2) {\n          container.appendChild(note.children[0].cloneNode(true));\n          for (let i = 1; i &lt; note.children.length; i++) {\n            const child = note.children[i];\n            if (child.tagName === \"P\" && child.innerText === \"\") {\n              continue;\n            } else {\n              container.appendChild(child.cloneNode(true));\n              break;\n            }\n          }\n          if (window.Quarto?.typesetMath) {\n            window.Quarto.typesetMath(container);\n          }\n          return container.innerHTML\n        } else {\n          if (window.Quarto?.typesetMath) {\n            window.Quarto.typesetMath(note);\n          }\n          return note.innerHTML;\n        }\n      } else {\n        // Remove any anchor links if they are present\n        const anchorLink = note.querySelector('a.anchorjs-link');\n        if (anchorLink) {\n          anchorLink.remove();\n        }\n        if (window.Quarto?.typesetMath) {\n          window.Quarto.typesetMath(note);\n        }\n        if (note.classList.contains(\"callout\")) {\n          return note.outerHTML;\n        } else {\n          return note.innerHTML;\n        }\n      }\n    }\n    for (var i=0; i&lt;xrefs.length; i++) {\n      const xref = xrefs[i];\n      tippyHover(xref, undefined, function(instance) {\n        instance.disable();\n        let url = xref.getAttribute('href');\n        let hash = undefined; \n        if (url.startsWith('#')) {\n          hash = url;\n        } else {\n          try { hash = new URL(url).hash; } catch {}\n        }\n        if (hash) {\n          const id = hash.replace(/^#\\/?/, \"\");\n          const note = window.document.getElementById(id);\n          if (note !== null) {\n            try {\n              const html = processXRef(id, note.cloneNode(true));\n              instance.setContent(html);\n            } finally {\n              instance.enable();\n              instance.show();\n            }\n          } else {\n            // See if we can fetch this\n            fetch(url.split('#')[0])\n            .then(res =&gt; res.text())\n            .then(html =&gt; {\n              const parser = new DOMParser();\n              const htmlDoc = parser.parseFromString(html, \"text/html\");\n              const note = htmlDoc.getElementById(id);\n              if (note !== null) {\n                const html = processXRef(id, note);\n                instance.setContent(html);\n              } \n            }).finally(() =&gt; {\n              instance.enable();\n              instance.show();\n            });\n          }\n        } else {\n          // See if we can fetch a full url (with no hash to target)\n          // This is a special case and we should probably do some content thinning / targeting\n          fetch(url)\n          .then(res =&gt; res.text())\n          .then(html =&gt; {\n            const parser = new DOMParser();\n            const htmlDoc = parser.parseFromString(html, \"text/html\");\n            const note = htmlDoc.querySelector('main.content');\n            if (note !== null) {\n              // This should only happen for chapter cross references\n              // (since there is no id in the URL)\n              // remove the first header\n              if (note.children.length &gt; 0 && note.children[0].tagName === \"HEADER\") {\n                note.children[0].remove();\n              }\n              const html = processXRef(null, note);\n              instance.setContent(html);\n            } \n          }).finally(() =&gt; {\n            instance.enable();\n            instance.show();\n          });\n        }\n      }, function(instance) {\n      });\n    }\n        let selectedAnnoteEl;\n        const selectorForAnnotation = ( cell, annotation) =&gt; {\n          let cellAttr = 'data-code-cell=\"' + cell + '\"';\n          let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n          return selector;\n        }\n        const selectCodeLines = (annoteEl) =&gt; {\n          const doc = window.document;\n          const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n          const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n          const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n          const lineIds = lines.map((line) =&gt; {\n            return targetCell + \"-\" + line;\n          })\n          let top = null;\n          let height = null;\n          let parent = null;\n          if (lineIds.length &gt; 0) {\n              //compute the position of the single el (top and bottom and make a div)\n              const el = window.document.getElementById(lineIds[0]);\n              top = el.offsetTop;\n              height = el.offsetHeight;\n              parent = el.parentElement.parentElement;\n            if (lineIds.length &gt; 1) {\n              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n              const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n              height = bottom - top;\n            }\n            if (top !== null && height !== null && parent !== null) {\n              // cook up a div (if necessary) and position it \n              let div = window.document.getElementById(\"code-annotation-line-highlight\");\n              if (div === null) {\n                div = window.document.createElement(\"div\");\n                div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n                div.style.position = 'absolute';\n                parent.appendChild(div);\n              }\n              div.style.top = top - 2 + \"px\";\n              div.style.height = height + 4 + \"px\";\n              div.style.left = 0;\n              let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n              if (gutterDiv === null) {\n                gutterDiv = window.document.createElement(\"div\");\n                gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n                gutterDiv.style.position = 'absolute';\n                const codeCell = window.document.getElementById(targetCell);\n                const gutter = codeCell.querySelector('.code-annotation-gutter');\n                gutter.appendChild(gutterDiv);\n              }\n              gutterDiv.style.top = top - 2 + \"px\";\n              gutterDiv.style.height = height + 4 + \"px\";\n            }\n            selectedAnnoteEl = annoteEl;\n          }\n        };\n        const unselectCodeLines = () =&gt; {\n          const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n          elementsIds.forEach((elId) =&gt; {\n            const div = window.document.getElementById(elId);\n            if (div) {\n              div.remove();\n            }\n          });\n          selectedAnnoteEl = undefined;\n        };\n          // Handle positioning of the toggle\n      window.addEventListener(\n        \"resize\",\n        throttle(() =&gt; {\n          elRect = undefined;\n          if (selectedAnnoteEl) {\n            selectCodeLines(selectedAnnoteEl);\n          }\n        }, 10)\n      );\n      function throttle(fn, ms) {\n      let throttle = false;\n      let timer;\n        return (...args) =&gt; {\n          if(!throttle) { // first call gets through\n              fn.apply(this, args);\n              throttle = true;\n          } else { // all the others get throttled\n              if(timer) clearTimeout(timer); // cancel #2\n              timer = setTimeout(() =&gt; {\n                fn.apply(this, args);\n                timer = throttle = false;\n              }, ms);\n          }\n        };\n      }\n        // Attach click handler to the DT\n        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n        for (const annoteDlNode of annoteDls) {\n          annoteDlNode.addEventListener('click', (event) =&gt; {\n            const clickedEl = event.target;\n            if (clickedEl !== selectedAnnoteEl) {\n              unselectCodeLines();\n              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n              if (activeEl) {\n                activeEl.classList.remove('code-annotation-active');\n              }\n              selectCodeLines(clickedEl);\n              clickedEl.classList.add('code-annotation-active');\n            } else {\n              // Unselect the line\n              unselectCodeLines();\n              clickedEl.classList.remove('code-annotation-active');\n            }\n          });\n        }\n    const findCites = (el) =&gt; {\n      const parentEl = el.parentElement;\n      if (parentEl) {\n        const cites = parentEl.dataset.cites;\n        if (cites) {\n          return {\n            el,\n            cites: cites.split(' ')\n          };\n        } else {\n          return findCites(el.parentElement)\n        }\n      } else {\n        return undefined;\n      }\n    };\n    var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n    for (var i=0; i&lt;bibliorefs.length; i++) {\n      const ref = bibliorefs[i];\n      const citeInfo = findCites(ref);\n      if (citeInfo) {\n        tippyHover(citeInfo.el, function() {\n          var popup = window.document.createElement('div');\n          citeInfo.cites.forEach(function(cite) {\n            var citeDiv = window.document.createElement('div');\n            citeDiv.classList.add('hanging-indent');\n            citeDiv.classList.add('csl-entry');\n            var biblioDiv = window.document.getElementById('ref-' + cite);\n            if (biblioDiv) {\n              citeDiv.innerHTML = biblioDiv.innerHTML;\n            }\n            popup.appendChild(citeDiv);\n          });\n          return popup.innerHTML;\n        });\n      }\n    }\n  });\n  &lt;/script&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "posts/agent_memory_101.html",
    "href": "posts/agent_memory_101.html",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "",
    "text": "In my opinion, the next major advancement in Applied AI won’t come from larger models or more training data, but from agents that can actually learn from experience. For that to happen, a robust memory layer is absolutely essential. In this blog, we’ll delve into why agents need such a memory layer and explore the exciting possibilities this unlocks. But first, to set the context, let’s understand the limitations of current agents:"
  },
  {
    "objectID": "posts/agent_memory_101.html#context-window-bottleneck",
    "href": "posts/agent_memory_101.html#context-window-bottleneck",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Context Window Bottleneck:",
    "text": "Context Window Bottleneck:\nThe most simplest mental model to understand Context window is:\n\nComputer is to RAM as an LLM is to its context window.\n\n\nJust as RAM determines how much data a computer can actively work with at any given moment, the context window limits how much information a large language model (LLM) can “see” and process at once.\n\nIt acts as AI’s active short-term memory. In more simple words - it’s the amount of information - text/audio/video, that AI can “look at” or consider at any given point of time when generating a response. While you chat with an AI, your conversation history, up to a certain limit, fits into this window.\nThe trouble is, this window is finite. Remember those “maximum limit reached” alerts in Claude or ChatGPT, well we are talking about exactly that! While it’s true that these windows have been expanding (Gemini models having 10M context length), they are still fundamentally limited. This poses a huge challenge for:\n→ Extended Conversations - Chats that go on for days, weeks, or months will inevitably exceed the window size.\n→ Complex Document Analysis - Trying to understand or summarize very long documents, like a detailed research paper or a lengthy legal contract, becomes problematic if the whole thing doesn’t fit.\n→ Long-Term Tasks - Any project or goal that requires consistent input and understanding over a long period is hampered if the AI keeps forgetting earlier stages."
  },
  {
    "objectID": "posts/agent_memory_101.html#why-cant-we-just-have-infinite-context",
    "href": "posts/agent_memory_101.html#why-cant-we-just-have-infinite-context",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Why Can’t We Just Have Infinite Context?",
    "text": "Why Can’t We Just Have Infinite Context?\nA fair question! The technical reason, as I understand it, is largely tied to the underlying transformer architecture that powers many of these LLMs. The self-attention mechanism, which allows the model to weigh the importance of different parts of the input, generally has a computational and memory cost that increases quadratically with the length of the input sequence. Simply put, making the context window vastly larger makes the AI much, much slower and more expensive to run. While there’s ongoing research into more efficient architectures, just scaling up the current approach indefinitely isn’t practical.\n\n“Lost in the Middle”: Even Big Windows Have Blind Spots\nEven if we had incredibly long context windows, there’s another curious problem: LLMs don’t always use all the information within their context window equally well. Research, like the paper Lost in the Middle: How Language Models Use Long Contexts has shown a fascinating U-shaped performance curve. This means:\n\nModels are often best at recalling and using information that appears at the very beginning of their input context (primacy bias).\nThey are also quite good with information at the very end of the context (recency bias).\nHowever, performance can significantly degrade when they need to access and use information located somewhere in the middle of a long context.\n\nImagine feeding an AI 20 documents to answer a question, and the key document is the 10th one. The AI might struggle to find or correctly use that key document more than if it were the 1st or the 20th. In some cases, performance when information is in the middle can be worse than if the AI had no documents at all and was just relying on its pre-trained knowledge! This phenomenon is seen even in models specifically designed for long contexts.\n\nThe consequence of these limitations is that many AI agents today are essentially stateless. Each interaction is largely isolated, leading to a lack of true personalization, repeated questions, and inconsistent behavior. It’s like having a brilliant personal assistant who, unfortunately, gets their memory wiped clean every morning."
  },
  {
    "objectID": "posts/agent_memory_101.html#defining-memory-in-ai-more-than-just-recent-chat-history",
    "href": "posts/agent_memory_101.html#defining-memory-in-ai-more-than-just-recent-chat-history",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Defining “Memory” in AI: More Than Just Recent Chat History",
    "text": "Defining “Memory” in AI: More Than Just Recent Chat History\nWhen I talk about building AI agents with memory, I’m envisioning something far more sophisticated than just extending the context window. I’m talking about the AI’s capacity to retain, recall, and utilize relevant information across extended periods, diverse tasks, and numerous interactions. This is what allows an AI to transition from being a stateless tool to a stateful companion – one that truly learns and evolves alongside its user.\nI think it’s helpful to consider different types of memory, drawing inspiration from human cognition, which some AI memory systems are starting to model:\n\nShort-Term/Working Memory: This is the AI’s immediate “scratchpad,” analogous to the current context window. It holds what’s actively being processed.\nLong-Term Memory: This is the persistent store of information. We can break this down further:\n\nEpisodic Memory: Memories of specific events or interactions. For an AI, this would mean remembering past conversations with you, the specific advice it gave, or the tasks you worked on together.\nSemantic Memory: General knowledge and facts. This includes the AI’s vast pre-trained knowledge, but also, crucially, facts it learns specifically about you (your preferences, goals, style) or about a particular domain you’re working in.\nProcedural Memory: Knowledge of how to do things. For an AI, this could be remembering a multi-step process you often use, or learning the best way to present information to you.\n\n\nI have a strong opinion here. The Dotcom era Dotcom era enabled businesses to move online - and Apps emerged, I think AI is going to bring hyper-personalization to these apps. And for that to happen this much depth of memory so crucial. Moreover, Agent memory is going to play very important role in maintaining coherence and consistency in its interactions. It will enable an agent to Learn and adapt from past successes and failures."
  },
  {
    "objectID": "posts/agent_memory_101.html#another-challenge---llms-lack-focus",
    "href": "posts/agent_memory_101.html#another-challenge---llms-lack-focus",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Another challenge - LLMs lack focus!",
    "text": "Another challenge - LLMs lack focus!\nBeyond the context window size and the “lost in the middle” problem, there’s another tricky aspect: distractibility. I think this is a crucial point – it’s not just about how much the AI can see, but how clearly it can see it. If the important details are surrounded by noise, the AI can get sidetracked.\nThe paper Large Language Models Can Be Easily Distracted by Irrelevant Context really drives this home. They created a special dataset called Grade-School Math with Irrelevant Context (GSM-IC) where math problems were intentionally mixed with useless information. What they found was that the performance of cutting-edge LLMs dropped dramatically when this irrelevant information was present. Even if all the necessary information is within the context window, adding just one irrelevant sentence could throw the model off. This tells me that even with larger context windows, we still need to be smart about how information is presented and processed.\nThe consequence of all these limitations – finite windows and “lost in the middle” effects – is that many AI agents today are essentially stateless. This is a really important concept, so let’s dive into it."
  },
  {
    "objectID": "posts/agent_memory_101.html#stateless-agents",
    "href": "posts/agent_memory_101.html#stateless-agents",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Stateless Agents",
    "text": "Stateless Agents\nA stateless AI, in simple terms, is an AI that treats (almost) every interaction as a brand new one. It doesn’t have a persistent memory of your past conversations, preferences, or history beyond what fits in its immediate, fleeting context window.\nThink about these scenarios, which I’m sure many of us have experienced:\n→ The “Who Are You Again?” Syndrome: You’re in a long work session with an AI, perhaps coding or drafting a document. You’ve already told it your project name, your specific requirements, and your preferred style. Half an hour later, you ask it a follow-up question, and it responds as if it has no idea what project you’re even talking about.\n→ Groundhog Day Problem-Solving: You’re working through a multi-step problem with an AI. It helps you with step 1. When you move to step 2, it seems to have forgotten the conclusions or data from step 1, forcing you to re-explain or re-feed information.\n→ The Dietary Déjà Vu: You tell your AI assistant, “I’m a vegetarian.” It acknowledges this. The next day, you ask for dinner ideas, and it enthusiastically suggests a steakhouse. The “Mem0” paper has a great illustration of this exact problem (Figure 1 in their paper). Frustrating, right? That’s a stateless agent forgetting a critical piece of information once it falls out of the immediate context.\nThis statelessness is the arch-nemesis of hyper-personalization. It leads to inefficient interactions, repeated explanations, and a feeling that the AI, despite its power, doesn’t really “get” you or your ongoing needs. It’s like having that brilliant assistant I mentioned earlier, but one who gets their memory wiped clean every single morning."
  },
  {
    "objectID": "posts/agent_memory_101.html#stateful-agents",
    "href": "posts/agent_memory_101.html#stateful-agents",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Stateful Agents",
    "text": "Stateful Agents\nNow, imagine the opposite: a stateful AI agent. This is an AI designed with a memory system that allows it to maintain a persistent, evolving understanding of you, your interactions, your preferences, and the facts relevant to your shared tasks, across multiple sessions and over long periods.\nThis isn’t just about remembering your name; it’s about building a continuous thread of understanding.\nWhat could a truly stateful agent do?\n\nRemember Your Preferences, Proactively: It wouldn’t just avoid suggesting steak after you say you’re vegetarian; it might proactively say, “Since you’re vegetarian, how about this new pasta dish?” or “I found some great vegetarian recipes that align with your goal of eating healthier, which you mentioned last week.”\nRecall Your Work Context: When you resume a coding project, it would remember your preferred coding style, the libraries you commonly use, the overall architecture of your project, and the specific problem you were trying to solve last time.\nBuild on Past Interactions: It could reference a discussion you had weeks ago, saying, “Remember when we were talking about marketing strategies for X? I found an article that expands on that idea.”\nLearn Your Style: Over time, it would adapt to your communication style, the level of detail you prefer, and even your sense of humor, making interactions feel more natural and efficient.\nEnable True Learning and Adaptation: A stateful agent can learn from its mistakes. If it gave you a suggestion that didn’t work out, it could store that “outcome” and avoid similar errors in the future. This is crucial for agents that need to perform complex tasks or make decisions.\n\nThis statefulness transforms an AI from a generic tool into a genuine assistant, a collaborator, or even a companion that grows and learns with you.\nI believe the next wave of killer apps won’t just be smart; they’ll be smart about you. And the engine driving this hyper-personalization will be these advanced memory systems. Think about how this could transform the apps we use every day:\n\n\n\n\n\n\n\n\nCategory\nStateless\nStateful (with Memory)\n\n\n\n\nE-commerce & Shopping\nYou search for “running shoes” and get a generic list. Next week, it treats you like a new customer.\nThe AI remembers your shoe size, brand preference, and past purchases. When you search for “running gear,” it suggests compatible items like socks or running belts and might even recommend accessories in complementary colors to items you previously bought.\n\n\nProductivity & Work\nYou ask your AI coding assistant to write a Python function. For the next request, it might use a completely different style, forgetting your preferences for comments or error handling.\nThe AI coding assistant remembers your common coding patterns (e.g., preference for async functions, using black for formatting) and provides suggestions that fit seamlessly into your existing codebase and personal style.\n\n\nLearning & Education\nAn AI tutor gives you a lesson on fractions. The next day, it starts from scratch, unaware of the specific concepts you struggled with previously.\nThe AI tutor remembers you found a specific concept (e.g., improper fractions) tricky. It starts the next session with a recap of that point, offers a different explanation, or provides tailored practice problems before moving on.\n\n\nHealthcare & Wellness\nYou ask for help to reduce stress, and the app provides a generic meditation exercise.\nThe AI remembers your specific stressors (e.g., work deadlines) and preferred calming activities (e.g., nature walks). It suggests a targeted 10-minute meditation for deadline pressure or recommends a short walk, making the advice highly relevant.\n\n\nPersonal Assistants\nA chatbot that can answer questions but has no memory of you, your preferences, or your past interactions.\nAn AI that remembers your goals, important names, favorite music, and even your mood patterns. It can offer encouragement and make suggestions that are truly relevant because they are rooted in a shared, remembered history."
  },
  {
    "objectID": "posts/agent_memory_101.html#current-approaches-for-agentic-memory-layer",
    "href": "posts/agent_memory_101.html#current-approaches-for-agentic-memory-layer",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "Current Approaches for Agentic Memory layer",
    "text": "Current Approaches for Agentic Memory layer\n\nMemGPT (now part of LettaAI)’s LlmOS-Inspired Approach: The MemGPT: Towards LLMs as Operating Systems paper introduces “virtual context management,” where the LLM itself acts like an operating system for its memory. It intelligently manages a hierarchical memory system, with a limited “main context” (like RAM) for active processing and a larger “external context” (like disk storage, including archival and recall storage) for long-term information. The LLM uses function calls to page data between these tiers, effectively giving it an extended memory to handle tasks like large document analysis and multi-session chats where context far exceeds its physical window\n\n\n\nMem0’s Approach to Dynamic Memory: The Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory paper outlines a scalable, memory-centric architecture designed for real-world applications. Memo’s system dynamically extracts salient information from conversations in an extraction phase (using historical context like summaries and recent messages) and then, in an update phase, uses an LLM with tool-calling capabilities to evaluate these new “candidate memories” against existing ones, deciding whether to ADD, UPDATE, DELETE, or take NOOP (no operation) to maintain a coherent knowledge base."
  },
  {
    "objectID": "posts/agent_memory_101.html#the-human-element-feeling-understood",
    "href": "posts/agent_memory_101.html#the-human-element-feeling-understood",
    "title": "The AI Memory Layer Will Change Everything",
    "section": "The Human Element: Feeling Understood",
    "text": "The Human Element: Feeling Understood\nThere’s something deeply human about being remembered. When a friend recalls a small detail from a conversation months ago, it makes us feel valued and heard. I believe that as AI agents develop more sophisticated memory, our interactions with them will start to feel more meaningful.\nIt’s not about replacing human connection, of course. But for the tasks we delegate to AI, or the ways we use AI for support and companionship, memory will be key to building trust and fostering a sense of being genuinely understood. When an AI can consistently recall your context, preferences, and history, it reduces friction, saves you time, and makes the collaboration feel much more natural and effective.\nA Note on the Research That Inspired This Post:\nThroughout this piece, I’ve drawn inspiration and information from the work of many talented researchers and writers exploring the frontiers of AI memory. If you’re interested in digging deeper, here are some of the key resources I found particularly insightful:\n\nFor understanding the challenges with how LLMs use long contexts, the paper Lost in the Middle: How Language Models Use Long Contexts (Liu et al., 2023) is a must-read.\nThe concept of LLMs being easily sidetracked is well-explored in Large Language Models Can Be Easily Distracted by Irrelevant Context (Shi et al., 2023).\nFor an OS-inspired approach to memory management, check out “MemGPT: Towards LLMs as Operating Systems” (Packer et al., 2023).\nThe Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory paper (Chhikara et al., 2025) offers a look at a production-focused system with impressive results and graph-based memory.\nThe blogs from Letta (on RAG vs. Agent Memory, Memory Blocks, and Stateful Agents) and Mem0.ai (on Types of Memory, Making AI Companions Truly Personal, and Memory in Agents) provide excellent conceptual overviews and insights into building more personal AI."
  },
  {
    "objectID": "posts/interview_aakash.html",
    "href": "posts/interview_aakash.html",
    "title": "Interview with Aakash Kumar Nain, MLE at Merlyn Minds",
    "section": "",
    "text": "The AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard-of) stories out into the world — their struggles, their triumphs, and their journey. We will be conversing with people we have looked up to and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nThe two of us with Aakash in Uzbekistan! Aakash Kumar Nain — open source legend, MLE extraordinaire, Delhi lover, mountain climber. We had the privilege of meeting Aakash in Tashkent at the Google ML Community Summit and were taken aback by how laidback and easygoing he was. Looking at how meticulous he is with his work, we were expecting him to be the “serious kind”. The three of us instantly hit it off.\nWhen we say Aakash is an open source legend, we kid you not. Aakash started contributing to DL open source in 2016, and has never looked back since then. He is a TensorFlow-Addons maintainer. He is a core contributor to Keras 3.0, the new multi-backend Keras. Our favourite open source work of his is, however, Annotated Papers, where he breaks down ML/DL papers into simple explanations. Aakash is a huge proponent of JAX. Safe to say, he has inspired folks like us to take to open source.\nOn top of his impressive open source work, he has 7 years of experience in the industry as an ML Engineer/Researcher, starting his career at Parallel Dots and then working at Ola, H2O.ai and now Merlyn Minds where he is working on building an AI assistant for education. Despite having spent 7 years in the industry, he has lost none of his inquisitiveness and curiosity.\nWe can keep raving on and on about Aakash’s technical accomplishments, but to us, what separates Aakash from the rest is his approach to life. Read on to find out more about the man!\nHi, Aakash! Thank you for doing this.\nThank you for having me here today.\nQ: Could you discuss any machine learning projects or applications that you’ve found personally rewarding, either in terms of impact or the technical challenges involved?\nThough all the open-source projects I worked on were rewarding, contributing to TensorFlow-Addons, Keras, JAX, and Keras Core created the biggest impact. Although I have been a TF user since 2016, my open-source journey started with TF-addons in 2019. Once you start exploring the low-level bits of a library or a framework, your understanding of the mental model improves automatically. I wrote many code examples for keras.io where I showcased the flexibility of the Keras API to build very complex models. The motivation for writing those code examples was to help people understand the mental model behind tf.keras\nI consider myself a power Keras user. I have always wanted to contribute to the core Keras engine. This year, I finally got an opportunity to collaborate with Francois Chollet and the team to rewrite the entire Keras codebase with multi-backend support, including but not limited to TensorFlow, JAX, and PyTorch. It is one of my biggest open-source collaborations with the highest impact.\nQ: The field of machine learning is continually evolving. Are there any emerging trends or developments that particularly excite you or that you believe will have a significant impact in the near future?\nDiffusion models are one of the things that have gotten me excited about the future of generative AI after a long time. Though generative machine learning algorithms are nothing new, it’s just that the underlying algorithm for Diffusion models feels more natural. The other thing that I am looking forward to is the successor of the Transformer architecture. CNNs to transformers was a big leap. I hope the next set of algorithms is equally revolutionary.\nQ: How do you approach maintaining a balance between your athletic interests, such as being a footballer, and your demanding career in data science? Do you find any synergies between these seemingly different pursuits?\nEver heard of the phrase “Too much of anything is good for nothing”? If you like/love doing something, that doesn’t mean you should devote your entire time to it. Of course, it’s a biased opinion, but to me, there is way more to life than just work. So, I distribute my time among all the things I enjoy doing. It doesn’t mean that it should be like that for anyone else in any sense. It’s a personal choice how much time one wants to dedicate to different aspects of life. Also, I am a firm believer that health is the true wealth. To ensure that I am physically fit, I go to the gym and play football regularly.\nQ: Many aspiring data scientists grapple with imposter syndrome. Have you ever experienced such feelings, and if so, how did you overcome them?\nOh absolutely! Though many people may suffer from imposter syndrome at different stages, I think it’s way more common during the early years. When working in a fast-moving field like ML, it is natural to feel lagging, seeing the incredible progress in ML and AI daily. The best way to overcome such feelings is to understand that you don’t have to follow every other trend. Fundamentals matter the most in the long run. If you have a solid understanding of the fundamentals, things will always be easy.\nQ: As a maintainer of TensorFlow addons and contributor to multi-backend Keras, can you give us some insights into the open-source development process, including the joys and challenges of contributing to these projects?\nContributing to open-source is a commitment. It’s not a one-time thing. If you are working on an open-source project, ideally you expect that the community will use it, and that too for a long period. Developing something that people are eager to use needs well-defined goals. The development of the project has to align with these goals.\nThe best part of OSS is that you get to collaborate with many talented people. Once you start developing things in collaboration, you get more clarity about the modalities (using the word modality loosely in this context). You start to optimize for maximum coverage-minimal maintenance. Talking about the challenges, I think the biggest challenge is commitment. Many people do OSS just to make it a bullet point in their resume, but some of us do it because we enjoy solving complex problems for a larger audience. If you have a full-time job, then working on OSS means cutting down on other things to find time for the project.\nQ: How do you stay updated with the latest developments in the field of machine learning? Are there specific resources or communities you recommend for those looking to stay informed?\nReading research papers is the easiest way. The problem, nowadays, is that it is very hard to filter out good papers from the “pile” of research papers being dumped on arXiv every day. Earlier I used to use arXiv-sanity for filtering paper, but recent changes made to it made it somewhat less usable for me (asking Karpathy to bring back the top-rated filter). Now I rely mainly on my Twitter feed for filtering papers. It’s not that good, but it works.\nTo answer your question about communities and resources, I think Twitter and Kaggle are hands down the best. If you constantly use the two, you pretty much are aware of the latest research trends and the things that work in practice.\nQ: If you had to offer a single piece of advice to aspiring data scientists or machine learning enthusiasts, what would it be, and why do you consider it so valuable?\nAs I said earlier, the most important thing is to learn the fundamental concepts. The biggest mistake I see people making, especially people starting their careers or those in the early stage, is that they get influenced(pun intended!) by the noise on social media. Also, learning “how” to use an algorithm and learning “when” to use an algorithm are two very different things.\nOne other unusual thing that I want to emphasize is that if you can afford higher education, go for it without a doubt. Degrees may not matter for your work, but they definitely help in your job search. I am not saying that you can’t do good without degrees, but you will get more opportunities easily if you have one. I couldn’t afford to apply for a master’s program after graduation, but if you can, you definitely should.\nQ: When you are given a problem statement, how do you devise AI solutions for it? What do you look for in a proposed solution?\nThe first thing I do before attempting anything else at all is to go through the data thoroughly. People naively jump on model building, but I tend to spend a lot of time with the data at hand. Once I understand the dataset, I look for a trivial solution that can be considered as a baseline. Defining metrics for a model and defining metrics that align with the business are two different aspects. We can’t ignore the latter. Putting models into production means adjusting to the constraints in the production environment. Blatantly scaling up/down doesn’t work for most of the scenarios.\nQ: You’ve always been bullish on JAX, the framework. What is it about JAX that you like? Why would you use JAX over say, TensorFlow or PyTorch? When you look at an ML framework, what attributes are you looking for?\nAlmost everything. Reproducibility is a first-class citizen, and so is parallelism in JAX. Have you tried sharding in JAX? I don’t think any other framework (past and present included) had such a good API design for implementing parallelism.\nI would use JAX over any other framework because it is efficient and much easier to optimize. People keep focusing on compilers for programming languages but don’t focus enough on the need for a compiler for deep learning workflows. In my opinion, Python is the perfect language, XLA is a great compiler, and JAX is the best framework for deep learning.\nIf I were to choose a framework today from a given list, here are the following things that I use as a checklist for evaluation:\n\nMental model\nEase of reproducibility\nEase of parallelism\nThe balance between low-level and high-level\nThe ecosystem\n\nI think the ecosystem is the part where JAX is lacking in a big way, but I hope with Keras 3, more people will contribute to the ecosystem, and it will catch up.\nQ: Can’t have an interview without asking this question to the Annotated Papers guy 😂: what is one research paper you’ve read recently which gave you that “aha” feeling? Basically, a paper which blew your mind!\nI wouldn’t say that I came across any paper in the last few months that blew my mind, but the latest paper from Apple titled MobileCLIP was a refreshing one. Deploying deep learning models on mobile devices is always a challenge, and requires a bit of rethinking, and modifications on the architectural side which aren’t that obvious when you are deploying things on a server. In that way, I think MobileCLIP is a must-read for anyone interested in those kinds of details.\nQ: Your open source work has mostly focused on vision. Has your work in the industry focused around vision as well? Or have you dabbled in different fields?\nHaha yes, my open-source work has mostly been focused on vision, but in terms of industrial experience, I have a T-shaped knowledge graph where I consider the depth of expertise in vision and width for other areas of Machine Learning. From 2017 to date, I have worked on business problems focused on traditional machine learning, time series, vision, speech, and NLP. I have been on the other side of ML as well, where I have focused on MLOPs along with my research work.\nComputer Vision is my favorite field, and the biggest reason for that is that when I focus on vision problems, I have a sense of “what’s happening” inside my pipelines. I find vision more mature compared to other fields, and I absolutely love working with images.\nQ: You’ve been in the industry for 6–7 years, and yet, manage to take time out to contribute to open source. What motivates you to keep contributing to open source?\nTwo major things. First, during my initial career phase, OSS played a big role in my learning and growth curves. Second, I consider solving problems in OSS as a mental exercise. The more you do it, the better you become. Your code being used by thousands of other developers is a true test of the logic baked in that code.\nQ: What’s the next big thing for Aakash Nain? :) What are you planning to work on next in open source? If you have any big announcements to make, this is the best place to make them 😂\nFor now, I will be focusing on making some tutorials for large-scale training using Keras and JAX. At some point, I will also develop another library based on JAX purely for fun, mostly focused on the vision side, given if I get enough time.\nAnshuman & Abheesht: Thank you, Aakash! It’s been great talking to you!\nAnd that concludes the fourth interview of our “AI Chronicles” series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an Applied Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "posts/text2sql_agent.html",
    "href": "posts/text2sql_agent.html",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "",
    "text": "Let me be honest… I don’t like writing SQL queries. And I always wished if someone could do it for me. Well that’s exactly what we are going to do today!\nWe are going to build an intelligent agent that does just that, using Google’s Gemini model and creating a neat web interface with Gradio.\nTraditional Text-to-SQL pipelines can sometimes be a bit like a black box. You give them a question, they spit out a SQL query, and you hope for the best.\nWhat’s the advantage of building something more “agent-like,” even in a simplified form?\nA standard text-to-sql pipeline can be brittle. The generated SQL query might be incorrect. Even worse, an incorrect query could run without errors but give you wrong or useless results, and you might not even realize it!\n👉 While a full-fledged agent system might involve complex reasoning and self-correction, our approach today focuses on building a robust pipeline with clear steps, good error handling when talking to the Large Language Model (LLM), and a way to learn from the process. This lays the foundation for more advanced agentic behavior later.\nLet’s build this agent! 💪\nAnd by the way, all the code related to this guide can be found here on my github."
  },
  {
    "objectID": "posts/text2sql_agent.html#the-big-picture-our-agents-workflow",
    "href": "posts/text2sql_agent.html#the-big-picture-our-agents-workflow",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "The Big Picture: Our Agent’s Workflow",
    "text": "The Big Picture: Our Agent’s Workflow\nBefore we dive into code, let’s look at the overall flow of our system. It’s a simple, logical sequence: a user’s question comes in one end, and an answer from the database comes out the other."
  },
  {
    "objectID": "posts/text2sql_agent.html#setting-the-stage---our-database",
    "href": "posts/text2sql_agent.html#setting-the-stage---our-database",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "Setting the Stage - Our Database",
    "text": "Setting the Stage - Our Database\nFirst things first, every SQL agent needs a database to talk to! For this tutorial, we’ll use SQLite, which is super convenient because it’s a file-based database and doesn’t require a separate server.\nOur Data: We’ll imagine a simple company database with two main tables:\n\ndepartments: Stores information about different departments.\nemployees: Stores information about employees and which department they belong to.\n\nHere’s the structure (schema) we’ll aim for:\n\nThis setup allows us to ask interesting questions like “Which employees work in Engineering?” or “What’s the average salary in the HR department?”.\nTo bring this to life, we need a script to create and populate this database. This ensures our agent always has a consistent environment to work in.\nFor those who want to see exactly how the database is created, you can check out the database_setup.py file in the GitHub repo. It’s a straightforward Python script using the sqlite3 library.\nThis script, when called by our UI application, will ensure we always have a fresh database with this structure and data for our agent to work with."
  },
  {
    "objectID": "posts/text2sql_agent.html#architecting-our-agent",
    "href": "posts/text2sql_agent.html#architecting-our-agent",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "Architecting Our Agent",
    "text": "Architecting Our Agent\nNow let’s make our agent! Since the goal of our repository is to build several different kinds of agents from scratch, we need a clean, reusable structure.\nWhy a Base Class? We use a base class to define a common “contract” or interface. This ensures all our agents, whether for Text-to-SQL or another task, have a consistent design. It’s like saying every vehicle must have a run method, even if a car and a motorcycle implement it differently. This makes our project scalable and easy to understand.\nWe’ll define a simple Agent base class in agents/base_agent.py that requires every agent to have a run method.\n# agents/base_agent.py\nfrom abc import ABC, abstractmethod\n\nclass Agent(ABC):\n    def __init__(self, name: str):\n        self._internal_name = name\n\n    @abstractmethod\n    def run(self, user_input: str, history: list[tuple[str, str]] = None) -&gt; str:\n        pass\n\n    @property\n    def name(self) -&gt; str:\n        return self._internal_name\nOur Text2SQLAgent will inherit from this, guaranteeing it fits into our agent framework."
  },
  {
    "objectID": "posts/text2sql_agent.html#the-core-logic-of-our-text2sql-agent",
    "href": "posts/text2sql_agent.html#the-core-logic-of-our-text2sql-agent",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "The Core Logic of our Text2SQL Agent",
    "text": "The Core Logic of our Text2SQL Agent\nOur agent’s job can be broken down into a few key steps. Let’s look at the concepts behind the main methods.\n\nGetting the Database Schema For an LLM to generate correct SQL, it must know the structure of your database. We need a method, let’s call it _get_database_schema, that connects to our SQLite file and programmatically extracts all table names, column names, and their data types. This schema is then passed to the LLM as part of the context.\nGenerating the SQL Query This is the heart of the agent, where we talk to Gemini. The process is called prompt engineering. We need to write a very clear set of instructions (a “prompt”) for the model.\n\nOur prompt includes:\n\nThe Role: “You are an expert SQLite SQL query generator.”\nThe Context: The full database schema we just extracted.\nThe Task: The user’s question.\nThe Constraints: “Output ONLY the SQL query. No extra text or markdown.”\n\nWe then wrap this prompt and send it to the Gemini API using Google’s Python SDK.\n# A conceptual look at the prompt structure inside the _generate_sql_with_gemini method\n\nsystem_instruction_text = \"You are an expert SQLite SQL query generator.\"\n\nprompt_for_task = f\"\"\"\nDatabase Schema:\n---\n{self.schema} \n---\nTask: ...\nConstraints: ...\n\nUser Question: {question}\n\nSQL Query:\n\"\"\"\nfull_prompt_for_gemini = f\"{system_instruction_text}\\n\\n{prompt_for_task}\"\nresponse = self.gemini_model.generate_content(full_prompt_for_gemini, ...)\nFull code can be found here.\n\nExecuting the Query Once Gemini returns what we hope is a valid SQL query, our agent needs to run it. A method _execute_sql_query connects to the database, executes the string, and fetches the results.\n\nA quick note on security: In a real-world application, directly running LLM-generated SQL can be risky. For this educational project, we’re keeping it simple, but in production, you would add validation layers or use read-only database permissions.\nOur Adventure with Gemini: From Bugs to Best Practices\nWorking with powerful LLMs is an iterative process. It’s rarely perfect on the first try! Here are some real challenges we faced and the key lessons learned.\nBest Practices We Adopted:\n\nClear, Instructive Prompts: Be explicit about the role, context, task, and constraints.\nConfiguration for Determinism: Using temperature=0.0 for SQL generation tells the model to be predictable and factual, not creative.\nHandling API Responses Carefully: Never assume the LLM’s response will be perfect. Always check for errors, finish_reason, and empty content before processing the output.\n\nBugs Along the Way (and How We Squashed Them!):\n\nThe “Empty Schema” Mystery: At first, our agent wasn’t sending the schema! Gemini received an empty prompt and couldn’t work. By meticulously printing the exact prompt string before sending it, we found the bug in our schema-loading logic and fixed it.\nThe MAX_TOKENS Puzzle: Even with the schema, our first model (a preview version) would often fail with finish_reason: MAX_TOKENS and no output. It seemed to be getting confused by the long, complex prompt.\n\n\nThe “Aha!” Moment: We used the “Extreme Prompt Simplification Test” we replaced our complex prompt with a simple question like “What is the capital of France?”. When that worked, it proved our API setup was fine and the issue was with how that specific model handled our complex prompt.\nThe Fix: Increasing max_output_tokens resolved this!\n\n\nThe response.text Trap: Using response.text to get the output can fail if the model’s response is blocked or empty. We learned to robustly parse response.candidates[0].content.parts instead.\n\nKey Takeaway: Debugging LLM interactions is an art. Log your inputs (prompts) and its outputs (raw responses) very carefully!"
  },
  {
    "objectID": "posts/text2sql_agent.html#bringing-our-agent-to-life-with-gradio",
    "href": "posts/text2sql_agent.html#bringing-our-agent-to-life-with-gradio",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "Bringing Our Agent to Life with Gradio",
    "text": "Bringing Our Agent to Life with Gradio\nNow, let’s build a cool UI! We want a chat window with clickable question suggestions on the side. For this custom layout, we’ll use Gradio’s gr.Blocks().\n(Here you could insert a screenshot of the final Gradio UI)\nThe UI has three main parts:\n\nThe Chatbot Display: A gr.Chatbot component shows the history of the conversation.\nThe Suggestion Column: We create a gr.Column and loop through a list of example questions, creating a gr.Button for each. The magic happens in the .click() event handler for each button, which updates the main input box.\nThe Input Area: A gr.Textbox for typing messages and a “Send” gr.Button. Both are wired to a single function, handle_chat_submission, which orchestrates the interaction with our agent.\n\nThis function takes the user’s message and the chat history, passes them to our agent’s .run() method, gets the result, and updates the chat display.\nFor the specific code that wires up these Gradio components, feel free to check out ui/app_text2sql.py in the repository."
  },
  {
    "objectID": "posts/text2sql_agent.html#how-good-is-our-agent-a-note-on-evaluation",
    "href": "posts/text2sql_agent.html#how-good-is-our-agent-a-note-on-evaluation",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "How Good is Our Agent? A Note on Evaluation",
    "text": "How Good is Our Agent? A Note on Evaluation\nCreating an agent is just the first step. To know if it’s truly useful, especially for production, we need to evaluate it.\nKey Evaluation Metrics for Text-to-SQL:\n\nExecution Accuracy: This is the most important one. Does the generated SQL run without errors and produce the correct answer? You’d need a test dataset of questions and their known correct answers to verify this.\nQuery Match: Does the generated SQL query exactly match a “golden” or reference SQL query? This is very strict and often not necessary, as multiple different SQL queries can produce the same correct result.\nRobustness: How well does the agent handle ambiguous questions, questions about things not in the schema, or slightly malformed inputs?\n\nFor a real project, you would build a “test suite” of many question-query-answer triplets and run your agent against it to calculate these metrics automatically. This helps track improvements as you refine your prompts or change models."
  },
  {
    "objectID": "posts/text2sql_agent.html#conclusion",
    "href": "posts/text2sql_agent.html#conclusion",
    "title": "Building Production Ready Text-to-SQL Agent from Scratch",
    "section": "Conclusion",
    "text": "Conclusion\nAnd there you have it! We’ve journeyed from understanding the need for a robust Text-to-SQL solution, setting up our database, architecting a scalable agent structure, and diving deep into the logic using Gemini. We navigated some tricky debugging and wrapped it all in a user-friendly Gradio interface.\nYou’ve now built a functional Text-to-SQL agent from scratch! This project covers many important concepts: OOP, database interaction, LLM prompting, debugging, and web UI creation.\nThis project touches on many important concepts:\n\nObject-Oriented Programming (base classes, inheritance)\nDatabase Interaction (SQLite)\nLLM Prompt Engineering and API Usage (Gemini)\nDebugging and Iterative Development\nWeb UI Creation (Gradio)\n\nFeel free to experiment further:\n\nMake the agent use conversation history for follow-up questions.\nAdd more error handling or self-correction attempts if the SQL fails.\nTry different LLMs or prompting strategies.\nExpand the Gradio UI with more features!\n\nHappy coding, and keep building amazing things!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "Hi there 👋. I am Anshuman Mishra. I work on large language models yet I like to call myself a software engineer.\nI work on large language models at FlipAI.\n\nProfessional Experience\n\nIndependent AI Consultant, Google\n\nFeb 2025 - Present\nWorking with Keras team\n\nML Engineer, FlipAI\n\nJuly 2023 - May 2025\nIn my current role, I am responsible for designing, building, and deploying machine learning models in a fast-paced production environment. My work involves the end-to-end ML lifecycle, from data preprocessing and model training to deployment and monitoring of core AI features.\n\nGoogle Summer of Code Mentee, Google\n\nJuly 2023 - May 2025\nWorked with Keras team.\n\nTechnical Author, Weights & Biases\n\nSeptember 2023 - February 2024\nDuring my time as a Technical Author, I focused on creating clear and compelling educational content for the machine learning community. I wrote in-depth articles, tutorials, and guides explaining complex ML concepts and demonstrating best practices using the W&B platform.\n\nData Science Intern, BNY Mellon\n\nJanuary 2023 - June 2023\nAs a Data Science Intern, I gained hands-on experience within the financial services industry. My responsibilities included analyzing large datasets to uncover insights, building predictive models, and presenting my findings to support data-driven decision-making.\n\nSoftware Engineer Intern, Amazon\n\nMay 2022 - July 2022\nDuring my internship at Amazon, I contributed to a team responsible for developing large-scale software systems. This experience provided a strong foundation in the software development lifecycle, including coding, debugging, and working with cloud infrastructure to deliver a project feature.\n\n\n\n\nSome stuff\n\nGoogle Developer Expert in Machine Learning and Google Cloud Platform (bit.ly/expert-sp)\nMentee, Google Summer of Code 2023, in TensorFlow. See my project here\nOpen Source Contributor at KerasNLP"
  },
  {
    "objectID": "pages/work.html",
    "href": "pages/work.html",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "As I venture into AI Consulting, I’m giving back to community via education. In this project I demonstrate how to build production ready AI Agent for task of text to sql generation and execution.\nThe assets related to this work can be found on X, github. I also published a blog for this. To stay upto-date with my work you can subscribe via substack.\n\n\n\n\n\nIn this blog we dive into Memory layer of AI and understand why stateful AI agents are path to hyperpersonalization and not the stateless once.\n\n\n\nI did this work while working as consultant for Keras team. Contributed the model to KerasHub. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making the state-of-the-art model accessible to the Keras community.\n\n\n\nI did this work while working as consultant for Keras team. Contributed the model to KerasHub. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making the state-of-the-art model accessible to the Keras community.\n\n\n\nThis work involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making Qwen-1.5-Mixture-of-Experts model model accessible to the Keras community. Challenges- High Memory. Mitigation- Llama4 based moe implementation, High Agency\n\n\n\nThrough this work I developed support for Mixtral model in Keras Hub, making it accessible to community."
  },
  {
    "objectID": "pages/work.html#section",
    "href": "pages/work.html#section",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "As I venture into AI Consulting, I’m giving back to community via education. In this project I demonstrate how to build production ready AI Agent for task of text to sql generation and execution.\nThe assets related to this work can be found on X, github. I also published a blog for this. To stay upto-date with my work you can subscribe via substack.\n\n\n\n\n\nIn this blog we dive into Memory layer of AI and understand why stateful AI agents are path to hyperpersonalization and not the stateless once.\n\n\n\nI did this work while working as consultant for Keras team. Contributed the model to KerasHub. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making the state-of-the-art model accessible to the Keras community.\n\n\n\nI did this work while working as consultant for Keras team. Contributed the model to KerasHub. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making the state-of-the-art model accessible to the Keras community.\n\n\n\nThis work involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making Qwen-1.5-Mixture-of-Experts model model accessible to the Keras community. Challenges- High Memory. Mitigation- Llama4 based moe implementation, High Agency\n\n\n\nThrough this work I developed support for Mixtral model in Keras Hub, making it accessible to community."
  },
  {
    "objectID": "pages/work.html#section-1",
    "href": "pages/work.html#section-1",
    "title": "Anshuman Mishra",
    "section": "2024",
    "text": "2024\n\nJMLR Paper on Keras Multi-framework Models Publication\nI co-authored a foundational paper for the Journal of Machine Learning Research (JMLR) detailing the multi-framework architecture of KerasCV and KerasNLP. The work showcases how these libraries were engineered to seamlessly support TensorFlow, PyTorch, and JAX.\n\nTitle: KerasCV and KerasNLP: Multi-framework Models\nPublished: Journal of Machine Learning Research (Volume 25, 2024)\nMy Role: Co-author, contributing to the framework development and experimental validation.\n\n\n\nYoutube video YouTube\n\n\n\n\nData hack summit Talk\nI gave this talk at Data Hack Summit by Analytics Vidhya. It covered CUDA C fundamentals and core CUDA concepts like threads, blocks, and memory hierarchies. The session guided attendees through writing their first CUDA kernels, aiming to build the skills needed to optimize deep learning workloads."
  },
  {
    "objectID": "pages/work.html#section-2",
    "href": "pages/work.html#section-2",
    "title": "Anshuman Mishra",
    "section": "2023",
    "text": "2023\nI contributed to open source (mainly Keras) heavily throught the year. Gave a lot of talks travelling across the country.\n\nSupercharge KerasNLP Models with Wandb Blog\nWhile at Weights & Biases, I authored this official report on fine-tuning KerasNLP models for Semantic Similarity. The guide demonstrates how to leverage W&B Sweeps on the SNLI dataset to find optimal hyperparameters and train a high-performing model.\nThis was also published on official keras website.\n\n\nGenerating High-quality Images with SD.Next, HuggingFace Diffusers and W&B Blog\nI documented my experiments with advanced diffusion models in this report. It’s a comprehensive guide to generating high-quality images with SD.Next and HuggingFace Diffusers, emphasizing the use of W&B for complete observability and reproducibility.\n\n\nLlama2 Open Source\nThe Llama2 was just released and community was going wild over it so I decided to add it in my gsoc project and contribute it to library. I applied all my learnings from GPTNeoX implementation in this project and learnt a ton of an amazing series of models to come, before 95% of the world even knew about it.\n\n\nGPT Neo X Open Source\nContributing GPTNeoX to KerasHub was main project that I did as part of GSoC. In first iteration I shipped GPTNeoX backbone and Preprocessor and by that time Keras3 was already in works. As soon as it reached a pre-release stable point I immediately ported the implementation to Keras3 to work with PyTorch and JAX as well. Later Matt and I realized that Rotary embedding layer that I wrote for GPT NeoX is gonna be fundamental component for Generative models so I shipped it as a generic layer. One more small PR followed\n\n\nBeam Sampler Open Source\nBeam Sampler already existed in KerasHub at the time, but I was curious about how it worked, so I took up the task of porting it to keras3.\n\n\nSupported rsqrt op in Keras API Open Source\nKeras API did not have inverse square root op. This op was being used in GPTNeoX’s layernorm layer. So I contributed this op to Keras.\n\n\nSupported Root Mean Square Scaling in keras.LayerNormalization Open Source\nGenerative models use root mean square scaling in layernorm layer which is an essential part of model architecture. This op was not available in official Keras API so I contributed it! [Keras - Add rms_scaling in LayerNormalization]\n\n\nAlbert Classifier Open Source\nAlbert was the first model on which I started working, for Keras. The backbone existed but there were no task layers. I contributed Classifier and Masked Language Modeling task layer for the model.\n\n\nMisc Bug Fixes Open Source\n\nFix RotaryEmbedding import\nFix Autograph error with perplexity metric\nFix ModuleNotFoundError keras_nlp.models.xlnet\nAdd compute_output_shape to tokenizer\nDefault compilation for Albert, Distilbert, Roberta MaskedLM\nCall super.config() in BartBackbone’s get_config()\nAdd API exports for tokenizers documented on keras.io\nAdd API exports for metrics documented on keras.io\nAdd API exports for samplers documented on keras.io\nAdd API exports for models documented on keras.io\nMove from_preset to base tokenizer classes\nAdd an add_prefix_space Arg in BytePairTokenizer\n\n\n\nData Parallel Training with KerasNLP Blog\nIn this guide I demonstrated how to significantly accelerate NLP model training using data parallelism. I showcased the process of fine-tuning a pretrained BERT model from Keras Hub across multiple GPUs, leveraging TensorFlow’s MirroredStrategy to efficiently scale deep learning workflows and dramatically reduce training time.\nThis guide was officially published on Keras website.\n\n\nExploratory Data Analysis Blog\nIn this Kaggle notebook, I developed a comprehensive template for Exploratory Data Analysis (EDA) on an enzyme substrate dataset. I demonstrated a full workflow, including visualizing feature correlations with heatmaps, analyzing distributions with histograms and box plots, and examining relationships between variables using pair plots to uncover key insights.\n\n\nTesting improvements Open Source\nSpeed up default RoBERTa testing roughly 3x Adding XXBackboneTPUTests\n\n\nXLMRoberta Open Source\nXLMRoberta backbone already existed but there were no task layers. I contributed Masked Language Modeling task layer for the model.\n\n\nTalks Talk\nIn later half of 2023, after I finished my GSoC, I decided to give back the knowledge I earned from Open Source, to community. So I flew 11 cities throughout India giving talks and workshops on various topics related to NLP. Few of them are listed below:\n\nKerasNLP: From Words to Wisdom, on October 7, 2023 at DevFest New Delhi’23 .\nModular NLP Workflows with KerasNLP, on September 29, 2023 at Google Developer Groups, Seattle.\nSupercharging Keras with WandB, on September 23, 2023 at TensorFlow User Group Mumbai.\nGSoC Success Secrets: Cracking the Code to Open Source Excellence, September 10, 2023 at National Institute of Technology, Warangal.\nRethinking LLM Design with KerasNLP, on August 26, 2023 at TensorFlow User Group Hyderabad.\nKerasNLP for Starters, on August 20, 2023 at TensorFlow User Group Durg\nTaking KerasNLP on GenAI Ride on July, 23, 2023 at TensorFlow User Group, Kolkata\nRe-imagining Keras in the evolving ML ecosystem on July 16, 2023 at Google I/O Extended’23 New Delhi.\n\n\n\nJulia Contributions Open Source\nMetalhead is FluxML’s (a julia deep learning package) official model zoo. I contributed the UNet image segmentation model to Metalhead.jl, Julia’s core computer vision library. By implementing the architecture with reference to the official PyTorch version, I made this popular model for semantic segmentation natively available to the FluxML community, fulfilling a key feature request for the library."
  },
  {
    "objectID": "pages/work.html#section-3",
    "href": "pages/work.html#section-3",
    "title": "Anshuman Mishra",
    "section": "2022",
    "text": "2022\nThis year I mainly focused on Kaggling. It also marked my venture into Open Source world.\n\nCNN From Tensorflow to Pytorch Open Source\nIn 2022, Deepchem Project decided to migrate from tensorflow to pytorch. I contributed to this mission by porting their generic, scalable CNN models and layers to torch. I also fixed couple (1, 2) of bugs in the library that have gone unnoticed.\n\n\nNeural Ordinary Differential Equations Open Source\n[Deepchem - Neural ODE tutorial 📚]\nThis tutorial demonstrates how to implement Neural Ordinary Differential Equations (Neural ODEs) within the DeepChem library by leveraging the torchdiffeq package. It explains how, instead of using discrete layers, a neural network can parameterize the derivative of a system’s hidden state.\nThe guide walks through embedding a torchdiffeq ODE solver inside a PyTorch neural network to model continuous-time dynamics, making it possible to predict the future states of dynamic systems within the DeepChem ecosystem.\n\n\nG2Net Detecting Continuous Gravitational Waves Kaggle\nI scored a bronze medal in this competition. The goal of this competition was to find continuous gravitational-wave signals. I developed a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data. Here is one of the competition notebooks that shows my approach to problem.\n\n\nICR - Identifying Age-Related Conditions Kaggle\nThe goal of this competition was to predict if a person has any of any medical conditions. Here are two notebooks that I submitted for the competition. One of them used an ensemble based technique and the other used just LigthGBM. I used optuna library for hyperparameter optimization.\n\n\nSupport safe serialization while pushing to Huggingface Hub from H2O llmstudioOpen Source\nPR\n\n\nDeepmind - optax Open Source\nThis is how I started open source. My first-ever contribution was a jax library called optax by google deepmind."
  }
]