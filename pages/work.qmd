---
# title: Work
aliases: 
  - /work
toc: true
---

## 2025

<!-- ### Building Production Ready Text to SQL Agent <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span> <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span><span style="font-size: 0.65em; font-weight: 600; background-color: #FEE2E2; color: #991B1B; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">YouTube</span>

As I venture into AI Consulting, I'm giving back to community via education. In this project I demonstrate how to build production ready AI Agent for task of text to sql generation and execution.

The assets related to this work can be found on [X](https://x.com/kanpuriyanawab/status/1931018241419477078), [github](https://github.com/kanpuriyanawab/awesome-ai-agents/tree/main). I also published [a blog](https://heyyanshuman.com/posts/text2sql_agent) for this. 
To stay upto-date with my work you can subscribe via [substack](https://fullstackagents.substack.com/p/building-production-ready-text-to). -->


<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/ezOhLC10uSQ?si=eC1KW1yHw6vw-ozV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe> -->


<!-- ### The AI Memory Layer That will change everything <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span>

In this [blog](https://heyyanshuman.com/posts/agent_memory_101) we dive into Memory layer of AI and understand why stateful AI agents are path to hyperpersonalization and not the stateless once. -->


### Qwen Model Family Integration in Keras  <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

As a consultant for the Keras team, I led the integration of the state-of-the-art **Qwen model family** into KerasHub. This work made multiple versions of this powerful generative model, including its advanced Mixture-of-Experts variant, accessible to the global Keras community.

* **Qwen 3 & Qwen 2.5:** For both the [Qwen 3](https://github.com/keras-team/keras-hub/pull/2249) and [Qwen 2.5](https://github.com/keras-team/keras-hub/pull/2088) models, my contributions included implementing the core backbone, custom attention mechanisms, and building a robust checkpoint conversion pipeline.

* **Qwen-1.5 Mixture-of-Experts (MoE):** The integration of the [Qwen-1.5-MoE model](https://github.com/keras-team/keras-hub/pull/2163) presented unique challenges, particularly its high memory footprint. To address this, I engineered a memory-efficient MoE layer, adapting techniques from other advanced implementations to ensure the model was performant and accessible on standard hardware.


### Mixtral <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
Through this work I developed support for [Mixtral](https://github.com/keras-team/keras-hub/pull/2196) model in Keras Hub, making it accessible to community.

## 2024

### JMLR Paper on Keras Multi-framework Models <span style="font-size: 0.65em; font-weight: 600; background-color: #FEF9C3; color: #854d0e; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Publication</span>

I co-authored a foundational paper for the **Journal of Machine Learning Research (JMLR)** detailing the multi-framework architecture of KerasCV and KerasNLP. The work showcases how these libraries were engineered to seamlessly support TensorFlow, PyTorch, and JAX.

* **Title:** [*KerasCV and KerasNLP: Multi-framework Models*](http://jmlr.org/papers/v25/24-0404.html)
* **Published:** Journal of Machine Learning Research (Volume 25, 2024)
* **My Role:** Co-author, contributing to the framework development and experimental validation.

### Youtube video <span style="font-size: 0.65em; font-weight: 600; background-color: #FEE2E2; color: #991B1B; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">YouTube</span>


<iframe width="560" height="315" src="https://www.youtube.com/embed/2Xo11kjJiZk?si=8WjEXpvuFG8jRfmx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### Data hack summit <span style="font-size: 0.65em; font-weight: 600; background-color: #dcfce7; color: #166534; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Talk</span>

I gave [this](https://www.analyticsvidhya.com/datahacksummit-2024/sessions/demystify-parallel-programming-hands-on-with-cuda-for-genai) talk at Data Hack Summit by Analytics Vidhya. It covered CUDA C fundamentals and core CUDA concepts like threads, blocks, and memory hierarchies. The session guided attendees through writing their first CUDA kernels, aiming to build the skills needed to optimize deep learning workloads.

## 2023

*I contributed to open source (mainly Keras) heavily throught the year. Gave a lot of talks travelling across the country.*

<!-- ### Supercharge KerasNLP Models with Wandb <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span>

While at Weights & Biases, I authored this official [report](https://wandb.ai/kanpuriyanawab/keras-nlp-x-wandb/reports/Supercharge-KerasNLP-Models-with-Wandb--Vmlldzo1Mjk1NjI2) on fine-tuning KerasNLP models for Semantic Similarity. The guide demonstrates how to leverage W&B Sweeps on the SNLI dataset to find optimal hyperparameters and train a high-performing model.

This was also [published](https://keras.io/examples/nlp/semantic_similarity_with_keras_hub/) on official keras website. -->

<!-- ### Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&B <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span>

I documented my experiments with advanced diffusion models in this [report](https://wandb.ai/ml-colabs/automatic/reports/Generating-High-quality-Images-with-SD-Next-HuggingFace-Diffusers-and-W-B--Vmlldzo1NTYzMzQy). It's a comprehensive guide to generating high-quality images with SD.Next and HuggingFace Diffusers, emphasizing the use of W&B for complete observability and reproducibility. -->

### Llama2 <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

The Llama2 was just released and community was going wild over it so I decided to add it in my gsoc project and [contribute](https://github.com/keras-team/keras-nlp/pull/1203) it to library. I applied all my learnings from GPTNeoX implementation in this project and learnt a ton of an amazing series of models to come, before 95% of the world even knew about it.

### GPT Neo X <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

Contributing GPTNeoX to KerasHub was main project that I did as part of GSoC. In first iteration I shipped [GPTNeoX backbone](https://github.com/keras-team/keras-nlp/pull/1056) and [Preprocessor](https://github.com/keras-team/keras-nlp/pull/1093) and by that time Keras3 was already in works. As soon as it reached a pre-release stable point I immediately [ported the implementation to Keras3](https://github.com/keras-team/keras-nlp/pull/1137) to work with PyTorch and JAX as well. Later [Matt](https://github.com/mattdangerw) and I realized that Rotary embedding layer that I [wrote](https://github.com/keras-team/keras-nlp/pull/1101) for GPT NeoX is gonna be fundamental component for Generative models so I shipped it as [a generic layer](https://github.com/keras-team/keras-nlp/pull/1180). One more small PR followed 



### Beam Sampler <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

Beam Sampler already existed in KerasHub at the time, but I was curious about how it worked, so I took up the task of [porting it to keras3](https://github.com/keras-team/keras-nlp/pull/1181).


### Supported `rsqrt` op in Keras API <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
Keras API did not have inverse square root op. This op was being used in GPTNeoX's layernorm layer. So I [contributed](https://github.com/keras-team/keras-core/pull/708) this op to Keras.

### Supported Root Mean Square Scaling in keras.LayerNormalization <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
Generative models use root mean square scaling in layernorm layer which is an essential part of model architecture. This op was not available in official Keras API so I [contributed](https://github.com/keras-team/keras-core/pull/726) it! 
[Keras - Add rms_scaling in LayerNormalization]

### Albert Classifier <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

Albert was the first model on which I started working, for Keras. The backbone existed but there were no task layers. I contributed [Classifier]((https://github.com/keras-team/keras-nlp/pull/668)) and [Masked Language Modeling]((https://github.com/keras-team/keras-nlp/pull/725)) task layer for the model.



<!-- 
### Data Parallel Training with KerasNLP <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span>
In this guide I demonstrated how to significantly accelerate NLP model training using data parallelism. I showcased the process of fine-tuning a pretrained BERT model from Keras Hub across multiple GPUs, leveraging TensorFlow's MirroredStrategy to efficiently scale deep learning workflows and dramatically reduce training time. 

This guide was officially [published](https://keras.io/examples/nlp/data_parallel_training_with_keras_hub/) on Keras website.


### Exploratory Data Analysis <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Blog</span>

In [this](https://www.kaggle.com/code/shivanshuman/one-stop-eda) Kaggle notebook, I developed a comprehensive template for Exploratory Data Analysis (EDA) on an enzyme substrate dataset. I demonstrated a full workflow, including visualizing feature correlations with heatmaps, analyzing distributions with histograms and box plots, and examining relationships between variables using pair plots to uncover key insights. -->


### Testing improvements <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
[Speed up default RoBERTa testing roughly 3x](https://github.com/keras-team/keras-nlp/pull/897)
[Adding XXBackboneTPUTests](https://github.com/keras-team/keras-nlp/pull/839)


### XLMRoberta <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

XLMRoberta backbone already existed but there were no task layers. I contributed [Masked Language Modeling](https://github.com/keras-team/keras-nlp/pull/950) task layer for the model.

### Talks <span style="font-size: 0.65em; font-weight: 600; background-color: #dcfce7; color: #166534; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Talk</span>
In later half of 2023, after I finished my GSoC, I decided to give back the knowledge I earned from OSS, to community. So I flew 11 cities throughout India giving talks and workshops on various topics related to NLP. Few of them are listed below:


1. [KerasNLP: From Words to Wisdom](https://twitter.com/gdg_nd/status/1709132486238724550?t=9V1JG6XwqQNJz6MEuiGqPA&s=08), on October 7, 2023 at DevFest New Delhi’23 .

2. [Modular NLP Workflows with KerasNLP](https://x.com/margaretmz/status/1706717545007546491?s=20), on September 29, 2023 at Google Developer Groups, Seattle.

3. [Supercharging Keras with WandB](https://twitter.com/tfugmumbai/status/1705456979756613978?s=20), on September 23, 2023 at TensorFlow User Group Mumbai.

4. [GSoC Success Secrets: Cracking the Code to OSS Excellence](https://www.linkedin.com/feed/update/urn:li:activity:7105945831630938112/), September 10, 2023 at National Institute of Technology, Warangal.

5. [Rethinking LLM Design with KerasNLP](https://twitter.com/kanpuriyanawab/status/1694540731783028926?s=20), on August 26, 2023 at TensorFlow User Group Hyderabad.

6. [KerasNLP for Starters](https://www.youtube.com/watch?v=KinLN4lA6Bg&t=3450s), on August 20, 2023 at TensorFlow User Group Durg

7. [Taking KerasNLP on GenAI Ride]() on July, 23, 2023 at TensorFlow User Group, Kolkata

8. [Re-imagining Keras in the evolving ML ecosystem](https://www.linkedin.com/posts/kanpuriyanawab_event-developercommunity-congratulations-activity-7086588533510934528-dS2y) on July 16, 2023 at Google I/O Extended'23 New Delhi.




### Julia Contributions <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
Metalhead is FluxML's (a julia deep learning package) official model zoo. I [contributed]((https://github.com/FluxML/Metalhead.jl/pull/210)) the UNet image segmentation model to Metalhead.jl, Julia's core computer vision library. By implementing the architecture with reference to the official PyTorch version, I made this popular model for semantic segmentation natively available to the FluxML community, fulfilling a key feature request for the library.

### Misc Bug Fixes <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

- [Fix RotaryEmbedding import](https://github.com/keras-team/keras-nlp/pull/1217)
- [Fix Autograph error with perplexity metric](https://github.com/keras-team/keras-nlp/pull/1211)
- [Fix ModuleNotFoundError keras_nlp.models.xlnet](https://github.com/keras-team/keras-nlp/pull/1204)
- [Add compute_output_shape to tokenizer](https://github.com/keras-team/keras-nlp/pull/1166)
- [Default compilation for Albert, Distilbert, Roberta MaskedLM](https://github.com/keras-team/keras-nlp/pull/833)
- [Call super.config() in BartBackbone's get_config()](https://github.com/keras-team/keras-nlp/pull/818)
- [Add API exports for tokenizers documented on keras.io](https://github.com/keras-team/keras-nlp/pull/817)
- [Add API exports for metrics documented on keras.io](https://github.com/keras-team/keras-nlp/pull/816)
- [Add API exports for samplers documented on keras.io](https://github.com/keras-team/keras-nlp/pull/815)
- [Add API exports for models documented on keras.io](https://github.com/keras-team/keras-nlp/pull/814)
- [Move from_preset to base tokenizer classes](https://github.com/keras-team/keras-nlp/pull/673)
- [Add an add_prefix_space Arg in BytePairTokenizer](https://github.com/keras-team/keras-nlp/pull/715)

## 2022

*This year I mainly focused on Kaggling. It also marked my venture into OSS world.*

### CNN From Tensorflow to Pytorch <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>

In 2022, [Deepchem Project](https://deepchem.io) decided to migrate from tensorflow to pytorch. I [contributed](https://github.com/deepchem/deepchem/pull/2963) to this mission by porting their generic, scalable CNN models and layers to torch. I also fixed couple ([1]((https://github.com/deepchem/deepchem/pull/2988)), [2](https://github.com/deepchem/deepchem/pull/2964)) of bugs in the library that have gone unnoticed. 




### Neural Ordinary Differential Equations <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
[Deepchem - Neural ODE tutorial 📚]

This [tutorial](https://deepchem.io/tutorials/about-node-using-torchdiffeq-in-deepchem/) demonstrates how to implement Neural Ordinary Differential Equations (Neural ODEs) within the DeepChem library by leveraging the torchdiffeq package. It explains how, instead of using discrete layers, a neural network can parameterize the derivative of a system's hidden state.

The [guide]((https://github.com/deepchem/deepchem/pull/2859)) walks through embedding a torchdiffeq ODE solver inside a PyTorch neural network to model continuous-time dynamics, making it possible to predict the future states of dynamic systems within the DeepChem ecosystem.



### G2Net Detecting Continuous Gravitational Waves <span style="font-size: 0.65em; font-weight: 600; background-color: #E0F3FF; color: #196F9E; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Kaggle</span>

I scored a bronze medal in this competition. The goal of this competition was to find continuous gravitational-wave signals. I developed a model sensitive enough to detect weak yet long-lasting signals emitted by rapidly-spinning neutron stars within noisy data. Here is one of the competition [notebooks]((https://www.kaggle.com/code/shivanshuman/kaggle-t4x2-multi-gpu-training-g2net)) that shows my approach to problem.


### ICR - Identifying Age-Related Conditions <span style="font-size: 0.65em; font-weight: 600; background-color: #E0F3FF; color: #196F9E; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">Kaggle</span>
The goal of this competition was to predict if a person has any of any medical conditions. Here are two notebooks that I submitted for the competition. One of them used an [ensemble based technique]((https://www.kaggle.com/code/shivanshuman/lb-0-23-ensemble-catboost-xgboost-lightgbm)) and the other used just LigthGBM. I used [optuna]((https://www.kaggle.com/code/shivanshuman/lightgbm-optuna-baseline)) library for hyperparameter optimization.



### Support safe serialization while pushing to Huggingface Hub from H2O llmstudio<span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span>
[PR](https://github.com/h2oai/h2o-llmstudio/pull/221)


### Deepmind - optax <span style="font-size: 0.65em; font-weight: 600; background-color: #fce8d1; color: #c55a11; padding: 3px 9px; border-radius: 12px; vertical-align: middle; text-transform: uppercase; letter-spacing: 0.5px;">OSS</span> 
[This]((https://github.com/google-deepmind/optax/pull/413)) is how I started open source. My first-ever contribution was a jax library called optax by google deepmind.
