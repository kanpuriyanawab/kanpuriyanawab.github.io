---
# title: Work
aliases: 
  - /work
toc: true
---

## 2025

TLDR

### Qwen2.5
**[Add Qwen 2.5 to KerasHub](https://github.com/keras-team/keras-hub/pull/2088)**
*üöÄ Contribution to [KerasHub](https://github.com/keras-team/keras-hub)*
*Led the integration of the powerful Qwen 2.5 language model. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making a state-of-the-art model accessible to the Keras community.*


### Qwen Moe
[KerasHub - Add Qwen 1.5 Moe](https://github.com/keras-team/keras-hub/pull/2163)

### Mixtral
[KerasHub - Add Mixtral](https://github.com/keras-team/keras-hub/pull/2196)


## 2024

### Weights & Biases
[Supercharge KerasNLP Models with Wandb](https://wandb.ai/kanpuriyanawab/keras-nlp-x-wandb/reports/Supercharge-KerasNLP-Models-with-Wandb--Vmlldzo1Mjk1NjI2)

### Weights & Biases

[Generating High-quality Images with SD.Next, HuggingFace Diffusers and W&B](https://wandb.ai/ml-colabs/automatic/reports/Generating-High-quality-Images-with-SD-Next-HuggingFace-Diffusers-and-W-B--Vmlldzo1NTYzMzQy)


## 2023

tl;dr - how many talks, open source contributions

### Llama

[KerasNLP - Add Llama Backbone](https://github.com/keras-team/keras-nlp/pull/1203)

### GPT Neo X

-- Tell a story

[KerasNLP - Generic RotaryEmbedding Layer](https://github.com/keras-team/keras-nlp/pull/1180)
[KerasNLP - Port GPTNeoX to KerasCore](https://github.com/keras-team/keras-nlp/pull/1137)
[KerasNLP - Adding GPTNeoXBackbone](https://github.com/keras-team/keras-nlp/pull/1056)
[Add GPTNeoXPreprocessor](https://github.com/keras-team/keras-nlp/pull/1093)
[Refactor RotaryEmbedding and GPTNeoXAttention](https://github.com/keras-team/keras-nlp/pull/1101)


### Beam Sampler

[Port BeamSampler to core](https://github.com/keras-team/keras-nlp/pull/1181)

### Keras Core layer contributions
[Keras - Add rsqrt to ops API](https://github.com/keras-team/keras-core/pull/708)
[Keras - Add rms_scaling in LayerNormalization](https://github.com/keras-team/keras-core/pull/726)

### Albert Classifier

[Add AlbertClassifier](https://github.com/keras-team/keras-nlp/pull/668)
[Adding an AlbertMaskedLM task + Fix Projection layer dimension in MaskedLMHead](https://github.com/keras-team/keras-nlp/pull/725)


### Misc Bug Fixes

[Fix RotaryEmbedding import](https://github.com/keras-team/keras-nlp/pull/1217)
[Fix Autograph error with perplexity metric](https://github.com/keras-team/keras-nlp/pull/1211)
[Fix ModuleNotFoundError keras_nlp.models.xlnet](https://github.com/keras-team/keras-nlp/pull/1204)
[Add compute_output_shape to tokenizer](https://github.com/keras-team/keras-nlp/pull/1166)
[Default compilation for Albert, Distilbert, Roberta MaskedLM](https://github.com/keras-team/keras-nlp/pull/833)
[Call super.config() in BartBackbone's get_config()](https://github.com/keras-team/keras-nlp/pull/818)
[Add API exports for tokenizers documented on keras.io](https://github.com/keras-team/keras-nlp/pull/817)
[Add API exports for metrics documented on keras.io](https://github.com/keras-team/keras-nlp/pull/816)
[Add API exports for samplers documented on keras.io](https://github.com/keras-team/keras-nlp/pull/815)
[Add API exports for models documented on keras.io](https://github.com/keras-team/keras-nlp/pull/814)
[Move from_preset to base tokenizer classes](https://github.com/keras-team/keras-nlp/pull/673)

[Add an add_prefix_space Arg in BytePairTokenizer](https://github.com/keras-team/keras-nlp/pull/715)

### Tutorials

[Port [KerasNLP] Transformer Pretraining guide to multi-backend Keras](https://github.com/keras-team/keras-io/pull/1438)

### Tutorials
[fixing getting started guide kerasnlp](https://github.com/keras-team/keras-io/pull/1414)

### Tutorials
[Add example : Data Parallel Training with KerasNLP](https://github.com/keras-team/keras-io/pull/1395)

### Tutorial
[Add Semantic Similarity with KerasNLP tutorial](https://github.com/keras-team/keras-io/pull/1299)

### Tutorial
https://github.com/keras-team/keras-io/blob/master/guides/keras_hub/transformer_pretraining.py

### Talk

[KerasNLP: From Words to Wisdom](https://twitter.com/gdg_nd/status/1709132486238724550?t=9V1JG6XwqQNJz6MEuiGqPA&s=08), on October 7, 2023 at DevFest New Delhi‚Äô23 .

### Kaggle
[One Stop EDA](https://www.kaggle.com/code/shivanshuman/one-stop-eda)


### Testing improvements
[Speed up default RoBERTa testing roughly 3x](https://github.com/keras-team/keras-nlp/pull/897)
[Adding XXBackboneTPUTests](https://github.com/keras-team/keras-nlp/pull/839)


### XLMRoberta

[Add an XLMRobertaMaskedLM task model](https://github.com/keras-team/keras-nlp/pull/950)

### talk

[Modular NLP Workflows with KerasNLP](https://x.com/margaretmz/status/1706717545007546491?s=20), on September 29, 2023 at Google Developer Groups, Seattle.

### talk

[Supercharging Keras with WandB](https://twitter.com/tfugmumbai/status/1705456979756613978?s=20), on September 23, 2023 at TensorFlow User Group Mumbai.


### talk

[GSoC Success Secrets: Cracking the Code to Open Source Excellence](https://www.linkedin.com/feed/update/urn:li:activity:7105945831630938112/), September 10, 2023 at National Institute of Technology, Warangal.


### talk

[Rethinking LLM Design with KerasNLP](https://twitter.com/kanpuriyanawab/status/1694540731783028926?s=20), on August 26, 2023 at TensorFlow User Group Hyderabad.


### talk

[KerasNLP for Starters](https://www.youtube.com/watch?v=KinLN4lA6Bg&t=3450s), on August 20, 2023 at TensorFlow User Group Durg


### talk

[Taking KerasNLP on GenAI Ride]() on July, 23, 2023 at TensorFlow User Group, Kolkata


### talk

[Re-imagining Keras in the evolving ML ecosystem](https://www.linkedin.com/posts/kanpuriyanawab_event-developercommunity-congratulations-activity-7086588533510934528-dS2y) on July 16, 2023 at Google I/O Extended'23 New Delhi.


### Julia Contributions
[FluxML - Adding UNet Model](https://github.com/FluxML/Metalhead.jl/pull/210)

## 2022

### CNN From Tensorflow to Pytorch

[Deepchem - Porting CNN from TF ‚û°Ô∏è PyTorch ](https://github.com/deepchem/deepchem/pull/2963)

### NeuralODE
[Deepchem - Neural ODE tutorial üìö](https://github.com/deepchem/deepchem/pull/2859)


### [h2o-llmstudio](https://github.com/h2oai/h2o-llmstudio)
[add: safe serialization while pushing to hf](https://github.com/h2oai/h2o-llmstudio/pull/221)



### [ Deepmind - optax](https://github.com/google-deepmind/optax)

- [added typing to linear_algebra.py](https://github.com/google-deepmind/optax/pull/413)



### Misc Bug fixes

[:octocat: Fixed potential bug in deepchem's CNN implementation](https://github.com/deepchem/deepchem/pull/2964)

[Fixing load_qm7_from_mat() not found](https://github.com/deepchem/deepchem/pull/2988)


### Kaggle 
[Kaggle T4x2 multi-gpu training G2Net](https://www.kaggle.com/code/shivanshuman/kaggle-t4x2-multi-gpu-training-g2net)

### Kaggle 

[LB:0.23 Ensemble {CatBoost, XGBoost, LightGBM}](https://www.kaggle.com/code/shivanshuman/lb-0-23-ensemble-catboost-xgboost-lightgbm)

### Kaggle 
[LightGBM + Optuna Baseline](https://www.kaggle.com/code/shivanshuman/lightgbm-optuna-baseline)





## interviews 
-   [An Interview with **Ravi Ramakrishnan**, Kaggle Grandmaster](https://kanpuriyanawab.medium.com/interview-with-2x-kaggle-grandmaster-data-science-manager-at-emirates-nbd-ravi-ramakrishnan-3308a3936c07) (joint work with [Abheesht Sharma](https://www.linkedin.com/in/kanpuriyanawab/))
-   [An Interview with **Rajaswa Patil**, AI Research Associate at Postman Labs](https://medium.com/@sharmabhee/interview-with-rajaswa-patil-ai-researcher-at-postman-labs-4c4537f98ee) (joint work with [Abheesht Sharma](https://www.linkedin.com/in/kanpuriyanawab/))
-   [An Interview with **Matthew Watson**, Senior Software Engineer, Google](https://kanpuriyanawab.medium.com/interview-with-matthew-watson-keras-team-google-a5df3913ab95) (joint work with [Abheesht Sharma](https://www.linkedin.com/in/kanpuriyanawab/))
-   [An Interview with Aakash Kumar Nain, MLE at Merlyn Minds](https://medium.com/ai-chronicles/interview-with-aakash-kumar-nain-mle-at-merlyn-minds-785f0f2e147b) (joint work with [Abheesht Sharma](https://www.linkedin.com/in/kanpuriyanawab/))

I have had an amazing time interviewing these incredible folks and I am grateful to them. For other competing priorities, I am discontinuing the series for an indefinite period. If you enjoy reading through interviews like these, you might want to check out the [**Machine Learning Street Talk**](https://www.youtube.com/channel/UCMLtBahI5DMrt0NPvDSoIRQ) podcast on YouTube.  

