---
aliases:
- /flux_is_so_flexible
permalink: /flux_is_so_flexible
badges: false
toc: true
categories:
- Model Architecture
- Julia
date: '2022-12-31'
description: Implementing ML models in Flux, the Julia Deep LearningÂ Package!
hide: false
output-file: flux_is_so_flexible.html
search_exclude: false
title: Deep Learning inÂ Julia
---



[Flux](https://fluxml.ai/) is amazing and it's far more than just an ML framework. Differentiable programming & Zygote, first class GPU support are features that set it apart among ML systems. The best thing I love about flux is mixing neural nets with differential equations, to get the best of black box and mechanistic modelling, this is what [SciML](https://sciml.ai/) doing.

Flux is fairly new, and needs attention of communityÂ ! In this article we'll learn about how to implement a model in Flux.jlÂ . We'll walkthrough UNet implementation, which I've been [working](https://github.com/shivance/Metalhead.jl/blob/ba54cf0219cc247a732c1710925a4eef7a2c70c7/src/convnets/unet.jl) on lately to contribute to flux's model zoo **Metalhead.jl.**


**After reading this article you'll learn about**

1. What is UNet?
2. A brief overview of Flux API
3. Reimplementing PyTorch Models in Julia

<div style="text-align: center; margin-top: 25px; margin-bottom: 25px;">
  <a href="https://fullstackagents.substack.com" class="btn btn-primary" target="_blank" title="Subscribe">
    Subscribe
  </a>
</div>

## UNet ðŸ¥…

UNet is a deep learning model which was released in the paper [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597v1). The architecture of the network looks likeÂ :
Fig. 1 ( Source: UNetÂ Paper)We'll not go into detail about UNet theory, as the paper explains it in best way, and explaination here would be redundant anyway.

![Source: UNet Paper](assets/flux_is_flexible/unet_full.png)

This tutorial will focus on implementation.

Let's dive into code rightawayÂ ! The article is written presuming that you have knowledge of implementing a neural networks in PyTorchÂ ! We simply create a class that inherits nn.ModuleÂ . We now create layers and assign it to class with selfÂ . When this model is instantiated, these layers will become attribute of the model object.

<script src="https://gist.github.com/kanpuriyanawab/c66010357e03ddcd21862c633162efb9.js"></script>

```
>>> exec(open("basic_model.py").read())
torch.Size([1, 5, 8, 8])
```


Let's see how it looks like in Julia.

<script src="https://gist.github.com/kanpuriyanawab/e0e278c4fcdd26ef998a2393b9e4bb9d.js"></script>

```
julia> include("basic_model.jl");
(8, 8, 5, 1)
```

Some Key things to note here areÂ :

1. Flux follows (H, W, C, N) standard for images while PyTorch uses (N, C, H, W)
2. Chain function is similar to nn.Sequential method from PyTorch
3. Julia implements Multiple Dispatch unlike PyThon which is designed on Single Dispatch paradigm. In short multiple dispatch allows us to implement a single method for different combination of different type arguments, unlike python which restricts the methods to be bound to a single object and reimplemented for different classes. Read more about multiple dispatch hereÂ .

Let's see how [official UNet implementation](https://medium.com/r/?url=https%3A%2F%2Fgithub.com%2Fmateuszbuda%2Fbrain-segmentation-pytorch%2Fblob%2Fd45f8908ab2f0246ba204c702a6161c9eb25f902%2Funet.py%23L1) from Torchhub looks like

The author of original implementation (hereon referred just as "author") created a helper method to create a convolutional blockÂ . This is a good practise called as DRY ( Do not Repeat Yourself).

![Source: UNet Paper](assets/flux_is_flexible/unet.png)

This allows us to create these conv blocks (fig. 2 zooms into fig. 1 to show these conv blocks) just by passing input and output channels.
To implement it with Flux, we reuse our knowledge from gist 1Â :

<script src="https://gist.github.com/kanpuriyanawab/6b63cf078466fda41cc5912b36ded6b2.js"></script>

This allows us to create these conv blocks (fig. 2 zooms into fig. 1 to show these conv blocks) just by passing input and output channels.

Pretty simple isn't itÂ ?

One thing to note here is, Flux doesn't allow us to name the layers explicitly, the reason can be found in [this](https://github.com/FluxML/Flux.jl/issues/2142) github issue. Now let's go ahead and see how layers were created by author.

Now we see that we have an encoder, bottleneck, decoder and an upconv layer.
1. EncoderÂ : The four downstairs in Fig. 1 form the encoder block, and it encodes image by successive convolutions
2. BottleneckÂ : The layer between encoder and decoder is called bottleneck. The output of decoder is passed on to decoder & upconv block.
3. Decoder & Upconv blockÂ : The upconv block upsamples the input matrix, i.e. deconvolves the input to output with bigger size ( H x W ) than input. Decoder follows upconv layer and increases channels by performing convolution.



Let's create the model class in Flux first. Julia doesn't have classes, it has structsÂ . So model struct would look like this.

<script src="https://gist.github.com/kanpuriyanawab/8e5eb8ed5722f6a574ae90558b5ab42e.js"></script>

To reduce the redundancy in code I'll implement the layers as array of layers, it'll allow us to write a clean forward pass later.
Don't forget to notice the relation between number of channels of different blocks of UNet model.

The unet_block is the convolutional block that we defined earlier is a simple chain of Convolutional and BatchNorm layers. We further chain these blocks keeping in mind the input & output features using the Chain function. SeeÂ ! How easy it is to create a model in Flux.
Now we have one last thing remaining, the forward passÂ . We do it like 

<script src="https://gist.github.com/kanpuriyanawab/4771bb348755b488597b461ab9868aa5.js"></script>

in PyTorch. The cat operation orchestrates the connection between encoder and decoder demonstrated by copy and crop represented by gray arrow in Fig. 1.

But don't you find the code above messyÂ ? That's where our definition of layers of arrays comes in. Let's see how the forward pass is written for UNet.

<script src="https://gist.github.com/kanpuriyanawab/9e3c6b8ee5cf77c3e29ec62e71ea49fc.js"></script>

Pretty simple & clean, we managed to keep the entire logic same.

Some key points to be noted here:

1. Ïƒ is nothing else but sigmoid functionÂ ! Julia allows us to use all the mathematical symbols as variablesÂ . Thus, flux defined sigmoid as Ïƒ rather than sigmoid()
2. Julia uses matlab like syntax for ranges (see 1:4 for iterating over 1, 2, 3, 4 and 4:-1:1 for 4, 3, 2, 1).
3. Julia uses 1 based indexing.
4. Any Julia function with a trailingÂ ! tells that operation will be inplace ( remember pass by reference from C++ using & operation).
5. The `return` keyword in last line is redundant, simply writing Ïƒ(u.final_conv(out)) would work as Julia always returns the output of last line of code, from any code block.

That brings us to the end of this tutorial. Thanks for readingÂ !

This is the second article of my first blog series Julia For the Win. You can find the previous article Kaggle x JuliaÂ : Advanced House Price PredictionÂ : EDA.