[
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Resources",
    "section": "",
    "text": "This page enlists the talks I have given, the podcasts I have appeared on and co-hosted, and other misc things.\n\nTalks / podcasts / events\nInterviews\nA bit about my formal education"
  },
  {
    "objectID": "pages/talksseminarsworkshops.html",
    "href": "pages/talksseminarsworkshops.html",
    "title": "Talks/Seminars/Workshops",
    "section": "",
    "text": "I love to attend developer meetups, conferences, workshops and learn from them as much as I can. I sometimes talk on a range of topics that I love the most. All the slides of my talks/sessions can be found below.\n\ngiven by me\n\nKerasNLP: From Words to Wisdom, on October 7, 2023 at DevFest New Delhi‚Äô23 .\nModular NLP Workflows with KerasNLP, on September 29, 2023 at Google Developer Groups, Seattle.\nSupercharging Keras with WandB, on September 23, 2023 at TensorFlow User Group Mumbai.\nGSoC Success Secrets: Cracking the Code to Open Source Excellence, September 10, 2023 at National Institute of Technology, Warangal.\nRethinking LLM Design with KerasNLP, on August 26, 2023 at TensorFlow User Group Hyderabad.\nKerasNLP for Starters, on August 20, 2023 at TensorFlow User Group Durg\nTaking KerasNLP on GenAI Ride on July, 23, 2023 at TensorFlow User Group, Kolkata\nRe-imagining Keras in the evolving ML ecosystem on July 16, 2023 at Google I/O Extended‚Äô23 New Delhi.\n\n\n\nco-organized by me\n\nHosted Tanay Mehta, Kaggle Notebooks Grandmaster on Hey Kaggle, at NIT Warangal.\n\nNote: If you are interested to invite me as a speaker for your event, please get in touch by dropping an email at shivanshuman021@gmail.com. If you are interested in having me submit a CFP first, that is absolutely fine! Please don‚Äôt hesitate to ask that."
  },
  {
    "objectID": "pages/education.html",
    "href": "pages/education.html",
    "title": "Education",
    "section": "",
    "text": "(The formal ones may be)\n\nB.Tech in Electronics and Communication Engineering from National Institute of Technology, Warangal (2019 - 23) (Final year dissertation: A CFS‚ÄìDNN-Based Intrusion Detection System)\nIntermediate from Army Public School, Kanpur (PCM) (2017 - 19)\nHigh School from BNSD Shiksha Niketan, Kanpur (PCMC) (2006 - 17)\nCourses/certifications relevant to my subject of interest:\n\nAWS Machine Learning Foundations\nComplete Python Bootcamp : From Zero to Hero in Python\nDeep Learning Specialization (Coursera)\nFundamentals of Reinforcement Learning"
  },
  {
    "objectID": "pages/open_source.html",
    "href": "pages/open_source.html",
    "title": "OpenSource Contributions",
    "section": "",
    "text": "[Highlights]\n\nKerasHub - Add Qwen 2.5\nKerasHub - Add Qwen 1.5 Moe\nKerasHub - Add Mixtral\nKerasNLP - Add Llama Backbone\nKerasNLP - Port GPTNeoX to KerasCore\nKerasNLP - Adding GPTNeoXBackbone\nKerasNLP - Generic RotaryEmbedding Layer\nKeras - Add rsqrt to ops API\nKeras - Add rms_scaling in LayerNormalization\nFluxML - Adding UNet Model\nDeepchem - Porting CNN from TF ‚û°Ô∏è PyTorch\nDeepchem - Neural ODE tutorial üìö\n\n\n\n[All Contributions]\n\nkeras-nlp\n\nAdd Qwen 2.5\nAdd Qwen 1.5 Moe\nAdd Mixtral\nFix RotaryEmbedding import\nFix Autograph error with perplexity metric\nFix ModuleNotFoundError keras_nlp.models.xlnet\nPort BeamSampler to core\nAdd compute_output_shape to tokenizer\nRefactor RotaryEmbedding and GPTNeoXAttention\nAdd GPTNeoXPreprocessor\nAdd an XLMRobertaMaskedLM task model\nSpeed up default RoBERTa testing roughly 3x\nAdding XXBackboneTPUTests\nDefault compilation for Albert, Distilbert, Roberta MaskedLM\nCall super.config() in BartBackbone‚Äôs get_config()\nAdd API exports for tokenizers documented on keras.io\nAdd API exports for metrics documented on keras.io\nAdd API exports for samplers documented on keras.io\nAdd API exports for models documented on keras.io\nAdding an AlbertMaskedLM task + Fix Projection layer dimension in MaskedLMHead\nAdd an add_prefix_space Arg in BytePairTokenizer\nMove from_preset to base tokenizer classes\nAdd AlbertClassifier\nAdd Llama Backbone\nPort GPTNeoX to KerasCore\nAdding GPTNeoXBackbone\nGeneric RotaryEmbedding Layer\n\n\n\nkeras-core\n\nAdd rsqrt to ops API\nAdd rms_scaling in LayerNormalization\n\n\n\nkeras-io\n\nPort [KerasNLP] Transformer Pretraining guide to multi-backend Keras\nfixing getting started guide kerasnlp\nAdd example : Data Parallel Training with KerasNLP\nAdd Semantic Similarity with KerasNLP tutorial\n\n\n\nh2o-llmstudio\n\nadd: safe serialization while pushing to hf\n\n\n\nflux-ml\n\nAdding UNet Model\n\n\n\noptax\n\nadded typing to linear_algebra.py\n\n\n\ndeepchem\n\n:octocat: Porting CNN, TF ‚û°Ô∏è PyTorch\n:octocat: Fixed potential bug in deepchem‚Äôs CNN implementation\nadded neural ode tutorial üìö\nFixing load_qm7_from_mat() not found"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Illustrated LLM OS\n\n\nAn Implementational Perspective of LLMs as Operating Systems\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "Hi there üëã. I am Anshuman Mishra. I work on large language models yet I like to call myself a software engineer.\n\n\n\nMy external articles and other publishing engagements (like books, liveProjects, etc.) are listed here. Decks from my speaking engagements are listed here. A detailed account of the things that are not directly available from the top navbar (interviews, talks etc.) can be found here.\nThe structure of this website is inspired by Sayak‚Äôs site.\nTo know about my projects, please refer to my GitHub profile."
  },
  {
    "objectID": "posts/illustrated_llmos.html",
    "href": "posts/illustrated_llmos.html",
    "title": "Illustrated LLM OS",
    "section": "",
    "text": "This blog post explores the implementation of large language models (LLMs) as operating systems, inspired by Andrej Karpathy‚Äôs vision of AI resembling an OS, akin to Jarvis from Iron Man. The focus is on practical considerations, proposing an application-level integration for LLMs within a terminal session. A novel approach involves injecting state machines into the decoding process, enabling real-time code execution and interaction. Additionally, this post proposes Reinforcement Learning by System Feedback (RLSF),‚Äù a reinforcement learning technique applied to code generation tasks. This method leverages a reward model to evaluate code correctness through Python subprocess execution, enhancing LLM performance. The findings contribute insights into the dynamic control of LLMs and their potential applications beyond coding tasks.\nImage source: [1hr Talk] Intro to Large Language Models by Andrej Karpathy"
  },
  {
    "objectID": "posts/illustrated_llmos.html#preliminaries",
    "href": "posts/illustrated_llmos.html#preliminaries",
    "title": "Illustrated LLM OS",
    "section": "Preliminaries:",
    "text": "Preliminaries:\n\nState Machines: A state machine is a mathematical abstraction used to design algorithms. A state machine reads a set of inputs and changes to a different state based on those inputs. A state is a description of the status of a system waiting to execute a transition. A transition is a set of actions to execute when a condition is fulfilled or an event is received. In a state diagram, circles represent each possible state and arrows represent transitions between states.\nConstrained Decoding: Constrained decoding is a technique used in natural language processing and sequence generation tasks, including those involving large language models (LLMs). In constrained decoding, the generation of sequences is guided or restricted by certain constraints or conditions. This approach is particularly useful when you want to control or influence the output of a language model to meet specific requirements or criteria.\n\nA major challenge in the implementation of LLM OS is establishing a Link between the Operating System and LLM, that ensures training following the principle of Responsible AI."
  },
  {
    "objectID": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "href": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "title": "Illustrated LLM OS",
    "section": "Where should LLM sit? Concerns, Possibilities, and Limitations",
    "text": "Where should LLM sit? Concerns, Possibilities, and Limitations\nImagine a futuristic Jarvis-like AI. It‚Äôll be able to search through the internet, access local files, videos, and images on the disk, and execute programs. Where should it sit? At the kernel level? At Python Level?\nAt Kernel Level: Consider integrating our advanced language model with the Linux kernel. This would provide the AI with comprehensive access to the operating system‚Äôs core functionalities. However, it‚Äôs important to recognize that large language models (LLMs) are designed for human-like interaction, not intricate coding tasks. While embedding the model at the kernel level offers the advantage of understanding and controlling detailed system operations, it raises valid security concerns. Responsible development is crucial to ensure that the AI‚Äôs evolving decision-making capabilities don‚Äôt inadvertently compromise system integrity.\nAt Application Level: Alternatively, we can position the LLM at the application level, operating seamlessly within a terminal session. Leveraging the abstractions of a programming language, such as Python, provides optimal control over the AI. This approach facilitates easy updates to the LLM OS and allows for extensive customization."
  },
  {
    "objectID": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "href": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "title": "Illustrated LLM OS",
    "section": "Towards Implementation: Injecting a State Machine in Decoder",
    "text": "Towards Implementation: Injecting a State Machine in Decoder\nFollowing is the illustration of how the generation process works in decoder-like models source.\n\nImage source: The Illustrated GPT-2 by Jay Alammar\nThe seamless interaction of a Large Language Model (LLM) with an operating system (OS) can be achieved through the strategic injection of a State Machine. By utilizing this approach, the model‚Äôs behavior can be dynamically controlled, enhancing its capability to engage with the OS. In particular, the injection of special tokens acts as triggers, facilitating interaction with the Python interpreter through subprocesses.\nTo illustrate this concept, consider the scenario where the LLM is tasked with generating the sum of the first N natural numbers. Here, I request the model to encapsulate the generated code within designated special tokens, [CODE] and [/CODE], during its response. The State Machine is activated upon the generation of the [CODE] token, initiating the collection of the generated code in a buffer.\nUpon the subsequent generation of the [/CODE] token, the State Machine orchestrates a temporary pause in the generation process. This interlude allows for the invocation of a Python subprocess to execute the collected code. The output of this execution is then appended to the current cache. Resuming the generation process, this technique enables the model to dynamically learn program execution behavior in real time.\nIn the following sections of this blog, we delve deeper into how this innovative technique empowers the model to adapt and refine its program execution understanding on the fly.\n\n\n\nimage/png\n\n\nFollowing is an illustration of a state machine to do various operations like db queries, accessing file systems, and even internet searches through Python subprocesses.\n\n\n\nimage/png\n\n\nThe Python programming language boasts a robust ecosystem that opens up a multitude of possibilities, especially when harnessed through subprocesses. By utilizing subprocesses, we can seamlessly execute Shell or Programming Language codes, expanding the functionality of our applications. Python‚Äôs versatility extends to internet access, where we can leverage powerful libraries like requests and urllib. This capability allows the Large Language Model (LLM) to interact dynamically with online resources. Additionally, the integration of command line packages, such as the one offered by Google, further enhances the LLM‚Äôs capacity to fetch information from the web. Python‚Äôs sophisticated file-handling utilities provide the LLM with the capability to navigate and manipulate files and multimedia at the user‚Äôs request. This functionality extends beyond mere text-based interactions, offering a rich, multimodal experience. The inclusion of multimodal models further augments the LLM‚Äôs ability to interpret and respond to a diverse range of user inputs."
  },
  {
    "objectID": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "href": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "title": "Illustrated LLM OS",
    "section": "Tuning LLMs for OS use case: A RL Problem:",
    "text": "Tuning LLMs for OS use case: A RL Problem:\nImagine confronting a challenging programming problem and enlisting the assistance of the Large Language Model (LLM) to generate a solution. Traditionally, the process involves articulating the problem to the LLM, receiving the generated code, manual inspection for correctness, and iterative feedback for bug resolution.\nNow, let‚Äôs recast this procedure as a Reinforcement Learning (RL) challenge, casting our LLM as an agent navigating a realm of computer processes. In this RL framework, the agent‚Äôs objective is to generate code, subject to correctness scrutiny.\nTo enhance this code generation process, we leverage a fundamental concept in RL ‚Äî the reward model. This model quantifies the quality of generated code based on execution results. Through training the reward model, the LLM is systematically penalized for generating incorrect code, eliminating the need for manual intervention.\nI call this technique Reinforcement Learning by System Feedback (RLSF). This technique can not only be applied to improving LLMs on code generation tasks but to a variety of tasks as shown in the state machine diagram."
  },
  {
    "objectID": "posts/illustrated_llmos.html#concluding-notes",
    "href": "posts/illustrated_llmos.html#concluding-notes",
    "title": "Illustrated LLM OS",
    "section": "Concluding Notes",
    "text": "Concluding Notes\nThe reason I published this blog post is because I think the discussion of integrating LLMs with Operating Systems should be Open. It should be developed responsibly in an Open Source Environment. If there is an initiative like this in the future, I would love to collaborate on it! At this stage, I couldn‚Äôt get a working prototype of the approaches I discussed due to a lack of computing resources.\nJust Imagine an operating system built on HuggingFace transformer‚Äôs LLM pipelines. This setup not only accommodates the effortless integration of new models but also ensures the system remains adaptable to the latest advancements in the field. LLMs can add a new layer of accessibility to our Operating System by acting as smart interfaces."
  },
  {
    "objectID": "posts/illustrated_llmos.html#acknowledgements",
    "href": "posts/illustrated_llmos.html#acknowledgements",
    "title": "Illustrated LLM OS",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nRavindra Majumdar and Sunil Mallya for introducing me to the concept of constrained decoding."
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About Me",
    "section": "",
    "text": "I work on large language models at FlipAI. For more details on my work experience, check out my LinkedIn profile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeline\n\n\n\n\n\n\n\n\nPosition\nCompany\nDuration\n\n\n\n\nML Engineer\nFlipAI\nJuly 2023 - Present\n\n\nTechnical Author\nWeights&Biases\nSep 2023 - Feb 2024\n\n\nData Science Intern\nBNY Mellon\nJan 2023 - June 2023\n\n\nSoftware Engineer Intern\nAmazon\nMay 2022 - July 2023\n\n\n\n\n\nSome stuff\n\nGoogle Developer Expert in Machine Learning and Google Cloud Platform (bit.ly/expert-sp)\nMentee, Google Summer of Code 2023, in TensorFlow. See my project here\nOpen Source Contributor at KerasNLP"
  },
  {
    "objectID": "pages/authoring.html",
    "href": "pages/authoring.html",
    "title": "Authoring",
    "section": "",
    "text": "Below are the blogs, articles, and tutorials I have written on Data Science, Machine Learning, and more. I am fortunate enough to collaborate with amazing folks from all around the globe.\n\nWeights&Biases\n\nSupercharge KerasNLP Models with Wandb\nGenerating High-quality Images with SD.Next, HuggingFace Diffusers and W&B\n\n\n\nKeras\n\nData Parallel Training with KerasNLP\nSemantic Similarity with KerasNLP\nPretraining a Transformer from scratch with KerasNLP\n\n\n\nKaggle\n\nSupercharge KerasNLP Models with Wandb\nOne Stop EDA\nKaggle T4x2 multi-gpu training G2Net\nLB:0.23 Ensemble {CatBoost, XGBoost, LightGBM}\nLightGBM + Optuna Baseline\n\n\n\nMedium\n\nInterview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD: Ravi Ramakrishnan\nFlux.jl is so flexible ‚Äî Deep Learning in Julia\nKaggle x Julia : Advanced House Price Prediction : EDA"
  },
  {
    "objectID": "pages/interviews.html",
    "href": "pages/interviews.html",
    "title": "Interviews",
    "section": "",
    "text": "The purpose of conducting these interviews is to mainly get insights about the real-world project experiences, perspectives on learning new things, some fun facts and thereby enriching the communities in the process. I sincerely thank the interviewees for taking the time out from their busy schedules and for agreeing to do these interviews. Here are the interviews I have done so far -\n\nAn Interview with Ravi Ramakrishnan, Kaggle Grandmaster (joint work with Abheesht Sharma)\nAn Interview with Rajaswa Patil, AI Research Associate at Postman Labs (joint work with Abheesht Sharma)\nAn Interview with Matthew Watson, Senior Software Engineer, Google (joint work with Abheesht Sharma)\nAn Interview with Aakash Kumar Nain, MLE at Merlyn Minds (joint work with Abheesht Sharma)\n\nI have had an amazing time interviewing these incredible folks and I am grateful to them. For other competing priorities, I am discontinuing the series for an indefinite period. If you enjoy reading through interviews like these, you might want to check out the Machine Learning Street Talk podcast on YouTube."
  }
]