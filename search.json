[
  {
    "objectID": "pages/work.html",
    "href": "pages/work.html",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "TLDR\n\n\nAdd Qwen 2.5 to KerasHub ğŸš€ Contribution to KerasHub Led the integration of the powerful Qwen 2.5 language model. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making a state-of-the-art model accessible to the Keras community.\n\n\n\nKerasHub - Add Qwen 1.5 Moe\n\n\n\nKerasHub - Add Mixtral"
  },
  {
    "objectID": "pages/work.html#section",
    "href": "pages/work.html#section",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "TLDR\n\n\nAdd Qwen 2.5 to KerasHub ğŸš€ Contribution to KerasHub Led the integration of the powerful Qwen 2.5 language model. This involved implementing the core backbone, attention mechanisms, and checkpoint conversion, making a state-of-the-art model accessible to the Keras community.\n\n\n\nKerasHub - Add Qwen 1.5 Moe\n\n\n\nKerasHub - Add Mixtral"
  },
  {
    "objectID": "pages/work.html#section-1",
    "href": "pages/work.html#section-1",
    "title": "Anshuman Mishra",
    "section": "2024",
    "text": "2024\n\nWeights & Biases\nSupercharge KerasNLP Models with Wandb\n\n\nWeights & Biases\nGenerating High-quality Images with SD.Next, HuggingFace Diffusers and W&B"
  },
  {
    "objectID": "pages/work.html#section-2",
    "href": "pages/work.html#section-2",
    "title": "Anshuman Mishra",
    "section": "2023",
    "text": "2023\ntl;dr - how many talks, open source contributions\n\nLlama\nKerasNLP - Add Llama Backbone\n\n\nGPT Neo X\nâ€“ Tell a story\nKerasNLP - Generic RotaryEmbedding Layer KerasNLP - Port GPTNeoX to KerasCore KerasNLP - Adding GPTNeoXBackbone Add GPTNeoXPreprocessor Refactor RotaryEmbedding and GPTNeoXAttention\n\n\nBeam Sampler\nPort BeamSampler to core\n\n\nKeras Core layer contributions\nKeras - Add rsqrt to ops API Keras - Add rms_scaling in LayerNormalization\n\n\nAlbert Classifier\nAdd AlbertClassifier Adding an AlbertMaskedLM task + Fix Projection layer dimension in MaskedLMHead\n\n\nMisc Bug Fixes\nFix RotaryEmbedding import Fix Autograph error with perplexity metric Fix ModuleNotFoundError keras_nlp.models.xlnet Add compute_output_shape to tokenizer Default compilation for Albert, Distilbert, Roberta MaskedLM Call super.config() in BartBackboneâ€™s get_config() Add API exports for tokenizers documented on keras.io Add API exports for metrics documented on keras.io Add API exports for samplers documented on keras.io Add API exports for models documented on keras.io Move from_preset to base tokenizer classes\nAdd an add_prefix_space Arg in BytePairTokenizer\n\n\nTutorials\nPort [KerasNLP] Transformer Pretraining guide to multi-backend Keras\n\n\nTutorials\nfixing getting started guide kerasnlp\n\n\nTutorials\nAdd example : Data Parallel Training with KerasNLP\n\n\nTutorial\nAdd Semantic Similarity with KerasNLP tutorial\n\n\nTutorial\nhttps://github.com/keras-team/keras-io/blob/master/guides/keras_hub/transformer_pretraining.py\n\n\nTalk\nKerasNLP: From Words to Wisdom, on October 7, 2023 at DevFest New Delhiâ€™23 .\n\n\nKaggle\nOne Stop EDA\n\n\nTesting improvements\nSpeed up default RoBERTa testing roughly 3x Adding XXBackboneTPUTests\n\n\nXLMRoberta\nAdd an XLMRobertaMaskedLM task model\n\n\ntalk\nModular NLP Workflows with KerasNLP, on September 29, 2023 at Google Developer Groups, Seattle.\n\n\ntalk\nSupercharging Keras with WandB, on September 23, 2023 at TensorFlow User Group Mumbai.\n\n\ntalk\nGSoC Success Secrets: Cracking the Code to Open Source Excellence, September 10, 2023 at National Institute of Technology, Warangal.\n\n\ntalk\nRethinking LLM Design with KerasNLP, on August 26, 2023 at TensorFlow User Group Hyderabad.\n\n\ntalk\nKerasNLP for Starters, on August 20, 2023 at TensorFlow User Group Durg\n\n\ntalk\nTaking KerasNLP on GenAI Ride on July, 23, 2023 at TensorFlow User Group, Kolkata\n\n\ntalk\nRe-imagining Keras in the evolving ML ecosystem on July 16, 2023 at Google I/O Extendedâ€™23 New Delhi.\n\n\nJulia Contributions\nFluxML - Adding UNet Model"
  },
  {
    "objectID": "pages/work.html#section-3",
    "href": "pages/work.html#section-3",
    "title": "Anshuman Mishra",
    "section": "2022",
    "text": "2022\n\nCNN From Tensorflow to Pytorch\nDeepchem - Porting CNN from TF â¡ï¸ PyTorch\n\n\nNeuralODE\nDeepchem - Neural ODE tutorial ğŸ“š\n\n\nh2o-llmstudio\nadd: safe serialization while pushing to hf\n\n\nDeepmind - optax\n\nadded typing to linear_algebra.py\n\n\n\nMisc Bug fixes\n:octocat: Fixed potential bug in deepchemâ€™s CNN implementation\nFixing load_qm7_from_mat() not found\n\n\nKaggle\nKaggle T4x2 multi-gpu training G2Net\n\n\nKaggle\nLB:0.23 Ensemble {CatBoost, XGBoost, LightGBM}\n\n\nKaggle\nLightGBM + Optuna Baseline"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html",
    "href": "posts/julia_house_price_prediction.html",
    "title": "Comprehensive data exploration with Julia",
    "section": "",
    "text": "SourceÂ : Analytics IndiaÂ MagazineIn this tutorial, weâ€™ll learn using Julia for exploratory data analysis. This is my first blog post ever and first article of my series Julia For The Win. Please feel free to give feedbackÂ ! Letâ€™s get started. In this tutorial weâ€™ll be reproducing Comprehensive data exploration with Python notebook, but in Julia.\nDataset used in this tutorial can be found here.\nWeâ€™ll be using following packages:\nAbove packages can be installed by using the following block of code (we take PlotlyJS for example)\nThis quote belongs to Thales of Miletus. Thales was a Greek/Phonecian philosopher, mathematician and astronomer, which is recognised as the first individual in Western civilisation known to have entertained and engaged in scientific thought (source: https://en.wikipedia.org/wiki/Thales)\nI wouldnâ€™t say that knowing your data is the most difficult thing in data science, but it is time-consuming. Therefore, itâ€™s easy to overlook this initial step and jump too soon into the water.\nSo I tried to learn how to swim before jumping into the water. Based on Hair et al. (2013), chapter â€˜Examining your dataâ€™, I did my best to follow a comprehensive, but not exhaustive, analysis of the data. Iâ€™m far from reporting a rigorous study in this kernel, but I hope that it can be useful for the community, so Iâ€™m sharing how I applied some of those data analysis principles to this problem.\nDespite the strange names I gave to the chapters, what we are doing in this kernel is something like:\nNow, itâ€™s time to have fun!\nDataFrames.jl is the stable alternative of Pandas in Julia. Its design and functionality are similar to those of pandas (in Python) and data.frame, data.table and dplyr (in R), making it a great general purpose data science tool.\nTo peek into the dataframe, Julia alternative of head() method is first"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#so-what-can-we-expect",
    "href": "posts/julia_house_price_prediction.html#so-what-can-we-expect",
    "title": "Comprehensive data exploration with Julia",
    "section": "Soâ€¦ What can weÂ expect?",
    "text": "Soâ€¦ What can weÂ expect?\nIn order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\nIn order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\nVariable - Variable name.\nTypeâ€Š-â€ŠIdentification of the variablesâ€™ type. There are two possible values for this field: â€˜numericalâ€™ or â€˜categoricalâ€™. By â€˜numericalâ€™ we mean variables for which the values are numbers, and by â€˜categoricalâ€™ we mean variables for which the values are categories.\nSegmentâ€Š-â€ŠIdentification of the variablesâ€™ segment. We can define three possible segments building, space or location. When we say â€˜buildingâ€™, we mean a variable that relates to the physical characteristics of the building (e.g.Â â€˜OverallQualâ€™). When we say â€˜spaceâ€™, we mean a variable that reports space properties of the house (e.g.Â â€˜TotalBsmtSFâ€™). Finally, when we say a â€˜locationâ€™, we mean a variable that gives information about the place where the house is located (e.g.Â â€˜Neighborhoodâ€™).\nExpectationâ€Š-â€ŠOur expectation about the variable influence in â€˜SalePriceâ€™. We can use a categorical scale with â€˜Highâ€™, â€˜Mediumâ€™ and â€˜Lowâ€™ as possible values.\nConclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in â€˜Expectationâ€™.\nCommentsâ€Š-â€ŠAny general comments that occured to us.\n\nWhile â€˜Typeâ€™ and â€˜Segmentâ€™ is just for possible future reference, the column â€˜Expectationâ€™ is important because it will help us develop a â€˜sixth senseâ€™. To fill this column, we should read the description of all the variables and, one by one, ask ourselves:\n\nDo we think about this variable when we are buying a house? (e.g.Â When we think about the house of our dreams, do we care about its â€˜Masonry veneer typeâ€™?).\nIf so, how important would this variable be? (e.g.Â What is the impact of having â€˜Excellentâ€™ material on the exterior instead of â€˜Poorâ€™? And of having â€˜Excellentâ€™ instead of â€˜Goodâ€™?).\nIs this information already described in any other variable? (e.g.Â If â€˜LandContourâ€™ gives the flatness of the property, do we really need to know the â€˜LandSlopeâ€™?).\n\nAfter this daunting exercise, we can filter the spreadsheet and look carefully to the variables with â€˜Highâ€™ â€˜Expectationâ€™. Then, we can rush into some scatter plots between those variables and â€˜SalePriceâ€™, filling in the â€˜Conclusionâ€™ column which is just the correction of our expectations.\nI went through this process and concluded that the following variables can play an important role in this problem:\n\nOverallQual (which is a variable that I donâ€™t like because I donâ€™t know how it was computed; a funny exercise would be to predict â€˜OverallQualâ€™ using all the other variables available).\nYearBuilt.\nTotalBsmtSF.\nGrLivArea.\n\nI ended up with two â€˜buildingâ€™ variables (â€˜OverallQualâ€™ and â€˜YearBuiltâ€™) and two â€˜spaceâ€™ variables (â€˜TotalBsmtSFâ€™ and â€˜GrLivAreaâ€™). This might be a little bit unexpected as it goes against the real estate mantra that all that matters is â€˜location, location and locationâ€™. It is possible that this quick data examination process was a bit harsh for categorical variables. For example, I expected the â€˜Neigborhoodâ€™ variable to be more relevant, but after the data examination I ended up excluding it. Maybe this is related to the use of scatter plots instead of boxplots, which are more suitable for categorical variables visualization. The way we visualize data often influences our conclusions.\nHowever, the main point of this exercise was to think a little about our data and expectactions, so I think we achieved our goal. Now itâ€™s time for â€˜a little less conversation, a little more action pleaseâ€™. Letâ€™s shake it!\n\n Subscribe"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#analysing-saleprice",
    "href": "posts/julia_house_price_prediction.html#analysing-saleprice",
    "title": "Comprehensive data exploration with Julia",
    "section": "Analysing SalePrice",
    "text": "Analysing SalePrice\nâ€˜SalePriceâ€™ is the reason of our quest. Itâ€™s like when weâ€™re going to a party. We always have a reason to be there. Usually, women are that reason. (disclaimer: adapt it to men, dancing or alcohol, according to your preferences)\nUsing the women analogy, letâ€™s build a little story, the story of â€˜How we met â€™SalePriceâ€™.\nEverything started in our Kaggle party, when we were looking for a dance partner. After a while searching in the dance floor, we saw a girl, near the bar, using dance shoes. Thatâ€™s a sign that sheâ€™s there to dance. We spend much time doing predictive modelling and participating in analytics competitions, so talking with girls is not one of our super powers. Even so, we gave it a try:\nâ€˜Hi, Iâ€™m Kaggly! And you? â€™SalePriceâ€™? What a beautiful name! You know â€˜SalePriceâ€™, could you give me some data about you? I just developed a model to calculate the probability of a successful relationship between two people. Iâ€™d like to apply it to us!â€™\ndescribe(df_train, cols=:SalePrice)\n\nVery wellâ€¦ It seems that your minimum price is larger than zero. Excellent! You donâ€™t have one of those personal traits that would destroy my model! Do you have any picture that you can send me? I donâ€™t knowâ€¦ like, you in the beachâ€¦ or maybe a selfie in the gym?\nplot(df_train, x=:SalePrice, kind=\"histogram\")\n\nAh! I see you that you use seaborn makeup when youâ€™re going outâ€¦ Thatâ€™s so elegant! I also see that you:\n\nDeviate from the normal distribution\nHave appreciable positive skewness\nShow peakedness\n\nThis is getting interesting! â€˜SalePriceâ€™, could you give me your body measures?\nusing StatsBase\nskw, kurt = skewness(collect(df_train.SalePrice)), kurtosis(collect(df_train.SalePrice))\nprintln(\"Skewness: $skw \\nKurtosis: $kurt\")\nThis prints:\nSkewness: 1.8809407460340335 \nKurtosis: 6.509812011089398\nAmazing! If my love calculator is correct, our success probability is 97.834657%. I think we should meet again! Please, keep my number and give me a call if youâ€™re free next Friday. See you in a while, crocodile!\nâ€˜SalePriceâ€™, her buddies and her interests\nIt is military wisdom to choose the terrain where you will fight. As soon as â€˜SalePriceâ€™ walked away, we went to Facebook. Yes, now this is getting serious. Notice that this is not stalking. Itâ€™s just an intense research of an individual, if you know what I mean.\nAccording to her profile, we have some common friends. Besides Chuck Norris, we both know â€˜GrLivAreaâ€™ and â€˜TotalBsmtSFâ€™. Moreover, we also have common interests such as â€˜OverallQualâ€™ and â€˜YearBuiltâ€™. This looks promising!\nTo take the most out of our research, we will start by looking carefully at the profiles of our common friends and later we will focus on our common interests. Relationship with numerical variables\nplot(scatter(df_train, x=:GrLivArea, y=:SalePrice, mode=\"markers\"))\n\nHmmmâ€¦ It seems that â€˜SalePriceâ€™ and â€˜GrLivAreaâ€™ are really old friends, with a linear relationship.\nAnd what about â€˜TotalBsmtSFâ€™?\nplot(scatter(df_train, x=:TotalBsmtSF, y=:SalePrice, mode=\"markers\"))\n\nâ€˜TotalBsmtSFâ€™ is also a great friend of â€˜SalePriceâ€™ but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. Moreover, itâ€™s clear that sometimes â€˜TotalBsmtSFâ€™ closes in itself and gives zero credit to â€˜SalePriceâ€™."
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#relationship-with-categorical-features",
    "href": "posts/julia_house_price_prediction.html#relationship-with-categorical-features",
    "title": "Comprehensive data exploration with Julia",
    "section": "Relationship with categorical features",
    "text": "Relationship with categorical features\nplot(df_train, x=:OverallQual, y=:SalePrice, color=:OverallQual, kind=\"box\")\n\nLike all the pretty girls, â€˜SalePriceâ€™ enjoys â€˜OverallQualâ€™. Note to self: consider whether McDonaldâ€™s is suitable for the first date.\nplot(df_train, x=:YearBuilt, y=:SalePrice, color=:OverallQual, kind=\"box\")\n\nAlthough itâ€™s not a strong tendency, Iâ€™d say that â€˜SalePriceâ€™ is more prone to spend more money in new stuff than in old relics.\nNote: we donâ€™t know if â€˜SalePriceâ€™ is in constant prices. Constant prices try to remove the effect of inflation. If â€˜SalePriceâ€™ is not in constant prices, it should be, so than prices are comparable over the years.\n\n Subscribe"
  },
  {
    "objectID": "posts/julia_house_price_prediction.html#in-summary",
    "href": "posts/julia_house_price_prediction.html#in-summary",
    "title": "Comprehensive data exploration with Julia",
    "section": "In summary",
    "text": "In summary\nStories aside, we can conclude that: - â€˜GrLivAreaâ€™ and â€˜TotalBsmtSFâ€™ seem to be linearly related with â€˜SalePriceâ€™. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of â€˜TotalBsmtSFâ€™, we can see that the slope of the linear relationship is particularly high. - â€˜OverallQualâ€™ and â€˜YearBuiltâ€™ also seem to be related with â€˜SalePriceâ€™. The relationship seems to be stronger in the case of â€˜OverallQualâ€™, where the box plot shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\nThat said, letâ€™s separate the wheat from the chaff.\nAs an engineer, I donâ€™t feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. Thereâ€™s a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.\nSo, letâ€™s overcome inertia and do a more objective analysis.\nThe â€˜plasma soupâ€™\nâ€˜In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.â€™ (source: http://umich.edu/~gs265/bigbang.htm)\nTo explore the universe, we will start with some practical recipes to make sense of our â€˜plasma soupâ€™:\n\nCorrelation matrix (heatmap style).\nâ€˜SalePriceâ€™ correlation matrix (zoomed heatmap style).\nScatter plots between the most correlated variables (move like Jagger style).\n\nFor heatmaps, I found JuliaPlots to be more userfriendly than PlotlyJS.jl\nimport Plots.heatmap as ht \n# alias because Plotly namescope conflicts with Plots.jl\n\nusing Statistics: cor \n# correlation matrix\n\navoid = names(df_train, String)\n\n# avoiding columns with string values : raises error\ndf_non_str = select(df_train, Not(avoid));\nco = cor(Matrix(df_non_str));\n\nnot_avoid = names(df_train, Not(avoid));\n\nht(co, xticks=(1:35, not_avoid), yticks=(1:35, not_avoid), \n    aspect_ratio=:equal, fill_z=co, xrotation=90, xtickfontsize=5, \n    ytickfontsize=5)\n\nAccording to our crystal ball, these are the variables most correlated with â€˜SalePriceâ€™. My thoughts on this:\n\nâ€˜OverallQualâ€™, â€˜GrLivAreaâ€™ and â€˜TotalBsmtSFâ€™ are strongly correlated with â€˜SalePriceâ€™. Check!\nâ€˜GarageCarsâ€™ and â€˜GarageAreaâ€™ are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. â€˜GarageCarsâ€™ and â€˜GarageAreaâ€™ are like twin brothers. Youâ€™ll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep â€˜GarageCarsâ€™ since its correlation with â€˜SalePriceâ€™ is higher).\nâ€˜TotalBsmtSFâ€™ and â€˜1stFloorâ€™ also seem to be twin brothers. We can keep â€˜TotalBsmtSFâ€™ just to say that our first guess was right (re-read â€˜Soâ€¦ What can we expect?â€™).\nâ€˜FullBathâ€™?? Really?\nâ€˜TotRmsAbvGrdâ€™ and â€˜GrLivAreaâ€™, twin brothers again. Is this dataset from Chernobyl?\nAhâ€¦ â€˜YearBuiltâ€™â€¦ It seems that â€˜YearBuiltâ€™ is slightly correlated with â€˜SalePriceâ€™. Honestly, it scares me to think about â€˜YearBuiltâ€™ because I start feeling that we should do a little bit of time-series analysis to get this right. Iâ€™ll leave this as a homework for you.\n\nLetâ€™s proceed to the pair plots.\nPair plots between â€˜SalePriceâ€™ and correlated variables (move like Jagger style)\ncorner(df_train[:, cols])\n\nAlthough we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\nOne of the figures we may find interesting is the one between â€˜TotalBsmtSFâ€™ and â€˜GrLiveAreaâ€™. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless youâ€™re trying to buy a bunker).\nThe plot concerning â€˜SalePriceâ€™ and â€˜YearBuiltâ€™ can also make us think. In the bottom of the â€˜dots cloudâ€™, we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the â€˜dots cloudâ€™ (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\nPhewwÂ ! Thatâ€™s it for today. Today we took a look at how cool Julia is and why can it rescue data scientists just in case Python disappears from this earthÂ !\n\n\n Subscribe"
  },
  {
    "objectID": "posts/interview_matt.html",
    "href": "posts/interview_matt.html",
    "title": "Interview with Matthew Watson, Keras Team, Google",
    "section": "",
    "text": "by Anshuman Mishra & Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard-of) stories out into the world â€” their struggles, their triumphs, and their journey. We will be conversing with people we have looked up to and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nIn this installment of our series, we sit down with Matthew Watson for a chat. Matt, a Stanford CS alumnus, is a software engineer with the Keras team at Google, where he works on KerasNLP, Kerasâ€™ native NLP offering. He has also been instrumental in developing Keras 3 (its announcement created significant buzz on Twitter!), a multi-backend Keras that can run on TensorFlow, JAX, and PyTorch. In the past decade or so, Matt has already had a wonderfully diverse career â€” he started off with literature in his undergrad, before developing a love for CS (more on this in the interview!).\nAll words seem inadequate when one sets out to describe Matt. Matt is one of those â€œ10xâ€ engineers/researchers and always has a solution to every problem. We have always found his intuition to be bang on target. Matt has played a pivotal role in both of our careers; he has mentored us during (and after) Google Summer of Code. But one needs to look beyond just his technical expertise. Matt has infinite patience; weâ€™ve spent late-night debugging sessions with him, and he has always been super-helpful and giving. Matt is an â€œopen source stanâ€, and you can see his love for open source in every response of his on GitHub, and in every meticulously drafted PR (Pull Request). Safe to say, Matt has influenced us like no one else.\nLetâ€™s begin!\nAnshuman & Abheesht: Welcome, Matt! Thanks for doing this.\nMatt: Hi Anshuman and Abheesht! And thanks for putting this whole series together. I should probably quickly note that opinions here are my own; Iâ€™m not speaking on behalf of Google or any organization.\nAnshuman: Can you tell us about your journey in the field of natural language processing (NLP) and how you became involved with the Keras team at Google?\nMatt: Honestly I have taken a meandering path, but two consistent threads I can pick out of the last decade of my career are a love of language and of open-source software.\nI started my undergraduate studies with a focus on literature before getting lured to Computer Science by some really fantastic teachers. Through that switch, I have remained fascinated with the intersection of language and math. My first hands-on experience with NLP was helping with a research project at my university extracting information from Wikipedia via support vector machines written in Java (times have changed!). I started using Keras in 2017 when everyone was discovering text generation with LSTMs.\nIâ€™ve had the chance to spend most of my career so far working in open source (first in the Linux community and now ML) and have always been so grateful for it. So joining Keras honestly felt like a no-brainer; it was a chance to do open work on my interests with a library I already loved.\nAbheesht: Iâ€™d like to talk a bit more about the career pivots youâ€™ve done. In school/college, you say that you were interested in literature, post which you pivoted to Computer Science. Even in the field of Computer Science, you started off with Computer Graphics, started working on building an operating system at Endless Computers, and worked in the Firebase team, before joining Keras. How much did learning/working in different â€œdisciplines/fieldsâ€ help you? Were there any opportunities for â€œcross-disciplineâ€ learning, say, something you learned from discipline A which helped you in discipline B?\nMatt: Iâ€™m really glad I got a start in the humanities because writing is so important for almost every field. Being able to articulate a thought clearly and concisely is an important skill we underplay too often in the hard sciences.\nComputer graphics and the ML work Iâ€™m doing today feel closely intertwined. My advisor for my Masterâ€™s degree, Pat Hanrahan, was a driving force behind programmable shading language, and that blows my mind to this day. The early days of GPUs were all about these fixed-function pipelines for creating and rasterizing triangles. Then came the idea to write these domain-specific shading languages that added a ton of expressivity to the process. And from that came the idea of general-purpose GPU computations, and things like CUDA, and here we are today! ML as a field has probably been moved forward literal decades thanks to ideas from the graphics world.\nIâ€™m grateful for my time in the Linux open-source world for quite different reasons. I canâ€™t say thereâ€™s much overlap in subject matter, but it was a wonderful start in the world of open source. It definitely made me a bit of a tinkerer, with everything from my OS to shell to IDE. Linux is built around supporting that. But itâ€™s something I think equally applies to the world of ML. The deeper you get into these big deep-learning frameworks the more clear it becomes they all have bugs. You gain so much as an ML developer when you can get into the weeds of a library and start changing things.\nAnyway, thatâ€™s a long-winded way of saying Iâ€™m quite glad my career hasnâ€™t been a straight line.\nAnshuman: Given your interest in English literature, which novels are you currently reading? Whatâ€™s on your to-read list? Do you have any writing aspirations?\nMatt: Hah, for any attempt to read literature with a capital â€œLâ€ in school, my reading diet since I was 10 has been mostly sci-fi and fantasy. Ursula K. Le Guin is a big hero of mine. I recently read Klara and the Sun, a dystopian novel narrated by an â€œautonomous friendâ€ that certainly feels relevant to all the work we do.\nI would love to try my hand at fiction someday, but most of my writing is technical these days.\nAbheesht: What are the focal attributes of KerasNLP that make it different from other NLP libraries out there?\nMatt: One thing that makes Keras unique is a detailed and strict style guide for any new API. Our top-level APIs should be accessible to a very broad audience. We achieve flexibility underneath this top level with a focus on modular components and layered abstractions.\nThis gives KerasNLP a somewhat unique positioning among â€œpre-trained modelingâ€ libraries. When we port a model from a research codebase, we tend to heavily rewrite the forward pass so it matches our conventions and uses common building blocks.\nThis has some neat side effects. For one, itâ€™s a great library to learn fromâ€“you can define a transformer from scratch with our base layers in about 20 lines of code. It also gives a very uniform experience of slicing and dicing our models. Thatâ€™s a big one given how many real-world workflows, from PEFT strategies to interpretability techniques, require reaching into model internals.\nAbheesht: How do you see the current landscape of NLP and deep learning evolving, and how does KerasNLP fit into this evolving landscape?\nMatt: Itâ€™s dizzying how fast the field is changing these days. There is a ton of energy in open-source NLP development. There are so many valid concerns with how all this new technology is being used. At a broad level, I keep trying to remind myself what a massive period of flux this is for the field. We should all hold our opinions lightly and stay curious.\nI think multi-backend Keras 3 is really a game changer for how KerasNLP fits it. On the KerasNLP side, we can take a pre-trained model, write our numerics once, and distribute our weights once. You can fine-tune a model in any of TensorFlow, Jax, or Torch, and use Kerasâ€™ save format as a way to seamlessly move between frameworks. I think this is a new tool we are putting in ML developerâ€™s toolboxes, and I am excited to see what people do with it.\nAnshuman: Could you share some real-world applications or projects where KerasNLP has been particularly instrumental or innovative?\nMatt: One of the first projects to use KerasNLP is the AutoKeras library, and I really love how it fits into the Keras broader Keras ecosystem. Itâ€™s a super accessible, code-first library. You can just define your data and problem and it will handle choosing an architecture, optimizer, and hyperparameters. Or you can define your own much more complex and tailored workflow as you get deeper in.\nI think there is a huge opportunity to take a similar approach to generative problems going forward. Meet people with accessible, understandable, code-first flows that still give you full control over an underlying language model when you need it. I havenâ€™t quite seen an â€œauto MLâ€ offering on the generative modeling front that has really blown me away, but I would bet itâ€™s just a matter of time.\nAbheesht: Natural language understanding and generation have seen remarkable advancements. What do you think are the key challenges and opportunities in NLP today?\nMatt: Thereâ€™s a huge amount of opportunity here. We are far from understanding all the ways we can put the emergent properties of LLMs to use. Given the amount of investment and attention, I am not particularly worried about the field suddenly losing steam.\nAn obvious challenge is the reliability of language models. If you had a service where a fellow human would often give you helpful advice, and occasionally confidently lie to your face, people would be out there flipping tables.\nI think there are two things we could do here. First, keep working on the engineering problem of reliable models that can, at least, indicate uncertainty when present. Second, donâ€™t oversell these things! There are a lot of unsolved problems with LLMs, and there is a persistent, magnetic attraction towards anthropomorphizing something that appears to understand language. Everyone working in this space needs to be careful with how we describe these systems, and continually flag the obvious limitations of these tools.\nAbheesht: One thing Iâ€™ve seen with the Keras team is the collaborative culture that has been fostered. All members of the Keras team are very helpful to the outside community. Evidence of this is seen in the number of students who contribute to the Keras ecosystem. Can you provide insights into the collaborative development process within the Keras team at Google and outside contributors and how it contributes to the libraryâ€™s success?\nMatt: A huge reason I was attracted to the Keras team was that everyone on it really cares about open-source tools, not as an afterthought or a way to boost your profile, but as an end unto itself.\nThe vast majority of my work is out in the open on GitHub, and I think that is true of the whole Keras ecosystem. Most real discussion about new APIs is happening on issues and pull requests.\nI think everyone on the team really believes in the virtuous cycle here. There is far more knowledge and skill in the large pool of Keras users than just the Keras team at Google.\nAnshuman: In the ever-evolving field of AI, staying updated is crucial. How do you personally keep up with the latest developments and trends in NLP and deep learning?\nMatt: Itâ€™s a mix right now of Twitter, word of mouth, and anything that naturally comes up in my work. There is just no way for anyone to ingest all the research in this space. I constantly try to curate a short list of concepts to understand deeply, rather than attempting to ingest the whole fire hose.\nIf I encounter a new claim that breaks an assumption I was holding, or a bug in my code I canâ€™t effectively explain, itâ€™s almost always worth digging in. ML as a field I think really rewards going deep on unknowns and constantly leveling up your understanding.\nAnshuman: Youâ€™ve done your Bachelorâ€™s and Masterâ€™s from Stanford, widely regarded as one of the best universities in the world. How was your time at Stanford? How do you think your education has shaped your career as an engineer over the past decade? What makes Stanford Stanford; what does Stanford do differently from other universities that makes it the best? Is it the faculty? Is it the peer group?\nMatt: The main thing I think Stanford provided was this massive playground to go out and build your own understanding. They give a lot of resources and freedom to students, and thereâ€™s a lot of bright people trying weird stuff you can talk to. Thereâ€™s definite energy in that environment.\nStanford also killed it with the introductory Computer Science courses. The lecturers and teaching assistants were really supportive and focused on sparking curiosity about what computers can do. That is for sure what lured me out of the social sciences.\nI wouldnâ€™t put too much stock into any one place though. The best coworkers in my working life have truly come from all sorts of educational backgrounds; I cannot pick out a pattern. People who teach themselves and even get a bit irritated by things they donâ€™t yet understand tend to make great engineers. And open source in particular is one of the most accessible ways to wade into real problems from anywhere in the world.\nAbheesht: What, do you think, is the future for KerasNLP? Where do you see it going? What are some exciting developments that are planned in the near future?\nMatt: The near future for KerasNLP is all about sticking the landing with Keras 3 (multi-backend Keras). We want to make sure we have the models people are most excited about (e.g.Â LLama 2 and Falcon), but equally important is a good ecosystem of tools to scale these models up and down.\nOn the scaling down front, we want a good multi-backend workflow for LoRA, and for scaling up, we are working on some dead simple model parallelism (think sharding individual matrices across GPUs). Itâ€™s definitely a lot to execute on, but thatâ€™s kinda fun. We are all heads down on our projects till the release of Keras 3 later this year.\nAbheesht: Having talked to you before, I know how bullish you are on open-source software. Is open source the way forward with LLMs? The upsides are obvious; the downsides include potential misuse, etc. What are your views on this?\nMatt: I think the biggest thing we can all do is stay open to changing our viewpoints. We are way out in uncharted territory in terms of model capability, so the most accurate (and maybe unhelpful) thing we can say is that we donâ€™t know what the biggest risks are or how to mitigate them.\nI should probably just recuse myself from any conversations of existential risk from language models; I donâ€™t know enough. I am glad some people out there are thinking about it, but personally, I am much more worried about the more immediate societal issuesâ€“bias, misuse, misinformation, disruption from automation, etc. These I feel strongly need to be addressed in the public square, and open models will be an essential tool.\nFirst, with an open model, you can crowdsource finding all the issues and fail states. More people probing a model will do a much more thorough job of making it â€œmisbehave.â€ Second, if we really open up the whole training recipe (what data is going in?!), we can have a much more open conversation about equitable outcomes with this new tech.\nI think we have a ways to go as a field, particularly with opening up details on training data. We also need to avoid a â€œrace to the bottom,â€ just throwing more in more compute and data to beat benchmarks without understanding what we are scaling to. But if we want this new tech to do right by society at large, I think open models have a large role to play.\nAbheesht: Keras 3, i..e, multi-backend Keras which allows users to switch backends (TensorFlow, JAX, and PyTorch) was released to much hubbub and fanfare a couple of months ago. What do you think are some of the most exciting use cases of a multi-backend Keras? Why should people be excited about it?\nMatt: Maybe a bit abstract, but I think Keras has the opportunity to become a â€œlingua francaâ€ for modeling problems for a lot more people with Keras 3. Keras is a familiar API to basically everyone in the ML fieldâ€“even if you donâ€™t realize it, its fingerprints are all over abstractions in so many deep learning toolkits.\nWith Keras 3, you can start with a backend agnostic modeling flow, and â€œlowerâ€ it into TensorFlow, JAX, or PyTorch anytime you need. Thereâ€™s no transpilation, all layers/metrics/optimizers are using backend native calls for the selected framework. So basically Keras just becomes an incredibly efficient thing to learn. You can use it as a diving off point for basically any ML problem, and have the expressivity of a low-level framework as soon as you need it.\nAbheesht: What is one research paper youâ€™ve read recently that gave you that â€œahaâ€ feeling? Basically, a paper that blew your mind!\nMatt: A while ago I was in a paper reading group digging into graph neural networks. I found the notions of message passing and permutation invariance among graph vertices to be this big lightbulb moment where a lot of disparate notions in my head suddenly coalesced.\nBasically, you can think of CNNs and Transformers, and RNNs as graph networks. A convolution is just a message passing among neighboring pixels. Transformers are quite literally densely connected graph attention networks, with positional information added in.\nFraming everything as a graph gives you this uniform lens to look across the whole field of deep learning, which is so cool. This article is a good introduction to the idea.\nAnshuman & Abheesht: Thank you, Matt! Itâ€™s been great talking to you!\nAnd that concludes the third interview of our â€œAI Chroniclesâ€ series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP.\nBest regards and happy learning!"
  },
  {
    "objectID": "posts/flux_is_so_flexible.html",
    "href": "posts/flux_is_so_flexible.html",
    "title": "Deep Learning inÂ Julia",
    "section": "",
    "text": "Flux is amazing and itâ€™s far more than just an ML framework. Differentiable programming & Zygote, first class GPU support are features that set it apart among ML systems. The best thing I love about flux is mixing neural nets with differential equations, to get the best of black box and mechanistic modelling, this is what SciML doing.\nFlux is fairly new, and needs attention of communityÂ ! In this article weâ€™ll learn about how to implement a model in Flux.jlÂ . Weâ€™ll walkthrough UNet implementation, which Iâ€™ve been working on lately to contribute to fluxâ€™s model zoo Metalhead.jl.\nAfter reading this article youâ€™ll learn about"
  },
  {
    "objectID": "posts/flux_is_so_flexible.html#unet",
    "href": "posts/flux_is_so_flexible.html#unet",
    "title": "Deep Learning inÂ Julia",
    "section": "UNet ğŸ¥…",
    "text": "UNet ğŸ¥…\nUNet is a deep learning model which was released in the paper U-Net: Convolutional Networks for Biomedical Image Segmentation. The architecture of the network looks likeÂ : Fig. 1 ( Source: UNetÂ Paper)Weâ€™ll not go into detail about UNet theory, as the paper explains it in best way, and explaination here would be redundant anyway.\n\n\n\nSource: UNet Paper\n\n\nThis tutorial will focus on implementation.\nLetâ€™s dive into code rightawayÂ ! The article is written presuming that you have knowledge of implementing a neural networks in PyTorchÂ ! We simply create a class that inherits nn.ModuleÂ . We now create layers and assign it to class with selfÂ . When this model is instantiated, these layers will become attribute of the model object.\n\n&gt;&gt;&gt; exec(open(\"basic_model.py\").read())\ntorch.Size([1, 5, 8, 8])\nLetâ€™s see how it looks like in Julia.\n\njulia&gt; include(\"basic_model.jl\");\n(8, 8, 5, 1)\nSome Key things to note here areÂ :\n\nFlux follows (H, W, C, N) standard for images while PyTorch uses (N, C, H, W)\nChain function is similar to nn.Sequential method from PyTorch\nJulia implements Multiple Dispatch unlike PyThon which is designed on Single Dispatch paradigm. In short multiple dispatch allows us to implement a single method for different combination of different type arguments, unlike python which restricts the methods to be bound to a single object and reimplemented for different classes. Read more about multiple dispatch hereÂ .\n\nLetâ€™s see how official UNet implementation from Torchhub looks like\nThe author of original implementation (hereon referred just as â€œauthorâ€) created a helper method to create a convolutional blockÂ . This is a good practise called as DRY ( Do not Repeat Yourself).\n\n\n\nSource: UNet Paper\n\n\nThis allows us to create these conv blocks (fig.Â 2 zooms into fig.Â 1 to show these conv blocks) just by passing input and output channels. To implement it with Flux, we reuse our knowledge from gist 1Â :\n\nThis allows us to create these conv blocks (fig.Â 2 zooms into fig.Â 1 to show these conv blocks) just by passing input and output channels.\nPretty simple isnâ€™t itÂ ?\nOne thing to note here is, Flux doesnâ€™t allow us to name the layers explicitly, the reason can be found in this github issue. Now letâ€™s go ahead and see how layers were created by author.\nNow we see that we have an encoder, bottleneck, decoder and an upconv layer. 1. EncoderÂ : The four downstairs in Fig. 1 form the encoder block, and it encodes image by successive convolutions 2. BottleneckÂ : The layer between encoder and decoder is called bottleneck. The output of decoder is passed on to decoder & upconv block. 3. Decoder & Upconv blockÂ : The upconv block upsamples the input matrix, i.e.Â deconvolves the input to output with bigger size ( H x W ) than input. Decoder follows upconv layer and increases channels by performing convolution.\nLetâ€™s create the model class in Flux first. Julia doesnâ€™t have classes, it has structsÂ . So model struct would look like this.\n\nTo reduce the redundancy in code Iâ€™ll implement the layers as array of layers, itâ€™ll allow us to write a clean forward pass later. Donâ€™t forget to notice the relation between number of channels of different blocks of UNet model.\nThe unet_block is the convolutional block that we defined earlier is a simple chain of Convolutional and BatchNorm layers. We further chain these blocks keeping in mind the input & output features using the Chain function. SeeÂ ! How easy it is to create a model in Flux. Now we have one last thing remaining, the forward passÂ . We do it like\n\nin PyTorch. The cat operation orchestrates the connection between encoder and decoder demonstrated by copy and crop represented by gray arrow in Fig. 1.\nBut donâ€™t you find the code above messyÂ ? Thatâ€™s where our definition of layers of arrays comes in. Letâ€™s see how the forward pass is written for UNet.\n\nPretty simple & clean, we managed to keep the entire logic same.\nSome key points to be noted here:\n\nÏƒ is nothing else but sigmoid functionÂ ! Julia allows us to use all the mathematical symbols as variablesÂ . Thus, flux defined sigmoid as Ïƒ rather than sigmoid()\nJulia uses matlab like syntax for ranges (see 1:4 for iterating over 1, 2, 3, 4 and 4:-1:1 for 4, 3, 2, 1).\nJulia uses 1 based indexing.\nAny Julia function with a trailingÂ ! tells that operation will be inplace ( remember pass by reference from C++ using & operation).\nThe return keyword in last line is redundant, simply writing Ïƒ(u.final_conv(out)) would work as Julia always returns the output of last line of code, from any code block.\n\nThat brings us to the end of this tutorial. Thanks for readingÂ !\nThis is the second article of my first blog series Julia For the Win. You can find the previous article Kaggle x JuliaÂ : Advanced House Price PredictionÂ : EDA."
  },
  {
    "objectID": "posts/interview_rajaswa.html",
    "href": "posts/interview_rajaswa.html",
    "title": "Interview with Rajaswa Patil, AI Researcher at Postman Labs",
    "section": "",
    "text": "by Anshuman Mishra and Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard of) stories out into the world â€” their struggles, their triumphs and their journey. We will be conversing with people we have looked up to, and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nRajaswaâ€™s Stellar Profile Today, we are honored to have Rajaswa Patil with us. Rajaswa, who graduated in 2021 from BITS Goa, has already had a chequered career so far. He started with a Pre-doctoral position at TCS Research, before working at Microsoft as a Research Fellow. He now works at Postman Labs as an AI Research Associate. Rajaswa is leveraging Generative AI and Large Language Models (LLMs) to build â€œPostbotâ€ at Postman.\nRajaswa is one of those people with whom you can never have a meaningless conversation. He is a cornucopia of knowledge, and always has tidbits of useful information to give out, not just restricted to AI or tech. Before he joined the industry, Rajaswa introduced a research culture in NLP at BITS Goa. He is one of the reasons why I, Abheesht, was able to get out of my initial failures in research and actually get some papers published during my undergrad.\nLetâ€™s begin!\nAnshuman & Abheesht: Hello, Rajaswa! Thank you for doing this!\nRajaswa: Hi, thanks for having me!\nAbheesht: Before we even start this interview, I want to get one heavy question out of the way ğŸ˜‚. What is your opinion on whether LLMs are sentient? Do you think LLMs are intelligent? Which side of the debate are you on?\nRajaswa: I donâ€™t have a strong opinion on this matter, as I find it challenging to provide precise definitions for terms like â€œsentientâ€ and â€œintelligent.â€ The complexity deepens when we consider the broader concept of Artificial General Intelligence (AGI).\nTo gain a more informed perspective on sentience and AGI, Iâ€™ve been delving into a book called â€œSuperintelligenceâ€ by Nick Bostrom. I highly recommend this book to anyone curious about these questions. It offers valuable insights into the subject.\nOne point we can generally agree upon is the potential for computational modeling of human-like intelligence in the future. Human intelligence is the result of millions of years of evolutionary computation, providing us with a relatively intelligent blank slate at birth, which then undergoes further training to reach adult human-level intelligence. In many ways, weâ€™re already making significant strides in engineering computational systems that can provide a head start equivalent to millions of years of evolution in just a month of model pre-training.\nHowever, there are still significant challenges to overcome. Limitations in computational power, data availability, data modalities (which may require more sophisticated model architectures), and other factors are currently hindering our progress. Nevertheless, weâ€™re making rapid advancements toward achieving human-level intelligence in machines.\nAnshuman: Youâ€™ve done your Bachelorâ€™s in Electrical Engineering. What brought you to data science and ML? Please tell the reader about your Machine Learning journey!\nRajaswa: My journey in engineering began in 2017, and my passion for Computer Science was evident from the start. However, despite my enthusiasm, I couldnâ€™t secure a spot in a prestigious Computer Science program at any institute. I faced a difficult choice between the branch of study and the quality of peers and the prevailing â€œcoding culture.â€ In the end, I made the decision to pursue Electrical Engineering at BITS Goa over Computer Science/Information Technology programs at institutions like COEP Pune and IIIT Delhi. What influenced my choice was BITS Goaâ€™s remarkable achievement of having the highest number of Google Summer of Code (GSoC) selections across the country in 2017, with many of those selections coming from students with non-CS backgrounds. This spoke volumes about the thriving tech culture at BITS Goa.\nThe year 2017 marked the rapid rise of Machine Learning and Deep Learning among the tech community. Platforms like Kaggle and Google Colab were emerging, creating new opportunities. Two key factors drew me towards Data Science and Machine Learning during this period. First, there were on-campus courses and bootcamps taught by senior students through programs like CTE or QSTP, which provided valuable insights into these fields. Second, my brother served as a project mentor for Udacityâ€™s Self-Driving Car Nanodegree program, introducing me to the fascinating world of AI and its potential for lucrative careers. This initial exposure piqued my interest, and as I explored further, my fascination with these fields deepened.\nI made a bold decision during this time by forgoing a summer internship offered by BITS in favor of a Summer Internship at IIT Bombay, where I worked on Information Retrieval and Natural Language Processing (NLP). This experience opened my eyes to the significance of research papers in the field of AI, and I learned about prestigious conferences like ACL and EMNLP. I returned from IIT Bombay with a strong determination to publish my own research and started participating in shared-tasks like SemEval. In 2020, our efforts resulted in the publication of three papers. This success led to the establishment of the Language Research Group (LRG) at BITS Goa.\nNLP was not a popular choice at the time, as it was more challenging to program compared to some other fields, and it lacked the visual appeal of areas like Computer Vision with GANs and Image Segmentation. However, our unique focus on NLP gave us a distinct advantage. The Language Research Group thrived, and we all built impressive profiles for ourselves in the field, thanks to our work in NLP.\nAbheesht: In my conversations with you back in 2021, I remember how eager you were to take up a PhD in Computational Linguistics. What made you change your mind? Why did you decide to ply your trade in the tech industry over choosing a career in academia? Have you completely closed the door on doing a PhD in the future?\nRajaswa: Indeed, I had been contemplating pursuing a PhD since my sophomore year. Teaching has always been a passion of mine, and I could envision myself in academia. However, I made the choice to remain in the industry for several reasons, a mix of personal and professional considerations.\nOn a personal level, I had originally envisioned a future where I would build my academic career in the West while also starting a family. Unfortunately, a series of unforeseen events led me to reconsider and ultimately give up on that dream. It became clear to me that academia, regardless of the location, would impose significant limitations on both my social and financial freedom. This realization played a significant role in shaping my decision to remain in the industry.\nWhen I turned 23, my friends gifted me â€œThe Almanack Of Naval Ravikantâ€, and within its pages, I stumbled upon a quote that left a profound impact on me, particularly concerning Applied Scientists:\nSociety, business & money are downstream of technology, which is itself downstream of science. Science applied is the engine of humanity.\nCorollary: Applied Scientists are the most powerful people in the world. This will be more obvious in coming years.\nIt doesnâ€™t take a genius to recognize that AI is poised to become one of the most significant advancements of the 21st century. This quote forced me to reevaluate how I could position myself in the midst of this historic progression. At the time, I had been immersed in the AI4Code domain since my graduation, and it was evident that this field was already undergoing substantial transformations with the emergence of groundbreaking tools like Copilot during the Large Language Model era. The evidence was right before my eyes, affirming the relevance and power of applied scientists in shaping the future.\nDuring that period, I was actively preparing for my PhD applications, juggling tasks like publishing papers at TCS Research and taking exams like GRE and TOEFL. It was during this time that Sumit Gulwani, a luminary in the field of AI4Code, approached me with an intriguing opportunity: a Research Fellowship at his team, Microsoft PROSE. This offer presented an industry alternative to pursuing a PhD. It entailed the possibility of joining the team as a Research Fellow, followed by a role as a Research Associate for 3â€“4 years, with a smooth transition into a full-time Scientist position, all without the need for a Masterâ€™s or PhD. This proposal put me in a considerable dilemma because, by then, I had more or less ruled out the traditional academic path and was leaning towards an industry career. Pursuing a PhD for an industry position seemed less attractive in comparison to this enticing offer.\nDespite my confusion, I sought counsel from a wide range of individuals with diverse backgrounds and varying levels of experience. However, even after these discussions, I found it challenging to make a decision. Eventually, I chose to move forward with an industry career primarily because it offered greater flexibility compared to the commitment required for a PhD.\nMy perspective, then and now, is that the industry holds immense potential for value creation. I believe this trend will continue for the next few years. With substantial investments from Big Tech companies, investors, and governments pouring into AI, there is no doubt that the industry is where the action is. Many top AI/non-AI academics worldwide are either launching startups, contributing to open-source projects, or collaborating with big tech firms and governments to apply AI in practical contexts. Itâ€™s evident that most major advancements, whether theoretical or practical, are emerging from the industry. This is because the industry boasts the necessary resources and motivation.\nAs of today, this is where I want to be. However, I havenâ€™t entirely abandoned my PhD aspirations. I do envision pursuing a PhD at some point, but it will likely be a part-time endeavor and may not necessarily be in Computer Science, although it will remain closely related to AI. I anticipate that this plan will materialize several years down the line, possibly 5 to 6 years from now, at the earliest.\nAbheesht: Your journey so far has been inspirational. You come from a third tier town, and did not have the opportunities one has in bustling, metropolitan cities. You fought all the odds, made it to a reputable college, performed research and now work at an enviable position at Postman. Tell us a bit more about your struggles, say, the culture shock you faced when you joined college, or how you gained the confidence to rub shoulders with the best in the business.\nRajaswa: Honestly, Iâ€™ve been incredibly fortunate in my life journey. Despite hailing from a Tier-3 city, Iâ€™ve had the privilege of a strong educational foundation â€” a bit like being a big fish in a small pond. My father holds a PhD in Biotechnology, and my mother earned a Masterâ€™s degree in Organic Chemistry. Moreover, my older brother, who is five years my senior, has always been a trailblazer for me. Whether it was excelling in the Joint Entrance Examination (JEE) or venturing into Software Engineering and Machine Learning, he led the way, providing me with invaluable guidance based on his experiences.\nHowever, when I entered college, especially during my first year, I experienced a significant culture shock. I quickly realized that people judged you based on your manner of speaking, your choice of clothing, and various other factors. BITS Goa was not home to many Tier-3 city students. In fact, Iâ€™m quite certain that over 70% of the student body came from Indiaâ€™s major metropolitan areas, with a notably Westernized mindset. This was a stark departure from the slightly more conservative culture I was accustomed to in Tier-3 cities in India. I was also highly introverted during this phase. In fact, during my first semester, I hardly left the campus at all. For the initial couple of months, I didnâ€™t even venture across to the A-wing of the campus. I didnâ€™t have much disposable income, I didnâ€™t indulge in drinking or openly participate in parties and celebrations â€” factors that significantly impacted my social life. To compound matters, I missed my mid-semester examinations due to a family medical emergency and found myself falling behind in all my courses.\nIt was an undeniably challenging period. However, things began to change when I made the decision to forego a summer internship opportunity and instead pursued an off-campus internship that allowed me to return, publish research papers, and start earning money through internship or project stipends. This shift helped me gain recognition on campus. Part of it stemmed from the fact that I became known as the person who could assist in securing research papers for MS applications or help others connect with top professors for projects and reading courses. With the establishment of the Language Research Group (LRG) and my role in teaching ML courses at CTE and QSTP, my social life improved, as did my self-confidence. Over time, I ended up assisting numerous individuals, making my on-campus experience quite rewarding. It became common for people to greet me with a smile of acknowledgment as we passed one another on walkways and in corridors.\nEven today, when Iâ€™m out in Bengaluru, itâ€™s a delightful surprise to be recognized by people, primarily BITSian juniors, who remember me from somewhere, despite the fact that I believe I havenâ€™t achieved anything extraordinary to warrant such recognition.\nAbheesht: I want to talk a bit more about how giving youâ€™ve been to the community. Youâ€™ve never â€œgatekeptâ€ the knowledge youâ€™ve had. A good example of this is when you formed the Language Research Group (LRG) at BITS Goa. What motivates you to keep giving back to the community?\nRajaswa: My passion for teaching is quite straightforward. Additionally, I am a staunch advocate of the â€œFeynman Technique.â€ I truly solidified my understanding of Machine Learning theory when I had the opportunity to teach it to my junior peers. Notably, my first significant paper was published with me as the last author, serving as a supervisor without direct contributions. This teaching and mentoring role has proven to be an invaluable professional growth experience for me, providing a natural incentive to continue.\nIn a broader context, I view education as an arena where one can achieve â€œlow-effort high-impactâ€ outcomes. Consequently, I consistently invest my time and resources in educational pursuits, whether itâ€™s for the betterment of society or to serve my own personal interests. I actively engage in both aspects, finding them equally rewarding and fulfilling.\nAnshuman: Could you tell us more about your role at Postman Labs and how a day looks like?\nRajaswa: My current role is nothing short of incredible. Iâ€™m a part of the Labs team, where we operate in a zero-to-one environment, emphasizing rapid experimentation and swift product deployment. At present, Iâ€™m the sole core-AI specialist within the team, which is composed of remarkable talents who inspire me daily. Among them are Abhijit Kane, the co-founder of Postman, and Shamasis Bhattacharya, who serves as my manager and is the Head of Labs. Alongside these brilliant minds, we have a dedicated team of engineers, designers, and data analysts. In many ways, I have all the essential elements within my team to transform the product of my imagination into a reality.\nMy role is exceptionally diverse, allowing me to engage in various facets of the product development process, including research, design, analytics, and even occasional contributions to our backend services. Given my unique position as the sole core-AI member, I also play a crucial role in cultivating an AI culture within Postman. This involves sharing daily updates on AI developments, organizing invited talks, and facilitating communication with external teams.\nThe discussions we have within the team are remarkably ambitious, challenging me to adopt new perspectives and ways of thinking to achieve goals on a larger scale. My work primarily involves researching new directions for our product, particularly Postbot, and I take great pride in contributing to Postmanâ€™s broader objectives.\nAbheesht: LLMs are prone to hallucination, prompt injection, etc. How do you deal with these issues? Are LLMs reliable enough in production?\nRajaswa: To mitigate potential attacks or abuses of the product, itâ€™s essential to engineer specific guardrails that closely monitor either the input or output to the system, as well as user behavior patterns. Itâ€™s worth noting that large language models (LLMs) are susceptible to generating incorrect or â€œhallucinatedâ€ content, which can occur periodically in most products. While there isnâ€™t a perfect solution available at this time, one temporary measure is to make the system as human-in-the-loop (HITL) as possible. This entails providing the system with capabilities for recording consent, gathering user feedback, and enabling correction mechanisms. The primary objective is not to create a flashy system but to develop a practical and usable one, doing so as quickly as possible.\nAnshuman : AI is moving at breakneck pace. How do you stay up to date with cutting edge?\nRajaswa: To be honest, I donâ€™t exert an immense effort to stay updated, but I have some effective strategies in place. I maintain a dedicated Twitter list focused on AI development, which I monitor daily. My LinkedIn feed has also become a valuable source of relevant updates. I subscribe to several newsletters and mailing lists, primarily related to the research community. Additionally, Iâ€™m an active participant in various communities on platforms like WhatsApp, Slack, and Discord. These communities provide valuable insights, and I engage actively in discussions within some of them.\nAbheesht: Youâ€™ve worked in several research labs. How has working in research labs been like for you? How do labs in India compare with labs in the US?\nRajaswa: My experience with Indian research labs, in general, has been less than stellar, and Iâ€™ve noticed that many of my peers share similar sentiments. However, itâ€™s worth noting that one potential upside to these experiences is that the learning curve can often be steeper, which can ultimately benefit you in the long term. I havenâ€™t had the opportunity to work with any research labs in the United States, so I canâ€™t provide a direct comparison in that regard.\nAbheesht: What suggestions/advice would you give someone who has just started learning/working on ML?\nRajaswa: Begin your AI journey by diving into building. Today, with the abundance of APIs available, you can construct and showcase an end-to-end working system swiftly. Simultaneously, explore the intricacies of machine learning on the side.\nI firmly believe that with your programming skills, you can assemble roughly 80% of any practical AI system. The remaining 20% can be tackled with in-depth ML knowledge, which will empower you to debug and optimize your creations. However, given the rapid pace of AI development, I donâ€™t think your primary objective should be focusing on that final 20%. Instead, prioritize getting started with building as soon as possible.\nAnshuman & Abheesht: Thank you, Rajaswa! Itâ€™s been great talking to you!\nAnd that concludes the second interview of our â€œAI Chroniclesâ€ series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anshuman Mishra",
    "section": "",
    "text": "Hi there ğŸ‘‹. I am Anshuman Mishra. I work on large language models yet I like to call myself a software engineer.\n\nTo know about my projects, please refer to my GitHub profile.\nI work on large language models at FlipAI. For more details on my work experience, check out my LinkedIn profile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeline\n\n\n\n\n\n\n\n\nPosition\nCompany\nDuration\n\n\n\n\nML Engineer\nFlipAI\nJuly 2023 - Present\n\n\nTechnical Author\nWeights&Biases\nSep 2023 - Feb 2024\n\n\nData Science Intern\nBNY Mellon\nJan 2023 - June 2023\n\n\nSoftware Engineer Intern\nAmazon\nMay 2022 - July 2023\n\n\n\n\n\nSome stuff\n\nGoogle Developer Expert in Machine Learning and Google Cloud Platform (bit.ly/expert-sp)\nMentee, Google Summer of Code 2023, in TensorFlow. See my project here\nOpen Source Contributor at KerasNLP"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Interview with Aakash Kumar Nain, MLE at Merlyn Minds\n\n\n\n\n\n\n\n\nDec 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIllustrated LLM OS\n\n\nAn Implementational Perspective of LLMs as Operating Systems\n\n\n\n\n\nDec 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with Matthew Watson, Keras Team, Google\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with Rajaswa Patil, AI Researcher at Postman Labs\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning inÂ Julia\n\n\nImplementing ML models in Flux, the Julia Deep LearningÂ Package!\n\n\n\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nComprehensive data exploration with Julia\n\n\nExploratory Data Analysis of Housing dataset in Julia\n\n\n\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nInterview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD - Ravi Ramakrishnan\n\n\nInterview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD- Ravi Ramakrishnan\n\n\n\n\n\nSep 12, 2022\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/interview_rrk.html",
    "href": "posts/interview_rrk.html",
    "title": "Interview with 2x Kaggle Grandmaster, Data Science Manager at Emirates NBD - Ravi Ramakrishnan",
    "section": "",
    "text": "by Anshuman Mishra & Abheesht Sharma\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard of) stories out into the world â€” their struggles, their triumphs and their journey. We will be conversing with people we have looked up to, and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nKaggle Profile of 2xGrandmaster Ravi Ramakrishnan Today, we are honored to have Ravi Ramakrishnan with us. Ravi is a Data Science Manager-Credit Risk at Emirates NBD. Prior to Emirates NBD, he used to work at the Commercial Bank of Dubai as a Data Scientist, and has 8+ years of experience in the industry. He holds a Bachelorâ€™s degree in Electronics, and completed his MBA in Finance from NMIMS. Ravi contributes extensively on Kaggle, and is a Notebooks Grandmaster (ranked #23) and a Discussions Grandmaster (ranked #2) on Kaggle.\nAnshuman & Abheesht : Namaste, Grandmaster! Thank you for taking the time to do this.\nRavi: Hello, Anshuman and Abheesht. Thanks for inviting me to do this. Saadar pranaam!\nAnshuman: Youâ€™ve done your Bachelorâ€™s in Electrical Engineering, post which you did an MBA. What brought you to data science and ML? Please tell the reader about your Machine Learning journey!\nRavi: Absolutely, Iâ€™d be happy to share my journey. While my academic background is in electrical engineering (BE) and I later pursued an MBA, my entry into the world of data science was somewhat serendipitous. Back in 2015, during a visit to Dubai for my CFA Level 3 exam, I had the opportunity to attend interviews with my previous employer, who was in the process of establishing a central analytics structure known as BICC. To my surprise, I was selected for the role of a data scientist, even though my previous focus had been on finance and investment banking. I decided to take up the challenge, giving myself a year to assess my career path. My initial days in this role were demanding as I had to bridge the gap in my knowledge of data science tools like SQL, SAS, MATLAB, SPSS, EDW, and CRM development. However, with dedication, guidance from colleagues, and consistent learning, I quickly overcome these challenges.\nOver the years, I became deeply involved in model development across various domains, including customer segmentation, CRM lead management, IFRS9 PD-LGD model development, ECL calculation engine development, and campaign management. This journey of hands-on experience and continuous learning led me to discover my passion for data science.\nWhen I look back, I believe that my career in data science found me rather than the other way around. Itâ€™s been a rewarding journey of almost a decade, and Iâ€™m excited about what the future holds. I hope my experience can inspire others to explore and embrace this dynamic field of data science and machine learning.\nAnshuman: Given your serendipitous journey into data science and your passion for continuous learning, Iâ€™m curious to know: what specifically inspired you to pick up Kaggle and dive into the world of machine learning competitions?\nRavi: My yester manager introduced me to Kaggle in 2020 during the lockdown. I was busy with some certifications in fitness then and promised him to have a look at it after passing those exams. I eventually joined Kaggle in September 2021 after completing my Orthopedics specialization. I thank him for introducing me to Kaggle as I feel I would have missed out on a very important component of my learning journey had I not joined Kaggle. I perused the tiers on this platform slowly and steadily and established a learning and development plan for myself akin to a fitness plan. I usually advise my patrons to follow a macro-meso-micro fitness periodization cycle with planned gains and achievements all along the journey. I followed my fitness regimen in Kaggle, focussing on small but consistent efforts on a daily basis. I set realistic goals for myself and consistently achieved them, else reset the goals apropos to contemporary situations, keeping a balance between Kaggle, fitness, running and office commitments.\nAbheesht: What was your motivation behind starting with the discussions track as opposed to say, the competitions track on Kaggle?\nRavi: My motivation to excel in the discussions track on Kaggle, ultimately achieving the status of Grandmaster, was driven by several factors. Firstly, discussions on Kaggle offer a unique platform for knowledge exchange and collaboration with fellow machine learning enthusiasts. I was drawn to the open and creative environment that discussions provided, offering an opportunity to engage meaningfully with the community. Additionally, active participation in discussions allowed me to continuously learn and stay updated with the latest trends and techniques in the field. It was a way for me to both contribute to the community and absorb insights from others, contributing to my own growth as a data scientist.\nFinally, achieving Grandmaster status in discussions represented a significant milestone in my Kaggle journey, reflecting my commitment to mastering various aspects of Kaggle. It served as a testament to my dedication to sharing knowledge, fostering collaboration, and contributing to the thriving Kaggle community.\nAbheesht: Given your demanding role as a Data Science manager, how do you effectively allocate time to maintain an active presence on Kaggle? This is something I struggle a bit with â€” managing time between work, and open source.\nRavi: Managing my role as a Data Science manager at Emirates Bank while maintaining an active presence on Kaggle requires effective time management and discipline. Iâ€™ve structured my daily routine to strike a balance between work and Kaggle activities.\nMy day is divided into distinct parts: 1. Morning (6 am â€” 8 am): Dedicated to exercise, ensuring physical well-being. 2. 8:30 am â€” 6 pm: Focused on my responsibilities at the bank. 3. 6 pm â€” 9:30 pm: Allocated for Kaggle activities, where I participate in discussions, competitions, and kernels. 4. 9:30 pm â€” 10:00 pm: Reserved for meditation and mindfulness activities to maintain mental clarity.\nThis disciplined approach allows me to optimize my time efficiently. I also prioritize health through mindfulness practices, exercise, and a balanced diet, which helps maintain physical and mental well-being. By adhering to this routine, I can effectively manage my demanding work and Kaggle commitments.\nAnshuman: People participating in the discussion track on Kaggle are very opinionated, and most competition grandmasters, from time to time, appeal to remove it from a ranking tier. What is your opinion on this?\nRavi: I respect their opinions but wish to mention that the discussion track is akin to a glue that binds the community together. Everyone canâ€™t be a competition participant but quite a few wish to learn as well. The discussion track posits some valuable information regarding general ML practices, MLOps, coding elements, competition solutions, resources, career advice, progression appreciation and key industry updates and provides valuable information for one and all, especially beginners to onboard, learn and develop appropriately. I wish to opine that we have 300+ competition GMs as on date and 66â€“67 discussion GMs too. It is far more difficult to progress in this tier than peopleâ€™s general fads. One needs to be good at communication and present oneâ€™s content well to secure any form of progress in this tier.\nAnshuman: Could you please share a few occasions when you made valuable connections or benefitted in great deal from it?\nRavi: I have learnt a lot from elite Kaggle users throughout my tryst with Kaggle. Users like AmbrosM, Marialla Pratta, Dr.Â Chris Deotte, Laurent Purchout, Dr.Â Carl McBridde Ellis, Sanyam Bhutani, Parul Pandey, SRK, Rohan Rao, etc. will always be my inspiration on Kaggle and even otherwise. I have learnt a lot from their Kaggle and other work and will consider them my mentors always. I wish to thank you as well for connecting with me and providing me a chance to elicit my thoughts on this topic.\nAbheesht: You are very fond of Kaggleâ€™s playground series. Why do you prefer it over other competitions on Kaggle?\nRavi: I like to participate in the playground series due to the below reasons-\nThis series offers room to experiment with feature engineering and models to a great extent. Datasets are simpler than featured competitions and hardware requirements are tepid. One could ace these competitions using the freely available resources on Kaggle and Colab without any undue advantage based on hardware. I opine that the select few participants in other featured competitions category using professional login Ids and having access to premium resources do carry a sizable advantage over the rest, rendering these competitions a no-so-level-playing-field. This issue is circumvented in this series almost entirely. I like ML model development on tabular data and these competitions offer me opportunities exactly matching my interests. These competitions are well spaced through the year with a 2â€“3 week cycle. This offers time to develop models within a limited time frame and move onto subsequent challenges swiftly. I somehow fail to align with a longer duration of 3 months elsewhere without any major reward for 90% participants elsewhere on the platform. I admire the extent of insightful discussions and kernels shared in the forums as we donâ€™t have any predisposed inhibitions arising from medal attributions herewith. I have learnt a lot from these forums and contribute to my best extent too, keeping a few tricks private. Most of the playground competitions have 1000+ participants making them liquid from a LB perspective. I have performed well in quite a few episodes and am receiving consistent results nowadays, making my participation more lucrative. Anshuman: Do you feel the Kaggle competitions are related to your work?\nRavi: I do not think that this series is particularly linked to my work tasks but opine that it mirrors my interest areas greatly. I can manage my daily routine perfectly with this series as one may spend a couple of hours daily to secure a good score in these competitions. This enables me to invest time in other activities keeping my Kaggle participation to a best feasible optimum.\nAnshuman: What kind of challenges do you look for today? How do you decide if the competition is worth your time?\nRavi: I wish to expand my horizons in featured competitions in the medium run of the next 1â€“2 years. In choosing Kaggle challenges today, my focus has evolved over time. Iâ€™m increasingly interested in featured competitions as I aspire to expand my horizons in this area over the next 1â€“2 years. Additionally, I aim to maintain a balance by contributing to the Kaggle playground series, which aligns well with my interests.\nWhen deciding if a competition is worth my time, I consider several factors:\nData Size: I assess the data size; smaller datasets can be appealing as they allow for quicker code completion and often align with simpler models. However, I consider the complexity of handling small datasets with numerous columns. CV Score Stability: I examine the standard deviation of the CV (cross-validation) score to gauge model stability across iterations and folds. Consistent CV scores are typically more appealing. CV-PLB Relations: I look for competitions where thereâ€™s a meaningful relationship between CV scores and the public leaderboard (PLB). This helps in making informed final submissions. Time Availability: Personal and professional commitments play a significant role. I avoid challenges where I may not be able to devote sufficient time toward the end, instead opting for competitions that align better with my schedule. Balancing these considerations helps me make informed choices about which competitions to participate in, ensuring that my time is well-invested and aligned with my goals.\nAbheesht: For noobs like me who havenâ€™t dabbled much in Kaggle competitions and discussions, what would be your best advice?\nRavi: I may suggest a few points based on my experience on the platform-\nKeep learning as cynosure of all your activities on Kaggle and otherwise. This is a much more satisfying experience than aiming for medals. Till date, I never aimed to become a GM but aimed to become a better ML enthusiast. Stay consistent in any life and personal endeavor. This stems from 2 elements- interest in the activity and realistic goal setting. I believe in SMART goal planning and periodization and have implemented it across all walks of life. This is an open secret to my success on Kaggle and in other walks of life too. Leann to be modestly assertive. Saying a no without being rude is a very important skill that requires some training and experience. This will help one and all at work I have deep respect for time. I value my time a lot and respect it a lot. I try and be punctual and leave on time. I donâ€™t believe in late work practices and to this day, have mostly avoided this to good effect and lots of professional success too. Develop a life-work balance â€” this becomes important as one ages, as one delves into multiple commitments too. One needs to balance multiple life events in parallel and oneâ€™s planning and execution skills are put to a rigorous test. Balancing various life and routine events is key to success in multiple facets of oneâ€™s personality Take structured breaks from one activity at a time rather than a complete break from all ongoing activities. I usually take structured breaks from the office (only) followed by a break from my fitness practice and then a break from Kaggle amortized over a span of 6â€“8 weeks. This keeps me motivated for a longer period of time, enabling me to render a fine balance across multiple events of almost equal priority Try and automate as much code as possible- this is specifically useful for competitions and repetitive tasks like curating baseline models, feature processing, preprocessing and general training. One may then edit the general pipeline to add assignment specifics. This is likely to save time and enable greater productivity Team up well and plan your strategy. Teaming up with friends helps a lot across all 4 tiers. Collective endeavor elicits significant synergy based power. Avoid dubious practices that could harm your reputation. This may hinder your progress a lot. Use Kaggle free resources to good effect. 50 GPU + TPU hours is significant and is available at your disposal per week. Post content consistently on Kaggle and share ideas, Inhibition is the enemy of collaboration and collaboration is a good route to success. Try and learn new skills/ improve existing skills periodically. Also try and match your current learning patterns with your long run learning goals periodically. If you digress from your long run goals, you may be better with either rebalancing the long run goal/ current activity. Develop an all-round profile outside of Kaggle too. ML is an ocean of opportunities and Kaggle is one of the ways to attain success. Stay active elsewhere as well, including but not limited to Analytics vidya, YouTube, GitHub, medium.com and any other community you find suitable. Hugging Face competitions are also good to learn and grow in this regard. I encourage one and all to participate in hackathons and any local competitions and conclaves that offer networking opportunities too. Remember that networking is as important as learning as the industry relies a lot on this aspect for referrals and job opportunities. Enjoy the journey and derive value from every step of your journey. I suggest one could break down a long run problem into a series of structured and achievable micro-goals that could eventually lead him/ her to success. I usually do this to good effect (with some meticulous planning and experience) and encourage others to follow suit. Keep others updated with your successes. LinkedIn is a good place to post about your professional successes including Kaggle progression, ranks and competition approaches too. Abheesht: When you are given a problem statement, how do you devise AI solutions for it? Do you mostly use classical ML models, since most of the data you use is tabular? What do you look for in a proposed solution?\nI predominantly work in credit risk areas that do not involve AI models to any extent. I think this is a huge drawback of this career path as Financial Regulators refrain from accepting results from AI models and latest advancements in this field of knowledge. Most of our models are classical ML models with emphasis on tabular data and simple algorithms.\nI usually devise my work assignment into the below steps for convenience and project planning-\nI understand the end-userâ€™s requirement and the assignmentâ€™s long run usage before starting work on the project. Usually I am involved in assignments that necessitate continual usage and a user-friendly and clean data and model production pipeline. I break down the assignment into several micro-goals spanning over a 1â€“2 week period. I plan these goals with emphasis on data wrangling as a primary task spanning over 80% of the overall time invested. I usually automate the data pipeline efficiently, working towards a production and deployment all through the development process. This eases the production and deployment process substantially and improves stakeholder satisfaction. I usually conduct a catch-up meeting with the team and the end user at the end of every micro-goal. Most of our projects and assignments are internally and externally validated. I ensure that all our data processes are completely reproducible and are validated before we commence with the model development. Hence, I engage the internal validation team early in the project, facilitating timely comments and concerns addressing them immediately. Once we zero in on the development data, we conduct a bigger meeting with senior management, explaining the key data challenges and assumptions in the data pipeline. We also demonstrate the feature shortlisting processes (we have built several internal automated tools for the same) and revert to feedback from key stakeholders. This enables us to build models freely thereafter without any adverse comments later in the project lifecycle. We document the minutes of this meeting and circulate to Steering committees and auditors to ensure a 4-eye check on the progress and results. We then build a simple baseline model and showcase the selected variables to the key end-users, accepting their feedback and working on the same. Finally, we tune our models, ensemble results (if needed) and prepare the final model and submit it to the business team for review. As a standard practice, I build 10â€“15 candidate options and deploy them simultaneously to elicit an end-state result. This may perhaps posit a provision forecast/ NPA value/ default rate prediction/ PPNR per model selected. Business teams are highly comfortable discussing the model along with the result in this manner rather than a percentage result usually generated from the model. We then engage the internal and external validators and ensure our project is well documented. This is highly important in my area of work as Regulators usually peruse our model documents in detail. As a final step, we engage the IT and deployment teams to deploy the model in production integrating the model results into a report and entering controls as deemed necessary. This is a lengthy process and has to be facilitated with several UAT rounds and stake-holder approvals. This usually spans across a couple of quarters after the model is internally validated. As a final step, we are also supposed to document the IT implementation reports in a prescribed format and send them to the Regulator upon instructions. The model governance process along with these documents and code are thoroughly scrutinized regularly by the Regulator as part of their regular audits and reviews. Considering the sensitivity of the results involved, we usually select a simple model with explainable features as our chosen model for an assignment. We are particularly careful about production specific costs and time involvement to deploy the model and usually do not choose variables that otherwise perform well in the training period but are difficult to curate in production. We usually consult domain experts (economic research teams, credit underwriters and credit policy experts) to ratify and opine on the model development process and consider their subjective inputs as part of model governance.\nAbheesht: For the initial years of your career, you must have been an Individual Contributor (IC), before you transitioned to a managerial role. What are the major changes one has to make, to succeed in a managerial role? Which role do you like more?\nMy role in my team is a combination of an individual contributor and a manager. As a manager, I have the freedom to design my project plan to good effect based on the overall resources available in the team and the budgets involved. I usually resort to the below norm while working on several assignments-\nI prefer to perform data wrangling individually if the project spans over a longer duration. I opine that this provides me more control on an important aspect of the project helping me to automate and design the pipeline to my strengths and weaknesses. I also engage the stakeholders to my schedule and am more comfortable engaging the validator in this process. Once the data is finalized, I hand it over to junior colleagues to build a model. This fosters a win-win for all colleagues involved and often results in a timely completion. In some cases, I individually complete the entire assignment on my own, including data and models too and hand over the results to others to deploy. Other projects often require collaboration. As an example, IFRS9 models for PD-LGD are long run assignments that span over a year. We usually split the project into 3â€“4 managers, with each manager responsible for a set of products/ business entities. Our bank is a large international conglomerate, hence we need to engage foreign teams too (this is an interesting challenge in itself). I usually collaborate with my team and foreign teams in such assignments and demarcate roles for each participant with strict timelines. We usually split the overall assignment into micro-cycles and plan them properly to ensure smooth and effective progress. I usually assign tasks to junior colleagues based on their strengths and weaknesses and often ensure a well documented peer review. This ensures correctness and smooth progress through the project. I usually liaise with external stakeholders and internal validators and often defend our assumptions in meetings. This requires some negotiation skills in my opinion. I leant this from my seniors in my 2 employers and feel that I am adept at this aspect of project management currently. My role in my previous employer was an individual contributor while my current role involves both aspects of project management. I opine that one needs to be highly diligent at time and resource management and set and plan goals efficiently as a manager/ lead. Oneâ€™s technical skills are seldom tested while managing assignments, but oneâ€™s negotiation skills, ability to work under tight timelines and strict budgets, handling escalations are tested to a greater extent as a lead. I think soft skills are more important herewith and this is gained with experience and inputs from senior colleagues. I have a good network in the industry and I use this to good effect to learn and improve myself over time. I usually do not have a preference for a role type, but given the overall career progression paths in the industry, I may perhaps choose to progress as a lead / senior lead with collaborative skills and roles going ahead.\nBest regards and happy learning!\nThatâ€™s it. This was the very first interview of our new blog series AI Chronicles ! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an ML Research Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "posts/interview_aakash.html",
    "href": "posts/interview_aakash.html",
    "title": "Interview with Aakash Kumar Nain, MLE at Merlyn Minds",
    "section": "",
    "text": "by Anshuman Mishra and Abheesht Sharma\n\nThe AI world is brimming with talent. Folks in the AI industry and the research world are continuously pushing the boundaries of innovation. With this interview series, we want to focus on bringing these inspiring (and possibly unheard-of) stories out into the world â€” their struggles, their triumphs, and their journey. We will be conversing with people we have looked up to and still look up to for their unparalleled contributions to the AI world. We call this interview series AI Chronicles.\nThe two of us with Aakash in Uzbekistan! Aakash Kumar Nain â€” open source legend, MLE extraordinaire, Delhi lover, mountain climber. We had the privilege of meeting Aakash in Tashkent at the Google ML Community Summit and were taken aback by how laidback and easygoing he was. Looking at how meticulous he is with his work, we were expecting him to be the â€œserious kindâ€. The three of us instantly hit it off.\nWhen we say Aakash is an open source legend, we kid you not. Aakash started contributing to DL open source in 2016, and has never looked back since then. He is a TensorFlow-Addons maintainer. He is a core contributor to Keras 3.0, the new multi-backend Keras. Our favourite open source work of his is, however, Annotated Papers, where he breaks down ML/DL papers into simple explanations. Aakash is a huge proponent of JAX. Safe to say, he has inspired folks like us to take to open source.\nOn top of his impressive open source work, he has 7 years of experience in the industry as an ML Engineer/Researcher, starting his career at Parallel Dots and then working at Ola, H2O.ai and now Merlyn Minds where he is working on building an AI assistant for education. Despite having spent 7 years in the industry, he has lost none of his inquisitiveness and curiosity.\nWe can keep raving on and on about Aakashâ€™s technical accomplishments, but to us, what separates Aakash from the rest is his approach to life. Read on to find out more about the man!\nHi, Aakash! Thank you for doing this.\nThank you for having me here today.\nQ: Could you discuss any machine learning projects or applications that youâ€™ve found personally rewarding, either in terms of impact or the technical challenges involved?\nThough all the open-source projects I worked on were rewarding, contributing to TensorFlow-Addons, Keras, JAX, and Keras Core created the biggest impact. Although I have been a TF user since 2016, my open-source journey started with TF-addons in 2019. Once you start exploring the low-level bits of a library or a framework, your understanding of the mental model improves automatically. I wrote many code examples for keras.io where I showcased the flexibility of the Keras API to build very complex models. The motivation for writing those code examples was to help people understand the mental model behind tf.keras\nI consider myself a power Keras user. I have always wanted to contribute to the core Keras engine. This year, I finally got an opportunity to collaborate with Francois Chollet and the team to rewrite the entire Keras codebase with multi-backend support, including but not limited to TensorFlow, JAX, and PyTorch. It is one of my biggest open-source collaborations with the highest impact.\nQ: The field of machine learning is continually evolving. Are there any emerging trends or developments that particularly excite you or that you believe will have a significant impact in the near future?\nDiffusion models are one of the things that have gotten me excited about the future of generative AI after a long time. Though generative machine learning algorithms are nothing new, itâ€™s just that the underlying algorithm for Diffusion models feels more natural. The other thing that I am looking forward to is the successor of the Transformer architecture. CNNs to transformers was a big leap. I hope the next set of algorithms is equally revolutionary.\nQ: How do you approach maintaining a balance between your athletic interests, such as being a footballer, and your demanding career in data science? Do you find any synergies between these seemingly different pursuits?\nEver heard of the phrase â€œToo much of anything is good for nothingâ€? If you like/love doing something, that doesnâ€™t mean you should devote your entire time to it. Of course, itâ€™s a biased opinion, but to me, there is way more to life than just work. So, I distribute my time among all the things I enjoy doing. It doesnâ€™t mean that it should be like that for anyone else in any sense. Itâ€™s a personal choice how much time one wants to dedicate to different aspects of life. Also, I am a firm believer that health is the true wealth. To ensure that I am physically fit, I go to the gym and play football regularly.\nQ: Many aspiring data scientists grapple with imposter syndrome. Have you ever experienced such feelings, and if so, how did you overcome them?\nOh absolutely! Though many people may suffer from imposter syndrome at different stages, I think itâ€™s way more common during the early years. When working in a fast-moving field like ML, it is natural to feel lagging, seeing the incredible progress in ML and AI daily. The best way to overcome such feelings is to understand that you donâ€™t have to follow every other trend. Fundamentals matter the most in the long run. If you have a solid understanding of the fundamentals, things will always be easy.\nQ: As a maintainer of TensorFlow addons and contributor to multi-backend Keras, can you give us some insights into the open-source development process, including the joys and challenges of contributing to these projects?\nContributing to open-source is a commitment. Itâ€™s not a one-time thing. If you are working on an open-source project, ideally you expect that the community will use it, and that too for a long period. Developing something that people are eager to use needs well-defined goals. The development of the project has to align with these goals.\nThe best part of OSS is that you get to collaborate with many talented people. Once you start developing things in collaboration, you get more clarity about the modalities (using the word modality loosely in this context). You start to optimize for maximum coverage-minimal maintenance. Talking about the challenges, I think the biggest challenge is commitment. Many people do OSS just to make it a bullet point in their resume, but some of us do it because we enjoy solving complex problems for a larger audience. If you have a full-time job, then working on OSS means cutting down on other things to find time for the project.\nQ: How do you stay updated with the latest developments in the field of machine learning? Are there specific resources or communities you recommend for those looking to stay informed?\nReading research papers is the easiest way. The problem, nowadays, is that it is very hard to filter out good papers from the â€œpileâ€ of research papers being dumped on arXiv every day. Earlier I used to use arXiv-sanity for filtering paper, but recent changes made to it made it somewhat less usable for me (asking Karpathy to bring back the top-rated filter). Now I rely mainly on my Twitter feed for filtering papers. Itâ€™s not that good, but it works.\nTo answer your question about communities and resources, I think Twitter and Kaggle are hands down the best. If you constantly use the two, you pretty much are aware of the latest research trends and the things that work in practice.\nQ: If you had to offer a single piece of advice to aspiring data scientists or machine learning enthusiasts, what would it be, and why do you consider it so valuable?\nAs I said earlier, the most important thing is to learn the fundamental concepts. The biggest mistake I see people making, especially people starting their careers or those in the early stage, is that they get influenced(pun intended!) by the noise on social media. Also, learning â€œhowâ€ to use an algorithm and learning â€œwhenâ€ to use an algorithm are two very different things.\nOne other unusual thing that I want to emphasize is that if you can afford higher education, go for it without a doubt. Degrees may not matter for your work, but they definitely help in your job search. I am not saying that you canâ€™t do good without degrees, but you will get more opportunities easily if you have one. I couldnâ€™t afford to apply for a masterâ€™s program after graduation, but if you can, you definitely should.\nQ: When you are given a problem statement, how do you devise AI solutions for it? What do you look for in a proposed solution?\nThe first thing I do before attempting anything else at all is to go through the data thoroughly. People naively jump on model building, but I tend to spend a lot of time with the data at hand. Once I understand the dataset, I look for a trivial solution that can be considered as a baseline. Defining metrics for a model and defining metrics that align with the business are two different aspects. We canâ€™t ignore the latter. Putting models into production means adjusting to the constraints in the production environment. Blatantly scaling up/down doesnâ€™t work for most of the scenarios.\nQ: Youâ€™ve always been bullish on JAX, the framework. What is it about JAX that you like? Why would you use JAX over say, TensorFlow or PyTorch? When you look at an ML framework, what attributes are you looking for?\nAlmost everything. Reproducibility is a first-class citizen, and so is parallelism in JAX. Have you tried sharding in JAX? I donâ€™t think any other framework (past and present included) had such a good API design for implementing parallelism.\nI would use JAX over any other framework because it is efficient and much easier to optimize. People keep focusing on compilers for programming languages but donâ€™t focus enough on the need for a compiler for deep learning workflows. In my opinion, Python is the perfect language, XLA is a great compiler, and JAX is the best framework for deep learning.\nIf I were to choose a framework today from a given list, here are the following things that I use as a checklist for evaluation:\n\nMental model\nEase of reproducibility\nEase of parallelism\nThe balance between low-level and high-level\nThe ecosystem\n\nI think the ecosystem is the part where JAX is lacking in a big way, but I hope with Keras 3, more people will contribute to the ecosystem, and it will catch up.\nQ: Canâ€™t have an interview without asking this question to the Annotated Papers guy ğŸ˜‚: what is one research paper youâ€™ve read recently which gave you that â€œahaâ€ feeling? Basically, a paper which blew your mind!\nI wouldnâ€™t say that I came across any paper in the last few months that blew my mind, but the latest paper from Apple titled MobileCLIP was a refreshing one. Deploying deep learning models on mobile devices is always a challenge, and requires a bit of rethinking, and modifications on the architectural side which arenâ€™t that obvious when you are deploying things on a server. In that way, I think MobileCLIP is a must-read for anyone interested in those kinds of details.\nQ: Your open source work has mostly focused on vision. Has your work in the industry focused around vision as well? Or have you dabbled in different fields?\nHaha yes, my open-source work has mostly been focused on vision, but in terms of industrial experience, I have a T-shaped knowledge graph where I consider the depth of expertise in vision and width for other areas of Machine Learning. From 2017 to date, I have worked on business problems focused on traditional machine learning, time series, vision, speech, and NLP. I have been on the other side of ML as well, where I have focused on MLOPs along with my research work.\nComputer Vision is my favorite field, and the biggest reason for that is that when I focus on vision problems, I have a sense of â€œwhatâ€™s happeningâ€ inside my pipelines. I find vision more mature compared to other fields, and I absolutely love working with images.\nQ: Youâ€™ve been in the industry for 6â€“7 years, and yet, manage to take time out to contribute to open source. What motivates you to keep contributing to open source?\nTwo major things. First, during my initial career phase, OSS played a big role in my learning and growth curves. Second, I consider solving problems in OSS as a mental exercise. The more you do it, the better you become. Your code being used by thousands of other developers is a true test of the logic baked in that code.\nQ: Whatâ€™s the next big thing for Aakash Nain? :) What are you planning to work on next in open source? If you have any big announcements to make, this is the best place to make them ğŸ˜‚\nFor now, I will be focusing on making some tutorials for large-scale training using Keras and JAX. At some point, I will also develop another library based on JAX purely for fun, mostly focused on the vision side, given if I get enough time.\nAnshuman & Abheesht: Thank you, Aakash! Itâ€™s been great talking to you!\nAnd that concludes the fourth interview of our â€œAI Chroniclesâ€ series! This series is a joint effort of Anshuman Mishra and Abheesht Sharma. Anshuman is currently working at Flip as a Machine Learning Engineer. Abheesht is an Applied Scientist at Amazon, where he works on detecting advertisement bots. Both of them contribute extensively to ML open-source software and have been involved in developing KerasNLP."
  },
  {
    "objectID": "posts/illustrated_llmos.html",
    "href": "posts/illustrated_llmos.html",
    "title": "Illustrated LLM OS",
    "section": "",
    "text": "This blog post explores the implementation of large language models (LLMs) as operating systems, inspired by Andrej Karpathyâ€™s vision of AI resembling an OS, akin to Jarvis from Iron Man. The focus is on practical considerations, proposing an application-level integration for LLMs within a terminal session. A novel approach involves injecting state machines into the decoding process, enabling real-time code execution and interaction. Additionally, this post proposes Reinforcement Learning by System Feedback (RLSF),â€ a reinforcement learning technique applied to code generation tasks. This method leverages a reward model to evaluate code correctness through Python subprocess execution, enhancing LLM performance. The findings contribute insights into the dynamic control of LLMs and their potential applications beyond coding tasks.\nImage source: [1hr Talk] Intro to Large Language Models by Andrej Karpathy"
  },
  {
    "objectID": "posts/illustrated_llmos.html#preliminaries",
    "href": "posts/illustrated_llmos.html#preliminaries",
    "title": "Illustrated LLM OS",
    "section": "Preliminaries:",
    "text": "Preliminaries:\n\nState Machines: A state machine is a mathematical abstraction used to design algorithms. A state machine reads a set of inputs and changes to a different state based on those inputs. A state is a description of the status of a system waiting to execute a transition. A transition is a set of actions to execute when a condition is fulfilled or an event is received. In a state diagram, circles represent each possible state and arrows represent transitions between states.\nConstrained Decoding: Constrained decoding is a technique used in natural language processing and sequence generation tasks, including those involving large language models (LLMs). In constrained decoding, the generation of sequences is guided or restricted by certain constraints or conditions. This approach is particularly useful when you want to control or influence the output of a language model to meet specific requirements or criteria.\n\nA major challenge in the implementation of LLM OS is establishing a Link between the Operating System and LLM, that ensures training following the principle of Responsible AI."
  },
  {
    "objectID": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "href": "posts/illustrated_llmos.html#where-should-llm-sit-concerns-possibilities-and-limitations",
    "title": "Illustrated LLM OS",
    "section": "Where should LLM sit? Concerns, Possibilities, and Limitations",
    "text": "Where should LLM sit? Concerns, Possibilities, and Limitations\nImagine a futuristic Jarvis-like AI. Itâ€™ll be able to search through the internet, access local files, videos, and images on the disk, and execute programs. Where should it sit? At the kernel level? At Python Level?\nAt Kernel Level: Consider integrating our advanced language model with the Linux kernel. This would provide the AI with comprehensive access to the operating systemâ€™s core functionalities. However, itâ€™s important to recognize that large language models (LLMs) are designed for human-like interaction, not intricate coding tasks. While embedding the model at the kernel level offers the advantage of understanding and controlling detailed system operations, it raises valid security concerns. Responsible development is crucial to ensure that the AIâ€™s evolving decision-making capabilities donâ€™t inadvertently compromise system integrity.\nAt Application Level: Alternatively, we can position the LLM at the application level, operating seamlessly within a terminal session. Leveraging the abstractions of a programming language, such as Python, provides optimal control over the AI. This approach facilitates easy updates to the LLM OS and allows for extensive customization."
  },
  {
    "objectID": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "href": "posts/illustrated_llmos.html#towards-implementation-injecting-a-state-machine-in-decoder",
    "title": "Illustrated LLM OS",
    "section": "Towards Implementation: Injecting a State Machine in Decoder",
    "text": "Towards Implementation: Injecting a State Machine in Decoder\nFollowing is the illustration of how the generation process works in decoder-like models source.\n\nImage source: The Illustrated GPT-2 by Jay Alammar\nThe seamless interaction of a Large Language Model (LLM) with an operating system (OS) can be achieved through the strategic injection of a State Machine. By utilizing this approach, the modelâ€™s behavior can be dynamically controlled, enhancing its capability to engage with the OS. In particular, the injection of special tokens acts as triggers, facilitating interaction with the Python interpreter through subprocesses.\nTo illustrate this concept, consider the scenario where the LLM is tasked with generating the sum of the first N natural numbers. Here, I request the model to encapsulate the generated code within designated special tokens, [CODE] and [/CODE], during its response. The State Machine is activated upon the generation of the [CODE] token, initiating the collection of the generated code in a buffer.\nUpon the subsequent generation of the [/CODE] token, the State Machine orchestrates a temporary pause in the generation process. This interlude allows for the invocation of a Python subprocess to execute the collected code. The output of this execution is then appended to the current cache. Resuming the generation process, this technique enables the model to dynamically learn program execution behavior in real time.\nIn the following sections of this blog, we delve deeper into how this innovative technique empowers the model to adapt and refine its program execution understanding on the fly.\n\n\n\nimage/png\n\n\nFollowing is an illustration of a state machine to do various operations like db queries, accessing file systems, and even internet searches through Python subprocesses.\n\n\n\nimage/png\n\n\nThe Python programming language boasts a robust ecosystem that opens up a multitude of possibilities, especially when harnessed through subprocesses. By utilizing subprocesses, we can seamlessly execute Shell or Programming Language codes, expanding the functionality of our applications. Pythonâ€™s versatility extends to internet access, where we can leverage powerful libraries like requests and urllib. This capability allows the Large Language Model (LLM) to interact dynamically with online resources. Additionally, the integration of command line packages, such as the one offered by Google, further enhances the LLMâ€™s capacity to fetch information from the web. Pythonâ€™s sophisticated file-handling utilities provide the LLM with the capability to navigate and manipulate files and multimedia at the userâ€™s request. This functionality extends beyond mere text-based interactions, offering a rich, multimodal experience. The inclusion of multimodal models further augments the LLMâ€™s ability to interpret and respond to a diverse range of user inputs."
  },
  {
    "objectID": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "href": "posts/illustrated_llmos.html#tuning-llms-for-os-use-case-a-rl-problem",
    "title": "Illustrated LLM OS",
    "section": "Tuning LLMs for OS use case: A RL Problem:",
    "text": "Tuning LLMs for OS use case: A RL Problem:\nImagine confronting a challenging programming problem and enlisting the assistance of the Large Language Model (LLM) to generate a solution. Traditionally, the process involves articulating the problem to the LLM, receiving the generated code, manual inspection for correctness, and iterative feedback for bug resolution.\nNow, letâ€™s recast this procedure as a Reinforcement Learning (RL) challenge, casting our LLM as an agent navigating a realm of computer processes. In this RL framework, the agentâ€™s objective is to generate code, subject to correctness scrutiny.\nTo enhance this code generation process, we leverage a fundamental concept in RL â€” the reward model. This model quantifies the quality of generated code based on execution results. Through training the reward model, the LLM is systematically penalized for generating incorrect code, eliminating the need for manual intervention.\nI call this technique Reinforcement Learning by System Feedback (RLSF). This technique can not only be applied to improving LLMs on code generation tasks but to a variety of tasks as shown in the state machine diagram."
  },
  {
    "objectID": "posts/illustrated_llmos.html#concluding-notes",
    "href": "posts/illustrated_llmos.html#concluding-notes",
    "title": "Illustrated LLM OS",
    "section": "Concluding Notes",
    "text": "Concluding Notes\nThe reason I published this blog post is because I think the discussion of integrating LLMs with Operating Systems should be Open. It should be developed responsibly in an Open Source Environment. If there is an initiative like this in the future, I would love to collaborate on it! At this stage, I couldnâ€™t get a working prototype of the approaches I discussed due to a lack of computing resources.\nJust Imagine an operating system built on HuggingFace transformerâ€™s LLM pipelines. This setup not only accommodates the effortless integration of new models but also ensures the system remains adaptable to the latest advancements in the field. LLMs can add a new layer of accessibility to our Operating System by acting as smart interfaces."
  },
  {
    "objectID": "posts/illustrated_llmos.html#acknowledgements",
    "href": "posts/illustrated_llmos.html#acknowledgements",
    "title": "Illustrated LLM OS",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nRavindra Majumdar and Sunil Mallya for introducing me to the concept of constrained decoding."
  },
  {
    "objectID": "pages/things.html",
    "href": "pages/things.html",
    "title": "Things",
    "section": "",
    "text": "My Strava: https://www.strava.com/athletes/141188010"
  }
]